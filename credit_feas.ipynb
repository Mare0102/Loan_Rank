{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold,  cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test = pd.read_csv('app_test.csv')\n",
    "app_train = pd.read_csv('app_train.csv')\n",
    "prev_app = pd.read_csv('prev_app.csv')\n",
    "installment_payment = pd.read_csv('installment_payment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unused columns\n",
    "app_train = app_train.drop('Unnamed: 0', axis=1)\n",
    "prev_app = prev_app.drop('Unnamed: 0', axis=1)\n",
    "installment_payment = installment_payment.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333538</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-11539</td>\n",
       "      <td>-921</td>\n",
       "      <td>-119.0</td>\n",
       "      <td>-2757</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406644</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>976711.5</td>\n",
       "      <td>49869.0</td>\n",
       "      <td>873000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15743</td>\n",
       "      <td>-4482</td>\n",
       "      <td>-1797.0</td>\n",
       "      <td>-2455</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259130</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>407520.0</td>\n",
       "      <td>25060.5</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20775</td>\n",
       "      <td>365243</td>\n",
       "      <td>-8737.0</td>\n",
       "      <td>-4312</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>NA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>411997</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>808650.0</td>\n",
       "      <td>26086.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20659</td>\n",
       "      <td>-10455</td>\n",
       "      <td>-4998.0</td>\n",
       "      <td>-4010</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241559</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-9013</td>\n",
       "      <td>-1190</td>\n",
       "      <td>-3524.0</td>\n",
       "      <td>-1644</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>Construction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LN_ID  TARGET    CONTRACT_TYPE GENDER  NUM_CHILDREN    INCOME  \\\n",
       "0  333538       0  Revolving loans      F             1   67500.0   \n",
       "1  406644       0       Cash loans      F             1  202500.0   \n",
       "2  259130       0       Cash loans      F             0  180000.0   \n",
       "3  411997       0       Cash loans      M             0  225000.0   \n",
       "4  241559       0  Revolving loans      M             0  135000.0   \n",
       "\n",
       "   APPROVED_CREDIT  ANNUITY     PRICE           INCOME_TYPE  \\\n",
       "0         202500.0  10125.0  202500.0               Working   \n",
       "1         976711.5  49869.0  873000.0  Commercial associate   \n",
       "2         407520.0  25060.5  360000.0             Pensioner   \n",
       "3         808650.0  26086.5  675000.0         State servant   \n",
       "4         180000.0   9000.0  180000.0  Commercial associate   \n",
       "\n",
       "                       EDUCATION         FAMILY_STATUS       HOUSING_TYPE  \\\n",
       "0  Secondary / secondary special               Married       With parents   \n",
       "1  Secondary / secondary special               Married  House / apartment   \n",
       "2  Secondary / secondary special               Married  House / apartment   \n",
       "3               Higher education               Married  House / apartment   \n",
       "4  Secondary / secondary special  Single / not married  House / apartment   \n",
       "\n",
       "   DAYS_AGE  DAYS_WORK  DAYS_REGISTRATION  DAYS_ID_CHANGE WEEKDAYS_APPLY  \\\n",
       "0    -11539       -921             -119.0           -2757        TUESDAY   \n",
       "1    -15743      -4482            -1797.0           -2455        TUESDAY   \n",
       "2    -20775     365243            -8737.0           -4312       THURSDAY   \n",
       "3    -20659     -10455            -4998.0           -4010      WEDNESDAY   \n",
       "4     -9013      -1190            -3524.0           -1644         SUNDAY   \n",
       "\n",
       "   HOUR_APPLY       ORGANIZATION_TYPE  \n",
       "0          18  Business Entity Type 3  \n",
       "1          14                   Other  \n",
       "2          14                     NA1  \n",
       "3          10                 Culture  \n",
       "4          11            Construction  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop EXT_SCORE\n",
    "app_train = app_train.drop(['EXT_SCORE_1', 'EXT_SCORE_2', 'EXT_SCORE_3'], axis=1)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61503 entries, 0 to 61502\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   LN_ID              61503 non-null  int64   \n",
      " 1   TARGET             61503 non-null  int64   \n",
      " 2   CONTRACT_TYPE      61503 non-null  category\n",
      " 3   GENDER             61503 non-null  category\n",
      " 4   NUM_CHILDREN       61503 non-null  int64   \n",
      " 5   INCOME             61503 non-null  float64 \n",
      " 6   APPROVED_CREDIT    61503 non-null  float64 \n",
      " 7   ANNUITY            61502 non-null  float64 \n",
      " 8   PRICE              61441 non-null  float64 \n",
      " 9   INCOME_TYPE        61503 non-null  category\n",
      " 10  EDUCATION          61503 non-null  category\n",
      " 11  FAMILY_STATUS      61503 non-null  category\n",
      " 12  HOUSING_TYPE       61503 non-null  category\n",
      " 13  DAYS_AGE           61503 non-null  int64   \n",
      " 14  DAYS_WORK          61503 non-null  int64   \n",
      " 15  DAYS_REGISTRATION  61503 non-null  float64 \n",
      " 16  DAYS_ID_CHANGE     61503 non-null  int64   \n",
      " 17  WEEKDAYS_APPLY     61503 non-null  category\n",
      " 18  HOUR_APPLY         61503 non-null  int64   \n",
      " 19  ORGANIZATION_TYPE  61503 non-null  category\n",
      "dtypes: category(8), float64(5), int64(7)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Change datatype\n",
    "\n",
    "app_train[app_train.select_dtypes('object').columns]=app_train.select_dtypes('object').astype('category')\n",
    "\n",
    "app_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN_ID                 0\n",
       "TARGET                0\n",
       "CONTRACT_TYPE         0\n",
       "GENDER                0\n",
       "NUM_CHILDREN          0\n",
       "INCOME                0\n",
       "APPROVED_CREDIT       0\n",
       "ANNUITY               1\n",
       "PRICE                62\n",
       "INCOME_TYPE           0\n",
       "EDUCATION             0\n",
       "FAMILY_STATUS         0\n",
       "HOUSING_TYPE          0\n",
       "DAYS_AGE              0\n",
       "DAYS_WORK             0\n",
       "DAYS_REGISTRATION     0\n",
       "DAYS_ID_CHANGE        0\n",
       "WEEKDAYS_APPLY        0\n",
       "HOUR_APPLY            0\n",
       "ORGANIZATION_TYPE     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "\n",
    "app_train.isna().sum()\n",
    "\n",
    "# ANNUITY will be dropped 1 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN_ID                 0\n",
       "TARGET                0\n",
       "CONTRACT_TYPE         0\n",
       "GENDER                0\n",
       "NUM_CHILDREN          0\n",
       "INCOME                0\n",
       "APPROVED_CREDIT       0\n",
       "ANNUITY               0\n",
       "PRICE                62\n",
       "INCOME_TYPE           0\n",
       "EDUCATION             0\n",
       "FAMILY_STATUS         0\n",
       "HOUSING_TYPE          0\n",
       "DAYS_AGE              0\n",
       "DAYS_WORK             0\n",
       "DAYS_REGISTRATION     0\n",
       "DAYS_ID_CHANGE        0\n",
       "WEEKDAYS_APPLY        0\n",
       "HOUR_APPLY            0\n",
       "ORGANIZATION_TYPE     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing value with 0\n",
    "app_train['ANNUITY'] = app_train['ANNUITY'].fillna(0)\n",
    "\n",
    "app_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>327096</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>125550.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18481</td>\n",
       "      <td>-3008</td>\n",
       "      <td>-5450.0</td>\n",
       "      <td>-2026</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Electricity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>274382</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-10275</td>\n",
       "      <td>-1937</td>\n",
       "      <td>-4103.0</td>\n",
       "      <td>-2946</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>17</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>294934</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>Municipal apartment</td>\n",
       "      <td>-10212</td>\n",
       "      <td>-2294</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-2868</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Industry: type 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>284082</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-10867</td>\n",
       "      <td>-286</td>\n",
       "      <td>-5510.0</td>\n",
       "      <td>-3475</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Business Entity Type 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>287492</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-22374</td>\n",
       "      <td>365243</td>\n",
       "      <td>-4751.0</td>\n",
       "      <td>-4816</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>NA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56644</th>\n",
       "      <td>170764</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-11412</td>\n",
       "      <td>-1147</td>\n",
       "      <td>-9431.0</td>\n",
       "      <td>-3379</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>Self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57530</th>\n",
       "      <td>422096</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-7950</td>\n",
       "      <td>-1068</td>\n",
       "      <td>-116.0</td>\n",
       "      <td>-577</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58776</th>\n",
       "      <td>107822</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-11079</td>\n",
       "      <td>-899</td>\n",
       "      <td>-3765.0</td>\n",
       "      <td>-572</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58803</th>\n",
       "      <td>295417</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-8141</td>\n",
       "      <td>-592</td>\n",
       "      <td>-6985.0</td>\n",
       "      <td>-812</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58840</th>\n",
       "      <td>131077</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-14069</td>\n",
       "      <td>-2646</td>\n",
       "      <td>-2821.0</td>\n",
       "      <td>-1141</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Postal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LN_ID  TARGET    CONTRACT_TYPE GENDER  NUM_CHILDREN    INCOME  \\\n",
       "722    327096       0  Revolving loans      F             0  125550.0   \n",
       "1448   274382       0  Revolving loans      M             0  180000.0   \n",
       "1509   294934       0  Revolving loans      M             1  202500.0   \n",
       "4378   284082       0  Revolving loans      F             0  247500.0   \n",
       "5267   287492       0  Revolving loans      F             0   67500.0   \n",
       "...       ...     ...              ...    ...           ...       ...   \n",
       "56644  170764       0  Revolving loans      F             1  112500.0   \n",
       "57530  422096       0  Revolving loans      F             1   90000.0   \n",
       "58776  107822       0  Revolving loans      F             1  121500.0   \n",
       "58803  295417       0  Revolving loans      F             1   67500.0   \n",
       "58840  131077       0  Revolving loans      F             0   90000.0   \n",
       "\n",
       "       APPROVED_CREDIT  ANNUITY  PRICE           INCOME_TYPE  \\\n",
       "722           225000.0  11250.0    NaN               Working   \n",
       "1448          270000.0  13500.0    NaN               Working   \n",
       "1509          202500.0  10125.0    NaN               Working   \n",
       "4378          405000.0  20250.0    NaN  Commercial associate   \n",
       "5267          135000.0   6750.0    NaN             Pensioner   \n",
       "...                ...      ...    ...                   ...   \n",
       "56644         202500.0  10125.0    NaN  Commercial associate   \n",
       "57530         202500.0  10125.0    NaN               Working   \n",
       "58776         180000.0   9000.0    NaN               Working   \n",
       "58803         180000.0   9000.0    NaN               Working   \n",
       "58840         202500.0  10125.0    NaN               Working   \n",
       "\n",
       "                           EDUCATION         FAMILY_STATUS  \\\n",
       "722    Secondary / secondary special               Married   \n",
       "1448   Secondary / secondary special  Single / not married   \n",
       "1509   Secondary / secondary special               Married   \n",
       "4378                Higher education               Married   \n",
       "5267   Secondary / secondary special               Married   \n",
       "...                              ...                   ...   \n",
       "56644  Secondary / secondary special  Single / not married   \n",
       "57530  Secondary / secondary special               Married   \n",
       "58776  Secondary / secondary special        Civil marriage   \n",
       "58803  Secondary / secondary special  Single / not married   \n",
       "58840  Secondary / secondary special               Married   \n",
       "\n",
       "              HOUSING_TYPE  DAYS_AGE  DAYS_WORK  DAYS_REGISTRATION  \\\n",
       "722      House / apartment    -18481      -3008            -5450.0   \n",
       "1448          With parents    -10275      -1937            -4103.0   \n",
       "1509   Municipal apartment    -10212      -2294              -67.0   \n",
       "4378     House / apartment    -10867       -286            -5510.0   \n",
       "5267     House / apartment    -22374     365243            -4751.0   \n",
       "...                    ...       ...        ...                ...   \n",
       "56644         With parents    -11412      -1147            -9431.0   \n",
       "57530    House / apartment     -7950      -1068             -116.0   \n",
       "58776    House / apartment    -11079       -899            -3765.0   \n",
       "58803    House / apartment     -8141       -592            -6985.0   \n",
       "58840    House / apartment    -14069      -2646            -2821.0   \n",
       "\n",
       "       DAYS_ID_CHANGE WEEKDAYS_APPLY  HOUR_APPLY       ORGANIZATION_TYPE  \n",
       "722             -2026       SATURDAY          13             Electricity  \n",
       "1448            -2946       THURSDAY          17                   Other  \n",
       "1509            -2868       SATURDAY          12        Industry: type 9  \n",
       "4378            -3475       THURSDAY          12  Business Entity Type 2  \n",
       "5267            -4816      WEDNESDAY          10                     NA1  \n",
       "...               ...            ...         ...                     ...  \n",
       "56644           -3379       THURSDAY          11           Self-employed  \n",
       "57530            -577       SATURDAY          14                   Other  \n",
       "58776            -572      WEDNESDAY          11              Government  \n",
       "58803            -812        TUESDAY           9              Restaurant  \n",
       "58840           -1141       THURSDAY          13                  Postal  \n",
       "\n",
       "[62 rows x 20 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values on price because of Revolving loans, thus change to 0\n",
    "app_train[app_train['PRICE'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN_ID                0\n",
       "TARGET               0\n",
       "CONTRACT_TYPE        0\n",
       "GENDER               0\n",
       "NUM_CHILDREN         0\n",
       "INCOME               0\n",
       "APPROVED_CREDIT      0\n",
       "ANNUITY              0\n",
       "PRICE                0\n",
       "INCOME_TYPE          0\n",
       "EDUCATION            0\n",
       "FAMILY_STATUS        0\n",
       "HOUSING_TYPE         0\n",
       "DAYS_AGE             0\n",
       "DAYS_WORK            0\n",
       "DAYS_REGISTRATION    0\n",
       "DAYS_ID_CHANGE       0\n",
       "WEEKDAYS_APPLY       0\n",
       "HOUR_APPLY           0\n",
       "ORGANIZATION_TYPE    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values with 0\n",
    "app_train['PRICE'] = app_train['PRICE'].fillna(0)\n",
    "\n",
    "app_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cash loans         55699\n",
       "Revolving loans     5804\n",
       "Name: CONTRACT_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "F    40549\n",
       "M    20954\n",
       "Name: GENDER, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Working                 31621\n",
       "Commercial associate    14217\n",
       "Pensioner               11249\n",
       "State servant            4407\n",
       "Unemployed                  5\n",
       "Student                     3\n",
       "Businessman                 1\n",
       "Name: INCOME_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Secondary / secondary special    43777\n",
       "Higher education                 14887\n",
       "Incomplete higher                 2045\n",
       "Lower secondary                    760\n",
       "Academic degree                     34\n",
       "Name: EDUCATION, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Married                 39370\n",
       "Single / not married     9029\n",
       "Civil marriage           5881\n",
       "Separated                3970\n",
       "Widow                    3253\n",
       "Name: FAMILY_STATUS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "House / apartment      54648\n",
       "With parents            2891\n",
       "Municipal apartment     2203\n",
       "Rented apartment         988\n",
       "Office apartment         534\n",
       "Co-op apartment          239\n",
       "Name: HOUSING_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TUESDAY      10838\n",
       "WEDNESDAY    10477\n",
       "MONDAY       10234\n",
       "THURSDAY     10011\n",
       "FRIDAY        9993\n",
       "SATURDAY      6795\n",
       "SUNDAY        3155\n",
       "Name: WEEKDAYS_APPLY, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Business Entity Type 3    13561\n",
       "NA1                       11253\n",
       "Self-employed              7700\n",
       "Other                      3305\n",
       "Medicine                   2224\n",
       "Business Entity Type 2     2083\n",
       "Government                 2045\n",
       "School                     1787\n",
       "Trade: type 7              1557\n",
       "Kindergarten               1388\n",
       "Construction               1374\n",
       "Business Entity Type 1     1185\n",
       "Transport: type 4          1020\n",
       "Trade: type 3               663\n",
       "Industry: type 9            660\n",
       "Industry: type 3            638\n",
       "Housing                     611\n",
       "Security                    596\n",
       "Military                    540\n",
       "Industry: type 11           533\n",
       "Bank                        526\n",
       "Agriculture                 504\n",
       "Police                      457\n",
       "Transport: type 2           436\n",
       "Postal                      413\n",
       "Security Ministries         399\n",
       "Trade: type 2               371\n",
       "Restaurant                  352\n",
       "University                  311\n",
       "Services                    301\n",
       "Industry: type 7            241\n",
       "Transport: type 3           238\n",
       "Industry: type 1            203\n",
       "Electricity                 200\n",
       "Industry: type 4            195\n",
       "Hotel                       194\n",
       "Trade: type 6               133\n",
       "Telecom                     125\n",
       "Industry: type 5            121\n",
       "Insurance                   119\n",
       "Emergency                   119\n",
       "Advertising                  95\n",
       "Industry: type 2             91\n",
       "Realtor                      79\n",
       "Trade: type 1                78\n",
       "Industry: type 12            76\n",
       "Legal Services               72\n",
       "Culture                      67\n",
       "Mobile                       66\n",
       "Cleaning                     47\n",
       "Transport: type 1            41\n",
       "Industry: type 10            25\n",
       "Industry: type 6             24\n",
       "Industry: type 13            17\n",
       "Religion                     15\n",
       "Trade: type 4                14\n",
       "Trade: type 5                 9\n",
       "Industry: type 8              6\n",
       "Name: ORGANIZATION_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check strange values\n",
    "\n",
    "for i in range(len(app_train.select_dtypes('category').columns)):\n",
    "    display(app_train[app_train.select_dtypes('category').columns[i]].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 61502 entries, 0 to 61502\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   LN_ID              61502 non-null  int64   \n",
      " 1   TARGET             61502 non-null  int64   \n",
      " 2   CONTRACT_TYPE      61502 non-null  category\n",
      " 3   GENDER             61502 non-null  category\n",
      " 4   NUM_CHILDREN       61502 non-null  int64   \n",
      " 5   INCOME             61502 non-null  float64 \n",
      " 6   APPROVED_CREDIT    61502 non-null  float64 \n",
      " 7   ANNUITY            61502 non-null  float64 \n",
      " 8   PRICE              61502 non-null  float64 \n",
      " 9   INCOME_TYPE        61502 non-null  category\n",
      " 10  EDUCATION          61502 non-null  category\n",
      " 11  FAMILY_STATUS      61502 non-null  category\n",
      " 12  HOUSING_TYPE       61502 non-null  category\n",
      " 13  DAYS_AGE           61502 non-null  int64   \n",
      " 14  DAYS_WORK          61502 non-null  int64   \n",
      " 15  DAYS_REGISTRATION  61502 non-null  float64 \n",
      " 16  DAYS_ID_CHANGE     61502 non-null  int64   \n",
      " 17  WEEKDAYS_APPLY     61502 non-null  category\n",
      " 18  HOUR_APPLY         61502 non-null  int64   \n",
      " 19  ORGANIZATION_TYPE  61502 non-null  category\n",
      "dtypes: category(8), float64(5), int64(7)\n",
      "memory usage: 6.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Income type businessman will cause an error later on modeling, as this only exist on train set. The row contain this type will be dropped (1 row)\n",
    "\n",
    "app_train = app_train.drop(index=app_train.index[app_train['INCOME_TYPE']=='Businessman'])\n",
    "\n",
    "app_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change NA1 in ORGANIZATION_TYPE to Unkonwn\n",
    "\n",
    "app_train['ORGANIZATION_TYPE'] = app_train['ORGANIZATION_TYPE'].str.replace(\"NA1\",\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [LN_ID, TARGET, CONTRACT_TYPE, GENDER, NUM_CHILDREN, INCOME, APPROVED_CREDIT, ANNUITY, PRICE, INCOME_TYPE, EDUCATION, FAMILY_STATUS, HOUSING_TYPE, DAYS_AGE, DAYS_WORK, DAYS_REGISTRATION, DAYS_ID_CHANGE, WEEKDAYS_APPLY, HOUR_APPLY, ORGANIZATION_TYPE]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Duplicated value\n",
    "app_train[app_train.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>YEAR_AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333538</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-11539</td>\n",
       "      <td>-921</td>\n",
       "      <td>-119.0</td>\n",
       "      <td>-2757</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>31.613699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406644</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>976711.5</td>\n",
       "      <td>49869.0</td>\n",
       "      <td>873000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15743</td>\n",
       "      <td>-4482</td>\n",
       "      <td>-1797.0</td>\n",
       "      <td>-2455</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Other</td>\n",
       "      <td>43.131507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259130</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>407520.0</td>\n",
       "      <td>25060.5</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20775</td>\n",
       "      <td>365243</td>\n",
       "      <td>-8737.0</td>\n",
       "      <td>-4312</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>56.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>411997</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>808650.0</td>\n",
       "      <td>26086.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>State servant</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20659</td>\n",
       "      <td>-10455</td>\n",
       "      <td>-4998.0</td>\n",
       "      <td>-4010</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Culture</td>\n",
       "      <td>56.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241559</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>...</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-9013</td>\n",
       "      <td>-1190</td>\n",
       "      <td>-3524.0</td>\n",
       "      <td>-1644</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>Construction</td>\n",
       "      <td>24.693151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LN_ID  TARGET    CONTRACT_TYPE GENDER  NUM_CHILDREN    INCOME  \\\n",
       "0  333538       0  Revolving loans      F             1   67500.0   \n",
       "1  406644       0       Cash loans      F             1  202500.0   \n",
       "2  259130       0       Cash loans      F             0  180000.0   \n",
       "3  411997       0       Cash loans      M             0  225000.0   \n",
       "4  241559       0  Revolving loans      M             0  135000.0   \n",
       "\n",
       "   APPROVED_CREDIT  ANNUITY     PRICE           INCOME_TYPE  ...  \\\n",
       "0         202500.0  10125.0  202500.0               Working  ...   \n",
       "1         976711.5  49869.0  873000.0  Commercial associate  ...   \n",
       "2         407520.0  25060.5  360000.0             Pensioner  ...   \n",
       "3         808650.0  26086.5  675000.0         State servant  ...   \n",
       "4         180000.0   9000.0  180000.0  Commercial associate  ...   \n",
       "\n",
       "          FAMILY_STATUS       HOUSING_TYPE DAYS_AGE  DAYS_WORK  \\\n",
       "0               Married       With parents   -11539       -921   \n",
       "1               Married  House / apartment   -15743      -4482   \n",
       "2               Married  House / apartment   -20775     365243   \n",
       "3               Married  House / apartment   -20659     -10455   \n",
       "4  Single / not married  House / apartment    -9013      -1190   \n",
       "\n",
       "   DAYS_REGISTRATION  DAYS_ID_CHANGE  WEEKDAYS_APPLY HOUR_APPLY  \\\n",
       "0             -119.0           -2757         TUESDAY         18   \n",
       "1            -1797.0           -2455         TUESDAY         14   \n",
       "2            -8737.0           -4312        THURSDAY         14   \n",
       "3            -4998.0           -4010       WEDNESDAY         10   \n",
       "4            -3524.0           -1644          SUNDAY         11   \n",
       "\n",
       "        ORGANIZATION_TYPE   YEAR_AGE  \n",
       "0  Business Entity Type 3  31.613699  \n",
       "1                   Other  43.131507  \n",
       "2                 Unknown  56.917808  \n",
       "3                 Culture  56.600000  \n",
       "4            Construction  24.693151  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change Age from days to year\n",
    "\n",
    "app_train['YEAR_AGE'] = app_train.apply(lambda x: x['DAYS_AGE']/-365, axis=1)\n",
    "\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for cleaning app_test, as it have the same columns and data types with app_train\n",
    "def wrangle(df):\n",
    "    df.drop(['EXT_SCORE_1','EXT_SCORE_2','EXT_SCORE_3'],axis=1,inplace=True)\n",
    "    df['ANNUITY']=df['ANNUITY'].fillna(0)\n",
    "    df[df.select_dtypes('object').columns]=df.select_dtypes('object').astype('category')\n",
    "    df['ORGANIZATION_TYPE']=df['ORGANIZATION_TYPE'].str.replace(\"NA1\",\"Unknown\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>219092</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>871029.0</td>\n",
       "      <td>44604.0</td>\n",
       "      <td>765000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17598</td>\n",
       "      <td>-2650</td>\n",
       "      <td>-1411</td>\n",
       "      <td>-1131</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141577</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>485640.0</td>\n",
       "      <td>34537.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>Office apartment</td>\n",
       "      <td>-14097</td>\n",
       "      <td>-7408</td>\n",
       "      <td>-7908</td>\n",
       "      <td>-4872</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180205</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>8887.5</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18384</td>\n",
       "      <td>-2826</td>\n",
       "      <td>-8226</td>\n",
       "      <td>-1930</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>357381</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>506889.0</td>\n",
       "      <td>24781.5</td>\n",
       "      <td>418500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-12170</td>\n",
       "      <td>-926</td>\n",
       "      <td>-916</td>\n",
       "      <td>-4048</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>271229</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>21888.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Incomplete higher</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-10790</td>\n",
       "      <td>-577</td>\n",
       "      <td>-4640</td>\n",
       "      <td>-2035</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14756</th>\n",
       "      <td>144293</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>7317.0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-12866</td>\n",
       "      <td>-3154</td>\n",
       "      <td>-1251</td>\n",
       "      <td>-4395</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14757</th>\n",
       "      <td>272093</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>313438.5</td>\n",
       "      <td>21073.5</td>\n",
       "      <td>283500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-11421</td>\n",
       "      <td>-3962</td>\n",
       "      <td>-5247</td>\n",
       "      <td>-3795</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>Self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14758</th>\n",
       "      <td>251117</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>1078200.0</td>\n",
       "      <td>31522.5</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-13313</td>\n",
       "      <td>-2258</td>\n",
       "      <td>-3899</td>\n",
       "      <td>-4515</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14759</th>\n",
       "      <td>334313</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>272520.0</td>\n",
       "      <td>21658.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Widow</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-24085</td>\n",
       "      <td>365243</td>\n",
       "      <td>-9354</td>\n",
       "      <td>-4740</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>15</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14760</th>\n",
       "      <td>217927</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>260640.0</td>\n",
       "      <td>20299.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-12782</td>\n",
       "      <td>-387</td>\n",
       "      <td>-1261</td>\n",
       "      <td>-4823</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>Self-employed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14761 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LN_ID  TARGET CONTRACT_TYPE GENDER  NUM_CHILDREN    INCOME  \\\n",
       "0      219092       0    Cash loans      M             3  135000.0   \n",
       "1      141577       0    Cash loans      F             0  144000.0   \n",
       "2      180205       0    Cash loans      F             1   90000.0   \n",
       "3      357381       0    Cash loans      M             2  112500.0   \n",
       "4      271229       0    Cash loans      M             0  216000.0   \n",
       "...       ...     ...           ...    ...           ...       ...   \n",
       "14756  144293       0    Cash loans      F             1   67500.0   \n",
       "14757  272093       1    Cash loans      F             1  135000.0   \n",
       "14758  251117       0    Cash loans      F             0  157500.0   \n",
       "14759  334313       0    Cash loans      F             0  216000.0   \n",
       "14760  217927       0    Cash loans      F             3  112500.0   \n",
       "\n",
       "       APPROVED_CREDIT  ANNUITY     PRICE    INCOME_TYPE  \\\n",
       "0             871029.0  44604.0  765000.0        Working   \n",
       "1             485640.0  34537.5  450000.0        Working   \n",
       "2             247500.0   8887.5  247500.0        Working   \n",
       "3             506889.0  24781.5  418500.0        Working   \n",
       "4             450000.0  21888.0  450000.0        Working   \n",
       "...                ...      ...       ...            ...   \n",
       "14756         112500.0   7317.0  112500.0        Working   \n",
       "14757         313438.5  21073.5  283500.0        Working   \n",
       "14758        1078200.0  31522.5  900000.0  State servant   \n",
       "14759         272520.0  21658.5  225000.0      Pensioner   \n",
       "14760         260640.0  20299.5  225000.0        Working   \n",
       "\n",
       "                           EDUCATION   FAMILY_STATUS       HOUSING_TYPE  \\\n",
       "0      Secondary / secondary special         Married  House / apartment   \n",
       "1                   Higher education         Married   Office apartment   \n",
       "2      Secondary / secondary special         Married  House / apartment   \n",
       "3      Secondary / secondary special         Married  House / apartment   \n",
       "4                  Incomplete higher         Married  House / apartment   \n",
       "...                              ...             ...                ...   \n",
       "14756  Secondary / secondary special         Married  House / apartment   \n",
       "14757  Secondary / secondary special  Civil marriage       With parents   \n",
       "14758               Higher education       Separated  House / apartment   \n",
       "14759  Secondary / secondary special           Widow  House / apartment   \n",
       "14760  Secondary / secondary special       Separated  House / apartment   \n",
       "\n",
       "       DAYS_AGE  DAYS_WORK  DAYS_REGISTRATION  DAYS_ID_CHANGE WEEKDAYS_APPLY  \\\n",
       "0        -17598      -2650              -1411           -1131       SATURDAY   \n",
       "1        -14097      -7408              -7908           -4872         MONDAY   \n",
       "2        -18384      -2826              -8226           -1930       SATURDAY   \n",
       "3        -12170       -926               -916           -4048       THURSDAY   \n",
       "4        -10790       -577              -4640           -2035         MONDAY   \n",
       "...         ...        ...                ...             ...            ...   \n",
       "14756    -12866      -3154              -1251           -4395        TUESDAY   \n",
       "14757    -11421      -3962              -5247           -3795       SATURDAY   \n",
       "14758    -13313      -2258              -3899           -4515         MONDAY   \n",
       "14759    -24085     365243              -9354           -4740       THURSDAY   \n",
       "14760    -12782       -387              -1261           -4823       THURSDAY   \n",
       "\n",
       "       HOUR_APPLY       ORGANIZATION_TYPE  \n",
       "0               7  Business Entity Type 3  \n",
       "1              14            Kindergarten  \n",
       "2              12           Self-employed  \n",
       "3              13                   Other  \n",
       "4              14  Business Entity Type 3  \n",
       "...           ...                     ...  \n",
       "14756          10              Government  \n",
       "14757           7           Self-employed  \n",
       "14758          12                   Other  \n",
       "14759          15                 Unknown  \n",
       "14760           9           Self-employed  \n",
       "\n",
       "[14761 rows x 20 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangle(app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14761 entries, 0 to 14760\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   LN_ID              14761 non-null  int64   \n",
      " 1   TARGET             14761 non-null  int64   \n",
      " 2   CONTRACT_TYPE      14761 non-null  category\n",
      " 3   GENDER             14761 non-null  category\n",
      " 4   NUM_CHILDREN       14761 non-null  int64   \n",
      " 5   INCOME             14761 non-null  float64 \n",
      " 6   APPROVED_CREDIT    14761 non-null  float64 \n",
      " 7   ANNUITY            14761 non-null  float64 \n",
      " 8   PRICE              14741 non-null  float64 \n",
      " 9   INCOME_TYPE        14761 non-null  category\n",
      " 10  EDUCATION          14761 non-null  category\n",
      " 11  FAMILY_STATUS      14761 non-null  category\n",
      " 12  HOUSING_TYPE       14761 non-null  category\n",
      " 13  DAYS_AGE           14761 non-null  int64   \n",
      " 14  DAYS_WORK          14761 non-null  int64   \n",
      " 15  DAYS_REGISTRATION  14761 non-null  int64   \n",
      " 16  DAYS_ID_CHANGE     14761 non-null  int64   \n",
      " 17  WEEKDAYS_APPLY     14761 non-null  category\n",
      " 18  HOUR_APPLY         14761 non-null  int64   \n",
      " 19  ORGANIZATION_TYPE  14761 non-null  object  \n",
      "dtypes: category(7), float64(4), int64(8), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "app_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN_ID                0\n",
       "TARGET               0\n",
       "CONTRACT_TYPE        0\n",
       "GENDER               0\n",
       "NUM_CHILDREN         0\n",
       "INCOME               0\n",
       "APPROVED_CREDIT      0\n",
       "ANNUITY              0\n",
       "PRICE                0\n",
       "INCOME_TYPE          0\n",
       "EDUCATION            0\n",
       "FAMILY_STATUS        0\n",
       "HOUSING_TYPE         0\n",
       "DAYS_AGE             0\n",
       "DAYS_WORK            0\n",
       "DAYS_REGISTRATION    0\n",
       "DAYS_ID_CHANGE       0\n",
       "WEEKDAYS_APPLY       0\n",
       "HOUR_APPLY           0\n",
       "ORGANIZATION_TYPE    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing value with 0\n",
    "app_test['PRICE'] = app_test['PRICE'].fillna(0)\n",
    "\n",
    "app_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [LN_ID, TARGET, CONTRACT_TYPE, GENDER, NUM_CHILDREN, INCOME, APPROVED_CREDIT, ANNUITY, PRICE, INCOME_TYPE, EDUCATION, FAMILY_STATUS, HOUSING_TYPE, DAYS_AGE, DAYS_WORK, DAYS_REGISTRATION, DAYS_ID_CHANGE, WEEKDAYS_APPLY, HOUR_APPLY, ORGANIZATION_TYPE]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Duplicated value\n",
    "app_test[app_test.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>INCOME_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>FAMILY_STATUS</th>\n",
       "      <th>HOUSING_TYPE</th>\n",
       "      <th>DAYS_AGE</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>YEAR_AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>219092</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>871029.0</td>\n",
       "      <td>44604.0</td>\n",
       "      <td>765000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17598</td>\n",
       "      <td>-2650</td>\n",
       "      <td>-1411</td>\n",
       "      <td>-1131</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>48.213699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141577</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>485640.0</td>\n",
       "      <td>34537.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>Office apartment</td>\n",
       "      <td>-14097</td>\n",
       "      <td>-7408</td>\n",
       "      <td>-7908</td>\n",
       "      <td>-4872</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Kindergarten</td>\n",
       "      <td>38.621918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180205</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>8887.5</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18384</td>\n",
       "      <td>-2826</td>\n",
       "      <td>-8226</td>\n",
       "      <td>-1930</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>50.367123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>357381</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>506889.0</td>\n",
       "      <td>24781.5</td>\n",
       "      <td>418500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-12170</td>\n",
       "      <td>-926</td>\n",
       "      <td>-916</td>\n",
       "      <td>-4048</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Other</td>\n",
       "      <td>33.342466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>271229</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>21888.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>...</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-10790</td>\n",
       "      <td>-577</td>\n",
       "      <td>-4640</td>\n",
       "      <td>-2035</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>29.561644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LN_ID  TARGET CONTRACT_TYPE GENDER  NUM_CHILDREN    INCOME  \\\n",
       "0  219092       0    Cash loans      M             3  135000.0   \n",
       "1  141577       0    Cash loans      F             0  144000.0   \n",
       "2  180205       0    Cash loans      F             1   90000.0   \n",
       "3  357381       0    Cash loans      M             2  112500.0   \n",
       "4  271229       0    Cash loans      M             0  216000.0   \n",
       "\n",
       "   APPROVED_CREDIT  ANNUITY     PRICE INCOME_TYPE  ... FAMILY_STATUS  \\\n",
       "0         871029.0  44604.0  765000.0     Working  ...       Married   \n",
       "1         485640.0  34537.5  450000.0     Working  ...       Married   \n",
       "2         247500.0   8887.5  247500.0     Working  ...       Married   \n",
       "3         506889.0  24781.5  418500.0     Working  ...       Married   \n",
       "4         450000.0  21888.0  450000.0     Working  ...       Married   \n",
       "\n",
       "        HOUSING_TYPE DAYS_AGE  DAYS_WORK  DAYS_REGISTRATION  DAYS_ID_CHANGE  \\\n",
       "0  House / apartment   -17598      -2650              -1411           -1131   \n",
       "1   Office apartment   -14097      -7408              -7908           -4872   \n",
       "2  House / apartment   -18384      -2826              -8226           -1930   \n",
       "3  House / apartment   -12170       -926               -916           -4048   \n",
       "4  House / apartment   -10790       -577              -4640           -2035   \n",
       "\n",
       "   WEEKDAYS_APPLY HOUR_APPLY       ORGANIZATION_TYPE   YEAR_AGE  \n",
       "0        SATURDAY          7  Business Entity Type 3  48.213699  \n",
       "1          MONDAY         14            Kindergarten  38.621918  \n",
       "2        SATURDAY         12           Self-employed  50.367123  \n",
       "3        THURSDAY         13                   Other  33.342466  \n",
       "4          MONDAY         14  Business Entity Type 3  29.561644  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change Age from days to year\n",
    "\n",
    "app_test['YEAR_AGE'] = app_test.apply(lambda x: x['DAYS_AGE']/-365, axis=1)\n",
    "\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cash loans         13281\n",
       "Revolving loans     1480\n",
       "Name: CONTRACT_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "F    9685\n",
       "M    5076\n",
       "Name: GENDER, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Working                 7571\n",
       "Commercial associate    3427\n",
       "Pensioner               2726\n",
       "State servant           1036\n",
       "Unemployed                 1\n",
       "Name: INCOME_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Secondary / secondary special    10431\n",
       "Higher education                  3615\n",
       "Incomplete higher                  500\n",
       "Lower secondary                    208\n",
       "Academic degree                      7\n",
       "Name: EDUCATION, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Married                 9539\n",
       "Single / not married    2084\n",
       "Civil marriage          1441\n",
       "Separated                937\n",
       "Widow                    760\n",
       "Name: FAMILY_STATUS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "House / apartment      13135\n",
       "With parents             697\n",
       "Municipal apartment      531\n",
       "Rented apartment         218\n",
       "Office apartment         128\n",
       "Co-op apartment           52\n",
       "Name: HOUSING_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TUESDAY      2601\n",
       "WEDNESDAY    2571\n",
       "FRIDAY       2417\n",
       "THURSDAY     2380\n",
       "MONDAY       2375\n",
       "SATURDAY     1653\n",
       "SUNDAY        764\n",
       "Name: WEEKDAYS_APPLY, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check strange values\n",
    "\n",
    "for i in range(len(app_test.select_dtypes('category').columns)):\n",
    "    display(app_test[app_test.select_dtypes('category').columns[i]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prev_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350712 entries, 0 to 350711\n",
      "Data columns (total 18 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   SK_ID_PREV                 350712 non-null  int64  \n",
      " 1   LN_ID                      350712 non-null  int64  \n",
      " 2   CONTRACT_TYPE              350712 non-null  object \n",
      " 3   ANNUITY                    274103 non-null  float64\n",
      " 4   APPLICATION                350712 non-null  float64\n",
      " 5   APPROVED_CREDIT            350712 non-null  float64\n",
      " 6   AMT_DOWN_PAYMENT           164205 non-null  float64\n",
      " 7   PRICE                      271072 non-null  float64\n",
      " 8   WEEKDAYS_APPLY             350712 non-null  object \n",
      " 9   HOUR_APPLY                 350712 non-null  int64  \n",
      " 10  CONTRACT_STATUS            350712 non-null  object \n",
      " 11  DAYS_DECISION              350712 non-null  int64  \n",
      " 12  TERM_PAYMENT               274103 non-null  float64\n",
      " 13  YIELD_GROUP                350712 non-null  object \n",
      " 14  FIRST_DRAW                 211407 non-null  float64\n",
      " 15  FIRST_DUE                  211407 non-null  float64\n",
      " 16  TERMINATION                211407 non-null  float64\n",
      " 17  NFLAG_INSURED_ON_APPROVAL  211407 non-null  float64\n",
      "dtypes: float64(10), int64(4), object(4)\n",
      "memory usage: 48.2+ MB\n"
     ]
    }
   ],
   "source": [
    "prev_app.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_PREV                        0\n",
       "LN_ID                             0\n",
       "CONTRACT_TYPE                     0\n",
       "ANNUITY                       76609\n",
       "APPLICATION                       0\n",
       "APPROVED_CREDIT                   0\n",
       "AMT_DOWN_PAYMENT             186507\n",
       "PRICE                         79640\n",
       "WEEKDAYS_APPLY                    0\n",
       "HOUR_APPLY                        0\n",
       "CONTRACT_STATUS                   0\n",
       "DAYS_DECISION                     0\n",
       "TERM_PAYMENT                  76609\n",
       "YIELD_GROUP                       0\n",
       "FIRST_DRAW                   139305\n",
       "FIRST_DUE                    139305\n",
       "TERMINATION                  139305\n",
       "NFLAG_INSURED_ON_APPROVAL    139305\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_PREV                   0\n",
       "LN_ID                        0\n",
       "CONTRACT_TYPE                0\n",
       "ANNUITY                      0\n",
       "APPLICATION                  0\n",
       "APPROVED_CREDIT              0\n",
       "AMT_DOWN_PAYMENT             0\n",
       "PRICE                        0\n",
       "WEEKDAYS_APPLY               0\n",
       "HOUR_APPLY                   0\n",
       "CONTRACT_STATUS              0\n",
       "DAYS_DECISION                0\n",
       "TERM_PAYMENT                 0\n",
       "YIELD_GROUP                  0\n",
       "FIRST_DRAW                   0\n",
       "FIRST_DUE                    0\n",
       "TERMINATION                  0\n",
       "NFLAG_INSURED_ON_APPROVAL    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values in numerical columns will be replaced with 0\n",
    "\n",
    "prev_app = prev_app.fillna(0)\n",
    "\n",
    "prev_app.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>APPLICATION</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>AMT_DOWN_PAYMENT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>CONTRACT_STATUS</th>\n",
       "      <th>DAYS_DECISION</th>\n",
       "      <th>TERM_PAYMENT</th>\n",
       "      <th>YIELD_GROUP</th>\n",
       "      <th>FIRST_DRAW</th>\n",
       "      <th>FIRST_DUE</th>\n",
       "      <th>TERMINATION</th>\n",
       "      <th>NFLAG_INSURED_ON_APPROVAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SK_ID_PREV, LN_ID, CONTRACT_TYPE, ANNUITY, APPLICATION, APPROVED_CREDIT, AMT_DOWN_PAYMENT, PRICE, WEEKDAYS_APPLY, HOUR_APPLY, CONTRACT_STATUS, DAYS_DECISION, TERM_PAYMENT, YIELD_GROUP, FIRST_DRAW, FIRST_DUE, TERMINATION, NFLAG_INSURED_ON_APPROVAL]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Duplicated value\n",
    "prev_app[prev_app.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cash loans         155822\n",
       "Consumer loans     154684\n",
       "Revolving loans     40131\n",
       "NA1                    75\n",
       "Name: CONTRACT_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TUESDAY      53694\n",
       "WEDNESDAY    53679\n",
       "MONDAY       53185\n",
       "FRIDAY       52771\n",
       "THURSDAY     52125\n",
       "SATURDAY     50687\n",
       "SUNDAY       34571\n",
       "Name: WEEKDAYS_APPLY, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Approved        219687\n",
       "Canceled         64590\n",
       "Refused          60795\n",
       "Unused offer      5640\n",
       "Name: CONTRACT_STATUS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NA1           106872\n",
       "middle         80248\n",
       "high           76040\n",
       "low_normal     68163\n",
       "low_action     19389\n",
       "Name: YIELD_GROUP, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check strange values\n",
    "\n",
    "for i in range(len(prev_app.select_dtypes('object').columns)):\n",
    "    display(prev_app[prev_app.select_dtypes('object').columns[i]].value_counts())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installment_payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2872306 entries, 0 to 2872305\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   SK_ID_PREV   int64  \n",
      " 1   LN_ID        int64  \n",
      " 2   INST_NUMBER  int64  \n",
      " 3   INST_DAYS    float64\n",
      " 4   PAY_DAYS     float64\n",
      " 5   AMT_INST     float64\n",
      " 6   AMT_PAY      float64\n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 153.4 MB\n"
     ]
    }
   ],
   "source": [
    "installment_payment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_PREV       0\n",
       "LN_ID            0\n",
       "INST_NUMBER      0\n",
       "INST_DAYS        0\n",
       "PAY_DAYS       673\n",
       "AMT_INST         0\n",
       "AMT_PAY        673\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "installment_payment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_PREV     0\n",
       "LN_ID          0\n",
       "INST_NUMBER    0\n",
       "INST_DAYS      0\n",
       "PAY_DAYS       0\n",
       "AMT_INST       0\n",
       "AMT_PAY        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values in numerical columns will be replaced with 0\n",
    "\n",
    "installment_payment = installment_payment.fillna(0)\n",
    "\n",
    "installment_payment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>INST_NUMBER</th>\n",
       "      <th>INST_DAYS</th>\n",
       "      <th>PAY_DAYS</th>\n",
       "      <th>AMT_INST</th>\n",
       "      <th>AMT_PAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261880</th>\n",
       "      <td>1604926</td>\n",
       "      <td>164271</td>\n",
       "      <td>3</td>\n",
       "      <td>-388.0</td>\n",
       "      <td>-405.0</td>\n",
       "      <td>2610.000</td>\n",
       "      <td>5220.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556029</th>\n",
       "      <td>1853107</td>\n",
       "      <td>111076</td>\n",
       "      <td>4</td>\n",
       "      <td>-692.0</td>\n",
       "      <td>-719.0</td>\n",
       "      <td>30558.420</td>\n",
       "      <td>61116.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561590</th>\n",
       "      <td>2174624</td>\n",
       "      <td>327438</td>\n",
       "      <td>14</td>\n",
       "      <td>-557.0</td>\n",
       "      <td>-560.0</td>\n",
       "      <td>11657.880</td>\n",
       "      <td>23315.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632523</th>\n",
       "      <td>1377916</td>\n",
       "      <td>345775</td>\n",
       "      <td>5</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-95.0</td>\n",
       "      <td>35100.000</td>\n",
       "      <td>70200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641072</th>\n",
       "      <td>1669001</td>\n",
       "      <td>336526</td>\n",
       "      <td>2</td>\n",
       "      <td>-327.0</td>\n",
       "      <td>-340.0</td>\n",
       "      <td>19230.930</td>\n",
       "      <td>38461.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942034</th>\n",
       "      <td>1377916</td>\n",
       "      <td>345775</td>\n",
       "      <td>3</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-156.0</td>\n",
       "      <td>36292.500</td>\n",
       "      <td>72585.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133461</th>\n",
       "      <td>1061066</td>\n",
       "      <td>416815</td>\n",
       "      <td>4</td>\n",
       "      <td>-327.0</td>\n",
       "      <td>-339.0</td>\n",
       "      <td>19724.355</td>\n",
       "      <td>39448.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274944</th>\n",
       "      <td>2585538</td>\n",
       "      <td>436164</td>\n",
       "      <td>2</td>\n",
       "      <td>-599.0</td>\n",
       "      <td>-628.0</td>\n",
       "      <td>22236.840</td>\n",
       "      <td>44473.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SK_ID_PREV   LN_ID  INST_NUMBER  INST_DAYS  PAY_DAYS   AMT_INST  \\\n",
       "261880      1604926  164271            3     -388.0    -405.0   2610.000   \n",
       "556029      1853107  111076            4     -692.0    -719.0  30558.420   \n",
       "1561590     2174624  327438           14     -557.0    -560.0  11657.880   \n",
       "1632523     1377916  345775            5      -92.0     -95.0  35100.000   \n",
       "1641072     1669001  336526            2     -327.0    -340.0  19230.930   \n",
       "1942034     1377916  345775            3     -152.0    -156.0  36292.500   \n",
       "2133461     1061066  416815            4     -327.0    -339.0  19724.355   \n",
       "2274944     2585538  436164            2     -599.0    -628.0  22236.840   \n",
       "\n",
       "          AMT_PAY  \n",
       "261880    5220.00  \n",
       "556029   61116.84  \n",
       "1561590  23315.76  \n",
       "1632523  70200.00  \n",
       "1641072  38461.86  \n",
       "1942034  72585.00  \n",
       "2133461  39448.71  \n",
       "2274944  44473.68  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Duplicated value\n",
    "installment_payment[installment_payment.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated values\n",
    "installment_payment.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installment_payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>INST_NUMBER</th>\n",
       "      <th>INST_DAYS</th>\n",
       "      <th>PAY_DAYS</th>\n",
       "      <th>AMT_INST</th>\n",
       "      <th>AMT_PAY</th>\n",
       "      <th>INST-PAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1137312</td>\n",
       "      <td>164489</td>\n",
       "      <td>12</td>\n",
       "      <td>-1384.0</td>\n",
       "      <td>-1417.0</td>\n",
       "      <td>5970.375</td>\n",
       "      <td>5970.375</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2723183</td>\n",
       "      <td>112102</td>\n",
       "      <td>14</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>70.740</td>\n",
       "      <td>70.740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2558880</td>\n",
       "      <td>154793</td>\n",
       "      <td>8</td>\n",
       "      <td>-1262.0</td>\n",
       "      <td>-1269.0</td>\n",
       "      <td>15031.080</td>\n",
       "      <td>15031.080</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1410565</td>\n",
       "      <td>197687</td>\n",
       "      <td>1</td>\n",
       "      <td>-1037.0</td>\n",
       "      <td>-1048.0</td>\n",
       "      <td>12514.050</td>\n",
       "      <td>12510.450</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2391610</td>\n",
       "      <td>183431</td>\n",
       "      <td>20</td>\n",
       "      <td>-1680.0</td>\n",
       "      <td>-1693.0</td>\n",
       "      <td>7875.000</td>\n",
       "      <td>7875.000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV   LN_ID  INST_NUMBER  INST_DAYS  PAY_DAYS   AMT_INST    AMT_PAY  \\\n",
       "0     1137312  164489           12    -1384.0   -1417.0   5970.375   5970.375   \n",
       "1     2723183  112102           14     -197.0    -197.0     70.740     70.740   \n",
       "2     2558880  154793            8    -1262.0   -1269.0  15031.080  15031.080   \n",
       "3     1410565  197687            1    -1037.0   -1048.0  12514.050  12510.450   \n",
       "4     2391610  183431           20    -1680.0   -1693.0   7875.000   7875.000   \n",
       "\n",
       "   INST-PAY  \n",
       "0      33.0  \n",
       "1       0.0  \n",
       "2       7.0  \n",
       "3      11.0  \n",
       "4      13.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to know if the customer is a late payer or not (subtract installment days with pay days)\n",
    "installment_payment['INST-PAY'] = installment_payment.apply(lambda x: x['INST_DAYS']-x['PAY_DAYS'], axis=1)\n",
    "\n",
    "installment_payment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>INST_NUMBER</th>\n",
       "      <th>INST_DAYS</th>\n",
       "      <th>PAY_DAYS</th>\n",
       "      <th>AMT_INST</th>\n",
       "      <th>AMT_PAY</th>\n",
       "      <th>INST-PAY</th>\n",
       "      <th>LATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1137312</td>\n",
       "      <td>164489</td>\n",
       "      <td>12</td>\n",
       "      <td>-1384.0</td>\n",
       "      <td>-1417.0</td>\n",
       "      <td>5970.375</td>\n",
       "      <td>5970.375</td>\n",
       "      <td>33.0</td>\n",
       "      <td>EARLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2723183</td>\n",
       "      <td>112102</td>\n",
       "      <td>14</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>70.740</td>\n",
       "      <td>70.740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ON-TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2558880</td>\n",
       "      <td>154793</td>\n",
       "      <td>8</td>\n",
       "      <td>-1262.0</td>\n",
       "      <td>-1269.0</td>\n",
       "      <td>15031.080</td>\n",
       "      <td>15031.080</td>\n",
       "      <td>7.0</td>\n",
       "      <td>EARLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1410565</td>\n",
       "      <td>197687</td>\n",
       "      <td>1</td>\n",
       "      <td>-1037.0</td>\n",
       "      <td>-1048.0</td>\n",
       "      <td>12514.050</td>\n",
       "      <td>12510.450</td>\n",
       "      <td>11.0</td>\n",
       "      <td>EARLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2391610</td>\n",
       "      <td>183431</td>\n",
       "      <td>20</td>\n",
       "      <td>-1680.0</td>\n",
       "      <td>-1693.0</td>\n",
       "      <td>7875.000</td>\n",
       "      <td>7875.000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>EARLY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV   LN_ID  INST_NUMBER  INST_DAYS  PAY_DAYS   AMT_INST    AMT_PAY  \\\n",
       "0     1137312  164489           12    -1384.0   -1417.0   5970.375   5970.375   \n",
       "1     2723183  112102           14     -197.0    -197.0     70.740     70.740   \n",
       "2     2558880  154793            8    -1262.0   -1269.0  15031.080  15031.080   \n",
       "3     1410565  197687            1    -1037.0   -1048.0  12514.050  12510.450   \n",
       "4     2391610  183431           20    -1680.0   -1693.0   7875.000   7875.000   \n",
       "\n",
       "   INST-PAY     LATE  \n",
       "0      33.0    EARLY  \n",
       "1       0.0  ON-TIME  \n",
       "2       7.0    EARLY  \n",
       "3      11.0    EARLY  \n",
       "4      13.0    EARLY  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "installment_payment['LATE'] = installment_payment['INST-PAY'].apply(lambda x: 'LATE' if x < 0 else ('EARLY' if x > 0 else 'ON-TIME'))\n",
    "\n",
    "installment_payment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>Inst-Behav</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31909</th>\n",
       "      <td>175358</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93684</th>\n",
       "      <td>320713</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93691</th>\n",
       "      <td>320731</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53287</th>\n",
       "      <td>226310</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93706</th>\n",
       "      <td>320770</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45339</th>\n",
       "      <td>207078</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12188</th>\n",
       "      <td>129087</td>\n",
       "      <td>ON-TIME</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118644</th>\n",
       "      <td>378841</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20388</th>\n",
       "      <td>147820</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>106175</td>\n",
       "      <td>ON-TIME</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72301 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LN_ID Inst-Behav         %\n",
       "31909   175358      EARLY  1.000000\n",
       "93684   320713      EARLY  1.000000\n",
       "93691   320731      EARLY  1.000000\n",
       "53287   226310      EARLY  1.000000\n",
       "93706   320770      EARLY  1.000000\n",
       "...        ...        ...       ...\n",
       "45339   207078      EARLY  0.333333\n",
       "12188   129087    ON-TIME  0.333333\n",
       "118644  378841      EARLY  0.333333\n",
       "20388   147820      EARLY  0.333333\n",
       "2545    106175    ON-TIME  0.333333\n",
       "\n",
       "[72301 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new dataframe with information on customer's payment behaviour \n",
    "df_behav=installment_payment.groupby(['LN_ID'])['LATE'].value_counts(normalize=True).to_frame().rename({\"LATE\":\"%\"},axis=1).reset_index().sort_values('%', ascending=False).groupby('LN_ID').head(1).rename({\"LATE\":\"Inst-Behav\"},axis=1)\n",
    "\n",
    "df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prev_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>CONTRACT_TYPE</th>\n",
       "      <th>ANNUITY</th>\n",
       "      <th>APPLICATION</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>AMT_DOWN_PAYMENT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>WEEKDAYS_APPLY</th>\n",
       "      <th>HOUR_APPLY</th>\n",
       "      <th>CONTRACT_STATUS</th>\n",
       "      <th>DAYS_DECISION</th>\n",
       "      <th>TERM_PAYMENT</th>\n",
       "      <th>YIELD_GROUP</th>\n",
       "      <th>FIRST_DRAW</th>\n",
       "      <th>FIRST_DUE</th>\n",
       "      <th>TERMINATION</th>\n",
       "      <th>NFLAG_INSURED_ON_APPROVAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030495</td>\n",
       "      <td>271877</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>1730.430</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>15</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2819243</td>\n",
       "      <td>176158</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>47041.335</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>470790.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-512</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-482.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1383531</td>\n",
       "      <td>199383</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>23703.930</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>340573.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>8</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-684</td>\n",
       "      <td>18.0</td>\n",
       "      <td>low_normal</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-654.0</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2315218</td>\n",
       "      <td>175704</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1715995</td>\n",
       "      <td>447712</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>11368.620</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>335754.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-735</td>\n",
       "      <td>54.0</td>\n",
       "      <td>low_normal</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-705.0</td>\n",
       "      <td>-334.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350707</th>\n",
       "      <td>1379569</td>\n",
       "      <td>309506</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>33389.100</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Refused</td>\n",
       "      <td>-156</td>\n",
       "      <td>60.0</td>\n",
       "      <td>low_normal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350708</th>\n",
       "      <td>1252861</td>\n",
       "      <td>363895</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>56754.000</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>Refused</td>\n",
       "      <td>-847</td>\n",
       "      <td>48.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350709</th>\n",
       "      <td>1379406</td>\n",
       "      <td>302265</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>-502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350710</th>\n",
       "      <td>2698899</td>\n",
       "      <td>161204</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>-359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NA1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350711</th>\n",
       "      <td>1024416</td>\n",
       "      <td>331038</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>7576.920</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>112068.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-2439</td>\n",
       "      <td>24.0</td>\n",
       "      <td>high</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-2409.0</td>\n",
       "      <td>-1716.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350712 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_PREV   LN_ID   CONTRACT_TYPE    ANNUITY  APPLICATION  \\\n",
       "0          2030495  271877  Consumer loans   1730.430      17145.0   \n",
       "1          2819243  176158      Cash loans  47041.335     450000.0   \n",
       "2          1383531  199383      Cash loans  23703.930     315000.0   \n",
       "3          2315218  175704      Cash loans      0.000          0.0   \n",
       "4          1715995  447712      Cash loans  11368.620     270000.0   \n",
       "...            ...     ...             ...        ...          ...   \n",
       "350707     1379569  309506      Cash loans  33389.100    1035000.0   \n",
       "350708     1252861  363895      Cash loans  56754.000    1350000.0   \n",
       "350709     1379406  302265      Cash loans      0.000          0.0   \n",
       "350710     2698899  161204      Cash loans      0.000          0.0   \n",
       "350711     1024416  331038      Cash loans   7576.920      99000.0   \n",
       "\n",
       "        APPROVED_CREDIT  AMT_DOWN_PAYMENT      PRICE WEEKDAYS_APPLY  \\\n",
       "0               17145.0               0.0    17145.0       SATURDAY   \n",
       "1              470790.0               0.0   450000.0         MONDAY   \n",
       "2              340573.5               0.0   315000.0       SATURDAY   \n",
       "3                   0.0               0.0        0.0        TUESDAY   \n",
       "4              335754.0               0.0   270000.0         FRIDAY   \n",
       "...                 ...               ...        ...            ...   \n",
       "350707        1035000.0               0.0  1035000.0       THURSDAY   \n",
       "350708        1350000.0               0.0  1350000.0       THURSDAY   \n",
       "350709              0.0               0.0        0.0      WEDNESDAY   \n",
       "350710              0.0               0.0        0.0       SATURDAY   \n",
       "350711         112068.0               0.0    99000.0         FRIDAY   \n",
       "\n",
       "        HOUR_APPLY CONTRACT_STATUS  DAYS_DECISION  TERM_PAYMENT YIELD_GROUP  \\\n",
       "0               15        Approved            -73          12.0      middle   \n",
       "1                7        Approved           -512          12.0      middle   \n",
       "2                8        Approved           -684          18.0  low_normal   \n",
       "3               11        Canceled            -14           0.0         NA1   \n",
       "4                7        Approved           -735          54.0  low_normal   \n",
       "...            ...             ...            ...           ...         ...   \n",
       "350707          10         Refused           -156          60.0  low_normal   \n",
       "350708          16         Refused           -847          48.0      middle   \n",
       "350709          13        Canceled           -502           0.0         NA1   \n",
       "350710          13        Canceled           -359           0.0         NA1   \n",
       "350711          14        Approved          -2439          24.0        high   \n",
       "\n",
       "        FIRST_DRAW  FIRST_DUE  TERMINATION  NFLAG_INSURED_ON_APPROVAL  \n",
       "0         365243.0      -42.0        -37.0                        0.0  \n",
       "1         365243.0     -482.0       -177.0                        1.0  \n",
       "2         365243.0     -654.0       -137.0                        1.0  \n",
       "3              0.0        0.0          0.0                        0.0  \n",
       "4         365243.0     -705.0       -334.0                        1.0  \n",
       "...            ...        ...          ...                        ...  \n",
       "350707         0.0        0.0          0.0                        0.0  \n",
       "350708         0.0        0.0          0.0                        0.0  \n",
       "350709         0.0        0.0          0.0                        0.0  \n",
       "350710         0.0        0.0          0.0                        0.0  \n",
       "350711    365243.0    -2409.0      -1716.0                        1.0  \n",
       "\n",
       "[350712 rows x 18 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>CONTRACT_STATUS</th>\n",
       "      <th>Approval Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100009</td>\n",
       "      <td>Approved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100039</td>\n",
       "      <td>Approved</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100039</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100044</td>\n",
       "      <td>Approved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100045</td>\n",
       "      <td>Approved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129103</th>\n",
       "      <td>456249</td>\n",
       "      <td>Approved</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129104</th>\n",
       "      <td>456249</td>\n",
       "      <td>Refused</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129105</th>\n",
       "      <td>456252</td>\n",
       "      <td>Approved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129106</th>\n",
       "      <td>456255</td>\n",
       "      <td>Approved</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129107</th>\n",
       "      <td>456255</td>\n",
       "      <td>Refused</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LN_ID CONTRACT_STATUS  Approval Rate\n",
       "0       100009        Approved            1.0\n",
       "1       100039        Approved            0.5\n",
       "2       100039        Canceled            0.5\n",
       "3       100044        Approved            1.0\n",
       "4       100045        Approved            1.0\n",
       "...        ...             ...            ...\n",
       "129103  456249        Approved            0.5\n",
       "129104  456249         Refused            0.5\n",
       "129105  456252        Approved            1.0\n",
       "129106  456255        Approved            0.5\n",
       "129107  456255         Refused            0.5\n",
       "\n",
       "[129108 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to know if the approval rate of the loan application\n",
    "df_attempt=prev_app.groupby(['LN_ID','CONTRACT_STATUS'],as_index=False).count()[['LN_ID','SK_ID_PREV','CONTRACT_STATUS']].rename({'SK_ID_PREV':'Loan Application Attempt'},axis=1).groupby('LN_ID')['CONTRACT_STATUS'].value_counts(normalize=True).to_frame().rename({\"CONTRACT_STATUS\":\"Approval Rate\"},axis=1).reset_index()\n",
    "\n",
    "df_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>Approval Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100009</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100039</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100044</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100045</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100046</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129100</th>\n",
       "      <td>456247</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129102</th>\n",
       "      <td>456248</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129103</th>\n",
       "      <td>456249</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129105</th>\n",
       "      <td>456252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129106</th>\n",
       "      <td>456255</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71917 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LN_ID  Approval Rate\n",
       "0       100009            1.0\n",
       "1       100039            0.5\n",
       "3       100044            1.0\n",
       "4       100045            1.0\n",
       "5       100046            0.5\n",
       "...        ...            ...\n",
       "129100  456247            0.5\n",
       "129102  456248            1.0\n",
       "129103  456249            0.5\n",
       "129105  456252            1.0\n",
       "129106  456255            0.5\n",
       "\n",
       "[71917 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only the approved loan\n",
    "df_attempt2=df_attempt[df_attempt['CONTRACT_STATUS']==\"Approved\"][['LN_ID',\"Approval Rate\"]]\n",
    "df_attempt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>Inst-Behav</th>\n",
       "      <th>Approval Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175358</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320713</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>320731</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226310</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320770</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72398</th>\n",
       "      <td>437729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72399</th>\n",
       "      <td>440618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72400</th>\n",
       "      <td>443497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72401</th>\n",
       "      <td>445708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72402</th>\n",
       "      <td>452635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72403 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LN_ID Inst-Behav  Approval Rate\n",
       "0      175358      EARLY            0.5\n",
       "1      320713      EARLY            0.5\n",
       "2      320731      EARLY            1.0\n",
       "3      226310      EARLY            1.0\n",
       "4      320770      EARLY            0.5\n",
       "...       ...        ...            ...\n",
       "72398  437729        NaN            0.5\n",
       "72399  440618        NaN            1.0\n",
       "72400  443497        NaN            0.5\n",
       "72401  445708        NaN            0.5\n",
       "72402  452635        NaN            0.5\n",
       "\n",
       "[72403 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features=pd.merge(df_behav,df_attempt2,how=\"outer\").drop('%',axis=1)\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>Inst-Behav</th>\n",
       "      <th>Approval Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175358</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320713</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>320731</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226310</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320770</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72398</th>\n",
       "      <td>437729</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72399</th>\n",
       "      <td>440618</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72400</th>\n",
       "      <td>443497</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72401</th>\n",
       "      <td>445708</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72402</th>\n",
       "      <td>452635</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72403 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LN_ID Inst-Behav  Approval Rate\n",
       "0      175358      EARLY            0.5\n",
       "1      320713      EARLY            0.5\n",
       "2      320731      EARLY            1.0\n",
       "3      226310      EARLY            1.0\n",
       "4      320770      EARLY            0.5\n",
       "...       ...        ...            ...\n",
       "72398  437729    Unknown            0.5\n",
       "72399  440618    Unknown            1.0\n",
       "72400  443497    Unknown            0.5\n",
       "72401  445708    Unknown            0.5\n",
       "72402  452635    Unknown            0.5\n",
       "\n",
       "[72403 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill the missing values\n",
    "df_features['Inst-Behav']=df_features['Inst-Behav'].fillna(\"Unknown\")\n",
    "df_features['Approval Rate']=df_features['Approval Rate'].fillna(0)\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the app_test dataframe\n",
    "app_test_new=pd.merge(app_test,df_features,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the app_train dataframe\n",
    "app_train_new=pd.merge(app_train,df_features,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values\n",
    "app_test_new['Inst-Behav']=app_test_new['Inst-Behav'].fillna(\"Unknown\")\n",
    "app_test_new['Approval Rate']=app_test_new['Approval Rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values\n",
    "app_train_new['Inst-Behav']=app_train_new['Inst-Behav'].fillna(\"Unknown\")\n",
    "app_train_new['Approval Rate']=app_train_new['Approval Rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LN_ID                0\n",
       "TARGET               0\n",
       "CONTRACT_TYPE        0\n",
       "GENDER               0\n",
       "NUM_CHILDREN         0\n",
       "INCOME               0\n",
       "APPROVED_CREDIT      0\n",
       "ANNUITY              0\n",
       "PRICE                0\n",
       "INCOME_TYPE          0\n",
       "EDUCATION            0\n",
       "FAMILY_STATUS        0\n",
       "HOUSING_TYPE         0\n",
       "DAYS_AGE             0\n",
       "DAYS_WORK            0\n",
       "DAYS_REGISTRATION    0\n",
       "DAYS_ID_CHANGE       0\n",
       "WEEKDAYS_APPLY       0\n",
       "HOUR_APPLY           0\n",
       "ORGANIZATION_TYPE    0\n",
       "YEAR_AGE             0\n",
       "Inst-Behav           0\n",
       "Approval Rate        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check missing values\n",
    "app_train_new.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns (DAYS_AGE & ORGANIZATION_TYPE)\n",
    "\n",
    "app_train_new = app_train_new.drop(['DAYS_AGE','ORGANIZATION_TYPE'], axis=1)\n",
    "app_test_new = app_test_new.drop(['DAYS_AGE','ORGANIZATION_TYPE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LN_ID</td>\n",
       "      <td>7.209276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM_CHILDREN</td>\n",
       "      <td>1.466597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>2.981305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPROVED_CREDIT</td>\n",
       "      <td>122.948047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANNUITY</td>\n",
       "      <td>11.686279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PRICE</td>\n",
       "      <td>121.294110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_WORK</td>\n",
       "      <td>1.914695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>3.386711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DAYS_ID_CHANGE</td>\n",
       "      <td>5.421491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOUR_APPLY</td>\n",
       "      <td>10.614008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>YEAR_AGE</td>\n",
       "      <td>18.218616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Approval Rate</td>\n",
       "      <td>4.621188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             variable         VIF\n",
       "0               LN_ID    7.209276\n",
       "1        NUM_CHILDREN    1.466597\n",
       "2              INCOME    2.981305\n",
       "3     APPROVED_CREDIT  122.948047\n",
       "4             ANNUITY   11.686279\n",
       "5               PRICE  121.294110\n",
       "6           DAYS_WORK    1.914695\n",
       "7   DAYS_REGISTRATION    3.386711\n",
       "8      DAYS_ID_CHANGE    5.421491\n",
       "9          HOUR_APPLY   10.614008\n",
       "10           YEAR_AGE   18.218616\n",
       "11      Approval Rate    4.621188"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the multicolinearity with threshold 7\n",
    "def calc_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['variable'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n",
    "    return vif\n",
    "\n",
    "calc_vif(app_train_new.select_dtypes(include=['int','float']).drop(columns='TARGET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LN_ID</td>\n",
       "      <td>7.175674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM_CHILDREN</td>\n",
       "      <td>1.461636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>2.819254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPROVED_CREDIT</td>\n",
       "      <td>3.534353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAYS_WORK</td>\n",
       "      <td>1.900633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>3.384952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_ID_CHANGE</td>\n",
       "      <td>5.417841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOUR_APPLY</td>\n",
       "      <td>10.499628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YEAR_AGE</td>\n",
       "      <td>18.060556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Approval Rate</td>\n",
       "      <td>4.600492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable        VIF\n",
       "0              LN_ID   7.175674\n",
       "1       NUM_CHILDREN   1.461636\n",
       "2             INCOME   2.819254\n",
       "3    APPROVED_CREDIT   3.534353\n",
       "4          DAYS_WORK   1.900633\n",
       "5  DAYS_REGISTRATION   3.384952\n",
       "6     DAYS_ID_CHANGE   5.417841\n",
       "7         HOUR_APPLY  10.499628\n",
       "8           YEAR_AGE  18.060556\n",
       "9      Approval Rate   4.600492"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop column price\n",
    "\n",
    "app_train_new = app_train_new.drop(['PRICE', 'ANNUITY'], axis=1)\n",
    "app_test_new = app_test_new.drop(['PRICE','ANNUITY'], axis=1)\n",
    "\n",
    "calc_vif(app_train_new.select_dtypes(include=['int','float']).drop(columns='TARGET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HOUR_APPLY to category by binning\n",
    "\n",
    "bins = [0, 5, 13, 17, 25]\n",
    "labels=['Night','Morning','Afternoon','Evening']\n",
    "app_train_new['HOUR_BIN'] = pd.cut(app_train_new['HOUR_APPLY'], bins, labels=labels, include_lowest=True)\n",
    "app_test_new['HOUR_BIN'] = pd.cut(app_test_new['HOUR_APPLY'], bins, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LN_ID</td>\n",
       "      <td>6.722274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM_CHILDREN</td>\n",
       "      <td>1.445216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>2.768648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPROVED_CREDIT</td>\n",
       "      <td>3.504690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAYS_WORK</td>\n",
       "      <td>1.804655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>3.369142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_ID_CHANGE</td>\n",
       "      <td>5.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YEAR_AGE</td>\n",
       "      <td>15.606050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Approval Rate</td>\n",
       "      <td>4.462801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable        VIF\n",
       "0              LN_ID   6.722274\n",
       "1       NUM_CHILDREN   1.445216\n",
       "2             INCOME   2.768648\n",
       "3    APPROVED_CREDIT   3.504690\n",
       "4          DAYS_WORK   1.804655\n",
       "5  DAYS_REGISTRATION   3.369142\n",
       "6     DAYS_ID_CHANGE   5.364100\n",
       "7           YEAR_AGE  15.606050\n",
       "8      Approval Rate   4.462801"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As its still above 7 we will drop column HOUR_APPLY\n",
    "app_train_new = app_train_new.drop(['HOUR_APPLY'], axis=1)\n",
    "app_test_new = app_test_new.drop(['HOUR_APPLY'], axis=1)\n",
    "\n",
    "calc_vif(app_train_new.select_dtypes(include=['int','float']).drop(columns='TARGET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LN_ID</td>\n",
       "      <td>5.515272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUM_CHILDREN</td>\n",
       "      <td>1.433599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>2.700096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPROVED_CREDIT</td>\n",
       "      <td>3.286591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAYS_WORK</td>\n",
       "      <td>1.450256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAYS_REGISTRATION</td>\n",
       "      <td>2.946103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAYS_ID_CHANGE</td>\n",
       "      <td>4.655571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Approval Rate</td>\n",
       "      <td>4.054729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            variable       VIF\n",
       "0              LN_ID  5.515272\n",
       "1       NUM_CHILDREN  1.433599\n",
       "2             INCOME  2.700096\n",
       "3    APPROVED_CREDIT  3.286591\n",
       "4          DAYS_WORK  1.450256\n",
       "5  DAYS_REGISTRATION  2.946103\n",
       "6     DAYS_ID_CHANGE  4.655571\n",
       "7      Approval Rate  4.054729"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop column YEAR_AGE\n",
    "app_train_new = app_train_new.drop(['YEAR_AGE'], axis=1)\n",
    "app_test_new = app_test_new.drop(['YEAR_AGE'], axis=1)\n",
    "\n",
    "calc_vif(app_train_new.select_dtypes(include=['int','float']).drop(columns='TARGET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Last check with heat map\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Kita buat mask jadi yang terlihat hanya bagian bawah\n",
    "mask = np.triu(np.ones_like(app_train_new.drop(columns='TARGET').corr(),dtype=bool))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(app_train_new.drop(columns='TARGET').corr(), annot=True, mask=mask, vmin=-1, vmax=1)\n",
    "plt.title('Features Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONTRACT_TYPE', 'GENDER', 'INCOME_TYPE', 'EDUCATION', 'FAMILY_STATUS',\n",
       "       'HOUSING_TYPE', 'WEEKDAYS_APPLY', 'Inst-Behav', 'HOUR_BIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable for categorical variables\n",
    "cat_var = app_train_new.select_dtypes(['category', 'object']).columns\n",
    "\n",
    "cat_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LN_ID', 'TARGET', 'NUM_CHILDREN', 'INCOME', 'APPROVED_CREDIT',\n",
       "       'DAYS_WORK', 'DAYS_REGISTRATION', 'DAYS_ID_CHANGE', 'Approval Rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable for numerical variables\n",
    "con_var = app_train_new.select_dtypes(exclude=['category','object']).columns\n",
    "\n",
    "con_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer\n",
    "transformer = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(drop='first'), cat_var)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# Define X and y\n",
    "X_train = app_train_new.drop(['TARGET'], axis=1)\n",
    "y_train = app_train_new['TARGET']\n",
    "\n",
    "X_test = app_test_new.drop(['TARGET'],axis=1)\n",
    "y_test = app_test_new['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if imbalanced\n",
    "app_train_new['TARGET'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0)\n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "adb = AdaBoostClassifier(random_state=0)\n",
    "xgb = XGBClassifier(random_state=0)\n",
    "cat = CatBoostClassifier(random_state=0)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "smote = SMOTE(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this case, the positive value is 1 (which means that the customer will be a late payer).\n",
    "Thus we will focused on the positive and the false positive (customer falsely categorized as a late payer), as we will lose potential customer if its higher\n",
    "\n",
    "We will use **RECALL** for our metrics because we want to decrease the false positive and focused on detecting the true positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Learning rate set to 0.070522\n",
      "0:\tlearn: 0.6537099\ttotal: 154ms\tremaining: 2m 33s\n",
      "1:\tlearn: 0.6142901\ttotal: 168ms\tremaining: 1m 23s\n",
      "2:\tlearn: 0.5920026\ttotal: 184ms\tremaining: 1m 1s\n",
      "3:\tlearn: 0.5722499\ttotal: 198ms\tremaining: 49.4s\n",
      "4:\tlearn: 0.5575401\ttotal: 212ms\tremaining: 42.2s\n",
      "5:\tlearn: 0.5263043\ttotal: 228ms\tremaining: 37.7s\n",
      "6:\tlearn: 0.5120121\ttotal: 244ms\tremaining: 34.6s\n",
      "7:\tlearn: 0.5012446\ttotal: 257ms\tremaining: 31.8s\n",
      "8:\tlearn: 0.4805508\ttotal: 270ms\tremaining: 29.7s\n",
      "9:\tlearn: 0.4729978\ttotal: 283ms\tremaining: 28s\n",
      "10:\tlearn: 0.4634692\ttotal: 296ms\tremaining: 26.6s\n",
      "11:\tlearn: 0.4579865\ttotal: 309ms\tremaining: 25.4s\n",
      "12:\tlearn: 0.4536286\ttotal: 321ms\tremaining: 24.4s\n",
      "13:\tlearn: 0.4489160\ttotal: 334ms\tremaining: 23.5s\n",
      "14:\tlearn: 0.4369769\ttotal: 349ms\tremaining: 22.9s\n",
      "15:\tlearn: 0.4323871\ttotal: 363ms\tremaining: 22.3s\n",
      "16:\tlearn: 0.4209827\ttotal: 375ms\tremaining: 21.7s\n",
      "17:\tlearn: 0.4170639\ttotal: 389ms\tremaining: 21.2s\n",
      "18:\tlearn: 0.4145382\ttotal: 402ms\tremaining: 20.7s\n",
      "19:\tlearn: 0.4055426\ttotal: 417ms\tremaining: 20.4s\n",
      "20:\tlearn: 0.4007134\ttotal: 432ms\tremaining: 20.1s\n",
      "21:\tlearn: 0.3984640\ttotal: 448ms\tremaining: 19.9s\n",
      "22:\tlearn: 0.3938398\ttotal: 463ms\tremaining: 19.7s\n",
      "23:\tlearn: 0.3920617\ttotal: 481ms\tremaining: 19.6s\n",
      "24:\tlearn: 0.3869148\ttotal: 493ms\tremaining: 19.2s\n",
      "25:\tlearn: 0.3843972\ttotal: 505ms\tremaining: 18.9s\n",
      "26:\tlearn: 0.3828830\ttotal: 518ms\tremaining: 18.7s\n",
      "27:\tlearn: 0.3794627\ttotal: 531ms\tremaining: 18.4s\n",
      "28:\tlearn: 0.3761656\ttotal: 543ms\tremaining: 18.2s\n",
      "29:\tlearn: 0.3706399\ttotal: 557ms\tremaining: 18s\n",
      "30:\tlearn: 0.3654471\ttotal: 569ms\tremaining: 17.8s\n",
      "31:\tlearn: 0.3642640\ttotal: 583ms\tremaining: 17.6s\n",
      "32:\tlearn: 0.3594051\ttotal: 596ms\tremaining: 17.5s\n",
      "33:\tlearn: 0.3510954\ttotal: 611ms\tremaining: 17.4s\n",
      "34:\tlearn: 0.3469877\ttotal: 625ms\tremaining: 17.2s\n",
      "35:\tlearn: 0.3445874\ttotal: 640ms\tremaining: 17.1s\n",
      "36:\tlearn: 0.3426880\ttotal: 654ms\tremaining: 17s\n",
      "37:\tlearn: 0.3381378\ttotal: 668ms\tremaining: 16.9s\n",
      "38:\tlearn: 0.3360550\ttotal: 682ms\tremaining: 16.8s\n",
      "39:\tlearn: 0.3344912\ttotal: 694ms\tremaining: 16.7s\n",
      "40:\tlearn: 0.3328166\ttotal: 707ms\tremaining: 16.5s\n",
      "41:\tlearn: 0.3284843\ttotal: 720ms\tremaining: 16.4s\n",
      "42:\tlearn: 0.3251526\ttotal: 734ms\tremaining: 16.3s\n",
      "43:\tlearn: 0.3200011\ttotal: 747ms\tremaining: 16.2s\n",
      "44:\tlearn: 0.3177238\ttotal: 761ms\tremaining: 16.2s\n",
      "45:\tlearn: 0.3118963\ttotal: 775ms\tremaining: 16.1s\n",
      "46:\tlearn: 0.3107646\ttotal: 787ms\tremaining: 16s\n",
      "47:\tlearn: 0.3100254\ttotal: 801ms\tremaining: 15.9s\n",
      "48:\tlearn: 0.3082913\ttotal: 815ms\tremaining: 15.8s\n",
      "49:\tlearn: 0.3060468\ttotal: 829ms\tremaining: 15.8s\n",
      "50:\tlearn: 0.3045239\ttotal: 843ms\tremaining: 15.7s\n",
      "51:\tlearn: 0.3017120\ttotal: 856ms\tremaining: 15.6s\n",
      "52:\tlearn: 0.2980749\ttotal: 869ms\tremaining: 15.5s\n",
      "53:\tlearn: 0.2968196\ttotal: 883ms\tremaining: 15.5s\n",
      "54:\tlearn: 0.2944438\ttotal: 896ms\tremaining: 15.4s\n",
      "55:\tlearn: 0.2934167\ttotal: 909ms\tremaining: 15.3s\n",
      "56:\tlearn: 0.2911311\ttotal: 921ms\tremaining: 15.2s\n",
      "57:\tlearn: 0.2893119\ttotal: 934ms\tremaining: 15.2s\n",
      "58:\tlearn: 0.2876640\ttotal: 947ms\tremaining: 15.1s\n",
      "59:\tlearn: 0.2846345\ttotal: 960ms\tremaining: 15s\n",
      "60:\tlearn: 0.2825833\ttotal: 974ms\tremaining: 15s\n",
      "61:\tlearn: 0.2788854\ttotal: 989ms\tremaining: 15s\n",
      "62:\tlearn: 0.2784096\ttotal: 1s\tremaining: 14.9s\n",
      "63:\tlearn: 0.2775982\ttotal: 1.02s\tremaining: 14.9s\n",
      "64:\tlearn: 0.2765312\ttotal: 1.03s\tremaining: 14.8s\n",
      "65:\tlearn: 0.2757563\ttotal: 1.04s\tremaining: 14.8s\n",
      "66:\tlearn: 0.2748100\ttotal: 1.06s\tremaining: 14.8s\n",
      "67:\tlearn: 0.2741747\ttotal: 1.08s\tremaining: 14.7s\n",
      "68:\tlearn: 0.2738117\ttotal: 1.09s\tremaining: 14.7s\n",
      "69:\tlearn: 0.2717067\ttotal: 1.1s\tremaining: 14.6s\n",
      "70:\tlearn: 0.2706502\ttotal: 1.11s\tremaining: 14.6s\n",
      "71:\tlearn: 0.2703212\ttotal: 1.13s\tremaining: 14.5s\n",
      "72:\tlearn: 0.2697275\ttotal: 1.14s\tremaining: 14.5s\n",
      "73:\tlearn: 0.2685456\ttotal: 1.15s\tremaining: 14.4s\n",
      "74:\tlearn: 0.2673529\ttotal: 1.16s\tremaining: 14.4s\n",
      "75:\tlearn: 0.2664267\ttotal: 1.18s\tremaining: 14.3s\n",
      "76:\tlearn: 0.2660135\ttotal: 1.19s\tremaining: 14.3s\n",
      "77:\tlearn: 0.2651526\ttotal: 1.2s\tremaining: 14.2s\n",
      "78:\tlearn: 0.2648040\ttotal: 1.22s\tremaining: 14.2s\n",
      "79:\tlearn: 0.2637019\ttotal: 1.23s\tremaining: 14.2s\n",
      "80:\tlearn: 0.2632740\ttotal: 1.24s\tremaining: 14.1s\n",
      "81:\tlearn: 0.2623194\ttotal: 1.26s\tremaining: 14.1s\n",
      "82:\tlearn: 0.2620150\ttotal: 1.27s\tremaining: 14s\n",
      "83:\tlearn: 0.2604628\ttotal: 1.28s\tremaining: 14s\n",
      "84:\tlearn: 0.2584962\ttotal: 1.3s\tremaining: 14s\n",
      "85:\tlearn: 0.2580166\ttotal: 1.31s\tremaining: 13.9s\n",
      "86:\tlearn: 0.2557015\ttotal: 1.32s\tremaining: 13.9s\n",
      "87:\tlearn: 0.2543868\ttotal: 1.33s\tremaining: 13.8s\n",
      "88:\tlearn: 0.2541624\ttotal: 1.35s\tremaining: 13.8s\n",
      "89:\tlearn: 0.2534821\ttotal: 1.36s\tremaining: 13.8s\n",
      "90:\tlearn: 0.2523914\ttotal: 1.37s\tremaining: 13.7s\n",
      "91:\tlearn: 0.2521104\ttotal: 1.39s\tremaining: 13.7s\n",
      "92:\tlearn: 0.2517758\ttotal: 1.4s\tremaining: 13.7s\n",
      "93:\tlearn: 0.2512771\ttotal: 1.41s\tremaining: 13.6s\n",
      "94:\tlearn: 0.2507718\ttotal: 1.43s\tremaining: 13.6s\n",
      "95:\tlearn: 0.2498338\ttotal: 1.44s\tremaining: 13.6s\n",
      "96:\tlearn: 0.2490466\ttotal: 1.46s\tremaining: 13.6s\n",
      "97:\tlearn: 0.2482160\ttotal: 1.47s\tremaining: 13.5s\n",
      "98:\tlearn: 0.2480244\ttotal: 1.49s\tremaining: 13.5s\n",
      "99:\tlearn: 0.2473406\ttotal: 1.5s\tremaining: 13.5s\n",
      "100:\tlearn: 0.2459857\ttotal: 1.51s\tremaining: 13.5s\n",
      "101:\tlearn: 0.2457890\ttotal: 1.52s\tremaining: 13.4s\n",
      "102:\tlearn: 0.2454101\ttotal: 1.54s\tremaining: 13.4s\n",
      "103:\tlearn: 0.2451723\ttotal: 1.55s\tremaining: 13.4s\n",
      "104:\tlearn: 0.2449662\ttotal: 1.56s\tremaining: 13.3s\n",
      "105:\tlearn: 0.2446067\ttotal: 1.58s\tremaining: 13.3s\n",
      "106:\tlearn: 0.2439094\ttotal: 1.59s\tremaining: 13.3s\n",
      "107:\tlearn: 0.2433470\ttotal: 1.6s\tremaining: 13.2s\n",
      "108:\tlearn: 0.2427467\ttotal: 1.61s\tremaining: 13.2s\n",
      "109:\tlearn: 0.2422319\ttotal: 1.63s\tremaining: 13.2s\n",
      "110:\tlearn: 0.2416435\ttotal: 1.65s\tremaining: 13.2s\n",
      "111:\tlearn: 0.2411867\ttotal: 1.66s\tremaining: 13.2s\n",
      "112:\tlearn: 0.2409706\ttotal: 1.68s\tremaining: 13.1s\n",
      "113:\tlearn: 0.2407924\ttotal: 1.69s\tremaining: 13.1s\n",
      "114:\tlearn: 0.2399606\ttotal: 1.7s\tremaining: 13.1s\n",
      "115:\tlearn: 0.2391987\ttotal: 1.71s\tremaining: 13.1s\n",
      "116:\tlearn: 0.2388144\ttotal: 1.73s\tremaining: 13s\n",
      "117:\tlearn: 0.2385522\ttotal: 1.74s\tremaining: 13s\n",
      "118:\tlearn: 0.2381267\ttotal: 1.75s\tremaining: 13s\n",
      "119:\tlearn: 0.2375906\ttotal: 1.77s\tremaining: 13s\n",
      "120:\tlearn: 0.2374233\ttotal: 1.78s\tremaining: 12.9s\n",
      "121:\tlearn: 0.2373037\ttotal: 1.79s\tremaining: 12.9s\n",
      "122:\tlearn: 0.2368526\ttotal: 1.8s\tremaining: 12.9s\n",
      "123:\tlearn: 0.2366603\ttotal: 1.82s\tremaining: 12.9s\n",
      "124:\tlearn: 0.2365195\ttotal: 1.83s\tremaining: 12.8s\n",
      "125:\tlearn: 0.2358086\ttotal: 1.85s\tremaining: 12.8s\n",
      "126:\tlearn: 0.2354652\ttotal: 1.86s\tremaining: 12.8s\n",
      "127:\tlearn: 0.2353195\ttotal: 1.88s\tremaining: 12.8s\n",
      "128:\tlearn: 0.2351417\ttotal: 1.89s\tremaining: 12.8s\n",
      "129:\tlearn: 0.2344151\ttotal: 1.9s\tremaining: 12.7s\n",
      "130:\tlearn: 0.2342319\ttotal: 1.92s\tremaining: 12.7s\n",
      "131:\tlearn: 0.2337637\ttotal: 1.93s\tremaining: 12.7s\n",
      "132:\tlearn: 0.2335679\ttotal: 1.94s\tremaining: 12.7s\n",
      "133:\tlearn: 0.2332116\ttotal: 1.96s\tremaining: 12.6s\n",
      "134:\tlearn: 0.2321525\ttotal: 1.97s\tremaining: 12.6s\n",
      "135:\tlearn: 0.2320602\ttotal: 1.98s\tremaining: 12.6s\n",
      "136:\tlearn: 0.2316963\ttotal: 1.99s\tremaining: 12.6s\n",
      "137:\tlearn: 0.2309980\ttotal: 2.01s\tremaining: 12.5s\n",
      "138:\tlearn: 0.2308589\ttotal: 2.02s\tremaining: 12.5s\n",
      "139:\tlearn: 0.2306808\ttotal: 2.04s\tremaining: 12.5s\n",
      "140:\tlearn: 0.2302461\ttotal: 2.05s\tremaining: 12.5s\n",
      "141:\tlearn: 0.2301197\ttotal: 2.07s\tremaining: 12.5s\n",
      "142:\tlearn: 0.2300136\ttotal: 2.08s\tremaining: 12.5s\n",
      "143:\tlearn: 0.2299125\ttotal: 2.09s\tremaining: 12.4s\n",
      "144:\tlearn: 0.2291132\ttotal: 2.1s\tremaining: 12.4s\n",
      "145:\tlearn: 0.2286384\ttotal: 2.12s\tremaining: 12.4s\n",
      "146:\tlearn: 0.2285371\ttotal: 2.13s\tremaining: 12.4s\n",
      "147:\tlearn: 0.2283235\ttotal: 2.14s\tremaining: 12.3s\n",
      "148:\tlearn: 0.2279221\ttotal: 2.16s\tremaining: 12.3s\n",
      "149:\tlearn: 0.2276882\ttotal: 2.17s\tremaining: 12.3s\n",
      "150:\tlearn: 0.2269997\ttotal: 2.19s\tremaining: 12.3s\n",
      "151:\tlearn: 0.2268608\ttotal: 2.2s\tremaining: 12.3s\n",
      "152:\tlearn: 0.2267233\ttotal: 2.21s\tremaining: 12.3s\n",
      "153:\tlearn: 0.2264133\ttotal: 2.23s\tremaining: 12.2s\n",
      "154:\tlearn: 0.2258014\ttotal: 2.24s\tremaining: 12.2s\n",
      "155:\tlearn: 0.2252243\ttotal: 2.26s\tremaining: 12.2s\n",
      "156:\tlearn: 0.2242604\ttotal: 2.27s\tremaining: 12.2s\n",
      "157:\tlearn: 0.2239277\ttotal: 2.28s\tremaining: 12.2s\n",
      "158:\tlearn: 0.2237731\ttotal: 2.3s\tremaining: 12.1s\n",
      "159:\tlearn: 0.2231490\ttotal: 2.31s\tremaining: 12.1s\n",
      "160:\tlearn: 0.2230434\ttotal: 2.32s\tremaining: 12.1s\n",
      "161:\tlearn: 0.2225231\ttotal: 2.33s\tremaining: 12.1s\n",
      "162:\tlearn: 0.2218560\ttotal: 2.35s\tremaining: 12.1s\n",
      "163:\tlearn: 0.2215995\ttotal: 2.36s\tremaining: 12s\n",
      "164:\tlearn: 0.2212393\ttotal: 2.37s\tremaining: 12s\n",
      "165:\tlearn: 0.2210264\ttotal: 2.39s\tremaining: 12s\n",
      "166:\tlearn: 0.2208375\ttotal: 2.4s\tremaining: 12s\n",
      "167:\tlearn: 0.2204781\ttotal: 2.41s\tremaining: 12s\n",
      "168:\tlearn: 0.2199382\ttotal: 2.43s\tremaining: 11.9s\n",
      "169:\tlearn: 0.2196557\ttotal: 2.44s\tremaining: 11.9s\n",
      "170:\tlearn: 0.2193277\ttotal: 2.46s\tremaining: 11.9s\n",
      "171:\tlearn: 0.2188815\ttotal: 2.47s\tremaining: 11.9s\n",
      "172:\tlearn: 0.2186425\ttotal: 2.48s\tremaining: 11.9s\n",
      "173:\tlearn: 0.2184912\ttotal: 2.5s\tremaining: 11.9s\n",
      "174:\tlearn: 0.2176122\ttotal: 2.51s\tremaining: 11.8s\n",
      "175:\tlearn: 0.2172080\ttotal: 2.52s\tremaining: 11.8s\n",
      "176:\tlearn: 0.2170732\ttotal: 2.54s\tremaining: 11.8s\n",
      "177:\tlearn: 0.2167503\ttotal: 2.55s\tremaining: 11.8s\n",
      "178:\tlearn: 0.2162440\ttotal: 2.56s\tremaining: 11.8s\n",
      "179:\tlearn: 0.2156411\ttotal: 2.58s\tremaining: 11.7s\n",
      "180:\tlearn: 0.2150326\ttotal: 2.59s\tremaining: 11.7s\n",
      "181:\tlearn: 0.2148444\ttotal: 2.6s\tremaining: 11.7s\n",
      "182:\tlearn: 0.2147158\ttotal: 2.62s\tremaining: 11.7s\n",
      "183:\tlearn: 0.2142456\ttotal: 2.63s\tremaining: 11.7s\n",
      "184:\tlearn: 0.2138073\ttotal: 2.65s\tremaining: 11.7s\n",
      "185:\tlearn: 0.2136764\ttotal: 2.66s\tremaining: 11.7s\n",
      "186:\tlearn: 0.2134507\ttotal: 2.67s\tremaining: 11.6s\n",
      "187:\tlearn: 0.2130788\ttotal: 2.69s\tremaining: 11.6s\n",
      "188:\tlearn: 0.2126651\ttotal: 2.7s\tremaining: 11.6s\n",
      "189:\tlearn: 0.2125072\ttotal: 2.71s\tremaining: 11.6s\n",
      "190:\tlearn: 0.2121432\ttotal: 2.73s\tremaining: 11.6s\n",
      "191:\tlearn: 0.2118621\ttotal: 2.74s\tremaining: 11.5s\n",
      "192:\tlearn: 0.2117205\ttotal: 2.75s\tremaining: 11.5s\n",
      "193:\tlearn: 0.2114511\ttotal: 2.77s\tremaining: 11.5s\n",
      "194:\tlearn: 0.2111681\ttotal: 2.78s\tremaining: 11.5s\n",
      "195:\tlearn: 0.2105566\ttotal: 2.79s\tremaining: 11.5s\n",
      "196:\tlearn: 0.2104169\ttotal: 2.81s\tremaining: 11.4s\n",
      "197:\tlearn: 0.2101035\ttotal: 2.82s\tremaining: 11.4s\n",
      "198:\tlearn: 0.2099652\ttotal: 2.83s\tremaining: 11.4s\n",
      "199:\tlearn: 0.2098249\ttotal: 2.85s\tremaining: 11.4s\n",
      "200:\tlearn: 0.2096813\ttotal: 2.86s\tremaining: 11.4s\n",
      "201:\tlearn: 0.2095840\ttotal: 2.87s\tremaining: 11.4s\n",
      "202:\tlearn: 0.2091938\ttotal: 2.89s\tremaining: 11.3s\n",
      "203:\tlearn: 0.2090518\ttotal: 2.9s\tremaining: 11.3s\n",
      "204:\tlearn: 0.2089198\ttotal: 2.91s\tremaining: 11.3s\n",
      "205:\tlearn: 0.2088109\ttotal: 2.93s\tremaining: 11.3s\n",
      "206:\tlearn: 0.2086382\ttotal: 2.94s\tremaining: 11.3s\n",
      "207:\tlearn: 0.2084211\ttotal: 2.96s\tremaining: 11.3s\n",
      "208:\tlearn: 0.2083349\ttotal: 2.97s\tremaining: 11.2s\n",
      "209:\tlearn: 0.2082217\ttotal: 2.98s\tremaining: 11.2s\n",
      "210:\tlearn: 0.2079031\ttotal: 2.99s\tremaining: 11.2s\n",
      "211:\tlearn: 0.2076030\ttotal: 3.01s\tremaining: 11.2s\n",
      "212:\tlearn: 0.2072355\ttotal: 3.02s\tremaining: 11.2s\n",
      "213:\tlearn: 0.2069264\ttotal: 3.04s\tremaining: 11.2s\n",
      "214:\tlearn: 0.2067157\ttotal: 3.06s\tremaining: 11.2s\n",
      "215:\tlearn: 0.2065493\ttotal: 3.07s\tremaining: 11.2s\n",
      "216:\tlearn: 0.2064387\ttotal: 3.08s\tremaining: 11.1s\n",
      "217:\tlearn: 0.2062028\ttotal: 3.1s\tremaining: 11.1s\n",
      "218:\tlearn: 0.2059879\ttotal: 3.11s\tremaining: 11.1s\n",
      "219:\tlearn: 0.2058857\ttotal: 3.12s\tremaining: 11.1s\n",
      "220:\tlearn: 0.2054282\ttotal: 3.14s\tremaining: 11.1s\n",
      "221:\tlearn: 0.2053145\ttotal: 3.15s\tremaining: 11s\n",
      "222:\tlearn: 0.2052100\ttotal: 3.16s\tremaining: 11s\n",
      "223:\tlearn: 0.2051106\ttotal: 3.17s\tremaining: 11s\n",
      "224:\tlearn: 0.2049079\ttotal: 3.19s\tremaining: 11s\n",
      "225:\tlearn: 0.2048001\ttotal: 3.2s\tremaining: 11s\n",
      "226:\tlearn: 0.2046749\ttotal: 3.21s\tremaining: 10.9s\n",
      "227:\tlearn: 0.2044039\ttotal: 3.23s\tremaining: 10.9s\n",
      "228:\tlearn: 0.2040293\ttotal: 3.24s\tremaining: 10.9s\n",
      "229:\tlearn: 0.2039502\ttotal: 3.26s\tremaining: 10.9s\n",
      "230:\tlearn: 0.2037064\ttotal: 3.27s\tremaining: 10.9s\n",
      "231:\tlearn: 0.2036304\ttotal: 3.28s\tremaining: 10.9s\n",
      "232:\tlearn: 0.2035255\ttotal: 3.29s\tremaining: 10.8s\n",
      "233:\tlearn: 0.2030942\ttotal: 3.31s\tremaining: 10.8s\n",
      "234:\tlearn: 0.2029083\ttotal: 3.32s\tremaining: 10.8s\n",
      "235:\tlearn: 0.2024291\ttotal: 3.34s\tremaining: 10.8s\n",
      "236:\tlearn: 0.2020361\ttotal: 3.35s\tremaining: 10.8s\n",
      "237:\tlearn: 0.2017859\ttotal: 3.36s\tremaining: 10.8s\n",
      "238:\tlearn: 0.2015219\ttotal: 3.38s\tremaining: 10.7s\n",
      "239:\tlearn: 0.2014362\ttotal: 3.39s\tremaining: 10.7s\n",
      "240:\tlearn: 0.2013379\ttotal: 3.4s\tremaining: 10.7s\n",
      "241:\tlearn: 0.2011377\ttotal: 3.42s\tremaining: 10.7s\n",
      "242:\tlearn: 0.2009761\ttotal: 3.43s\tremaining: 10.7s\n",
      "243:\tlearn: 0.2008157\ttotal: 3.45s\tremaining: 10.7s\n",
      "244:\tlearn: 0.2007054\ttotal: 3.46s\tremaining: 10.7s\n",
      "245:\tlearn: 0.2005560\ttotal: 3.48s\tremaining: 10.7s\n",
      "246:\tlearn: 0.2001657\ttotal: 3.49s\tremaining: 10.6s\n",
      "247:\tlearn: 0.2000854\ttotal: 3.5s\tremaining: 10.6s\n",
      "248:\tlearn: 0.1999526\ttotal: 3.52s\tremaining: 10.6s\n",
      "249:\tlearn: 0.1996604\ttotal: 3.53s\tremaining: 10.6s\n",
      "250:\tlearn: 0.1995897\ttotal: 3.54s\tremaining: 10.6s\n",
      "251:\tlearn: 0.1994857\ttotal: 3.56s\tremaining: 10.6s\n",
      "252:\tlearn: 0.1991655\ttotal: 3.57s\tremaining: 10.5s\n",
      "253:\tlearn: 0.1991084\ttotal: 3.58s\tremaining: 10.5s\n",
      "254:\tlearn: 0.1987866\ttotal: 3.6s\tremaining: 10.5s\n",
      "255:\tlearn: 0.1985469\ttotal: 3.61s\tremaining: 10.5s\n",
      "256:\tlearn: 0.1983932\ttotal: 3.63s\tremaining: 10.5s\n",
      "257:\tlearn: 0.1981938\ttotal: 3.64s\tremaining: 10.5s\n",
      "258:\tlearn: 0.1980807\ttotal: 3.65s\tremaining: 10.5s\n",
      "259:\tlearn: 0.1978712\ttotal: 3.67s\tremaining: 10.4s\n",
      "260:\tlearn: 0.1977399\ttotal: 3.68s\tremaining: 10.4s\n",
      "261:\tlearn: 0.1975748\ttotal: 3.69s\tremaining: 10.4s\n",
      "262:\tlearn: 0.1974504\ttotal: 3.71s\tremaining: 10.4s\n",
      "263:\tlearn: 0.1973095\ttotal: 3.72s\tremaining: 10.4s\n",
      "264:\tlearn: 0.1971064\ttotal: 3.73s\tremaining: 10.3s\n",
      "265:\tlearn: 0.1970593\ttotal: 3.74s\tremaining: 10.3s\n",
      "266:\tlearn: 0.1969741\ttotal: 3.76s\tremaining: 10.3s\n",
      "267:\tlearn: 0.1968829\ttotal: 3.77s\tremaining: 10.3s\n",
      "268:\tlearn: 0.1965879\ttotal: 3.78s\tremaining: 10.3s\n",
      "269:\tlearn: 0.1964677\ttotal: 3.8s\tremaining: 10.3s\n",
      "270:\tlearn: 0.1963834\ttotal: 3.81s\tremaining: 10.3s\n",
      "271:\tlearn: 0.1962127\ttotal: 3.82s\tremaining: 10.2s\n",
      "272:\tlearn: 0.1960811\ttotal: 3.84s\tremaining: 10.2s\n",
      "273:\tlearn: 0.1959183\ttotal: 3.85s\tremaining: 10.2s\n",
      "274:\tlearn: 0.1956501\ttotal: 3.86s\tremaining: 10.2s\n",
      "275:\tlearn: 0.1955411\ttotal: 3.87s\tremaining: 10.2s\n",
      "276:\tlearn: 0.1953602\ttotal: 3.89s\tremaining: 10.1s\n",
      "277:\tlearn: 0.1949458\ttotal: 3.9s\tremaining: 10.1s\n",
      "278:\tlearn: 0.1948018\ttotal: 3.91s\tremaining: 10.1s\n",
      "279:\tlearn: 0.1947667\ttotal: 3.93s\tremaining: 10.1s\n",
      "280:\tlearn: 0.1946359\ttotal: 3.94s\tremaining: 10.1s\n",
      "281:\tlearn: 0.1945081\ttotal: 3.95s\tremaining: 10.1s\n",
      "282:\tlearn: 0.1944461\ttotal: 3.97s\tremaining: 10s\n",
      "283:\tlearn: 0.1943188\ttotal: 3.98s\tremaining: 10s\n",
      "284:\tlearn: 0.1942175\ttotal: 3.99s\tremaining: 10s\n",
      "285:\tlearn: 0.1940998\ttotal: 4.01s\tremaining: 10s\n",
      "286:\tlearn: 0.1939484\ttotal: 4.02s\tremaining: 10s\n",
      "287:\tlearn: 0.1938348\ttotal: 4.04s\tremaining: 9.98s\n",
      "288:\tlearn: 0.1937731\ttotal: 4.05s\tremaining: 9.97s\n",
      "289:\tlearn: 0.1937133\ttotal: 4.07s\tremaining: 9.96s\n",
      "290:\tlearn: 0.1936580\ttotal: 4.08s\tremaining: 9.94s\n",
      "291:\tlearn: 0.1936082\ttotal: 4.09s\tremaining: 9.92s\n",
      "292:\tlearn: 0.1934663\ttotal: 4.11s\tremaining: 9.91s\n",
      "293:\tlearn: 0.1934034\ttotal: 4.12s\tremaining: 9.89s\n",
      "294:\tlearn: 0.1931661\ttotal: 4.13s\tremaining: 9.88s\n",
      "295:\tlearn: 0.1930179\ttotal: 4.15s\tremaining: 9.86s\n",
      "296:\tlearn: 0.1929482\ttotal: 4.16s\tremaining: 9.85s\n",
      "297:\tlearn: 0.1928603\ttotal: 4.17s\tremaining: 9.83s\n",
      "298:\tlearn: 0.1927613\ttotal: 4.19s\tremaining: 9.81s\n",
      "299:\tlearn: 0.1926579\ttotal: 4.2s\tremaining: 9.8s\n",
      "300:\tlearn: 0.1924281\ttotal: 4.21s\tremaining: 9.79s\n",
      "301:\tlearn: 0.1923415\ttotal: 4.24s\tremaining: 9.81s\n",
      "302:\tlearn: 0.1922677\ttotal: 4.27s\tremaining: 9.83s\n",
      "303:\tlearn: 0.1920865\ttotal: 4.29s\tremaining: 9.82s\n",
      "304:\tlearn: 0.1917515\ttotal: 4.31s\tremaining: 9.82s\n",
      "305:\tlearn: 0.1916379\ttotal: 4.32s\tremaining: 9.8s\n",
      "306:\tlearn: 0.1914155\ttotal: 4.33s\tremaining: 9.78s\n",
      "307:\tlearn: 0.1913676\ttotal: 4.35s\tremaining: 9.76s\n",
      "308:\tlearn: 0.1912509\ttotal: 4.36s\tremaining: 9.74s\n",
      "309:\tlearn: 0.1911422\ttotal: 4.37s\tremaining: 9.73s\n",
      "310:\tlearn: 0.1910570\ttotal: 4.38s\tremaining: 9.71s\n",
      "311:\tlearn: 0.1910088\ttotal: 4.4s\tremaining: 9.7s\n",
      "312:\tlearn: 0.1909100\ttotal: 4.41s\tremaining: 9.68s\n",
      "313:\tlearn: 0.1908121\ttotal: 4.43s\tremaining: 9.67s\n",
      "314:\tlearn: 0.1903008\ttotal: 4.45s\tremaining: 9.67s\n",
      "315:\tlearn: 0.1901720\ttotal: 4.46s\tremaining: 9.65s\n",
      "316:\tlearn: 0.1901099\ttotal: 4.47s\tremaining: 9.64s\n",
      "317:\tlearn: 0.1900558\ttotal: 4.49s\tremaining: 9.62s\n",
      "318:\tlearn: 0.1898674\ttotal: 4.5s\tremaining: 9.6s\n",
      "319:\tlearn: 0.1897787\ttotal: 4.51s\tremaining: 9.59s\n",
      "320:\tlearn: 0.1896949\ttotal: 4.52s\tremaining: 9.57s\n",
      "321:\tlearn: 0.1896087\ttotal: 4.54s\tremaining: 9.55s\n",
      "322:\tlearn: 0.1895099\ttotal: 4.55s\tremaining: 9.54s\n",
      "323:\tlearn: 0.1894400\ttotal: 4.56s\tremaining: 9.52s\n",
      "324:\tlearn: 0.1893896\ttotal: 4.58s\tremaining: 9.51s\n",
      "325:\tlearn: 0.1893369\ttotal: 4.59s\tremaining: 9.49s\n",
      "326:\tlearn: 0.1892941\ttotal: 4.6s\tremaining: 9.47s\n",
      "327:\tlearn: 0.1892343\ttotal: 4.62s\tremaining: 9.46s\n",
      "328:\tlearn: 0.1890169\ttotal: 4.63s\tremaining: 9.44s\n",
      "329:\tlearn: 0.1888689\ttotal: 4.64s\tremaining: 9.42s\n",
      "330:\tlearn: 0.1888116\ttotal: 4.66s\tremaining: 9.41s\n",
      "331:\tlearn: 0.1886906\ttotal: 4.67s\tremaining: 9.39s\n",
      "332:\tlearn: 0.1885966\ttotal: 4.68s\tremaining: 9.38s\n",
      "333:\tlearn: 0.1885038\ttotal: 4.7s\tremaining: 9.36s\n",
      "334:\tlearn: 0.1884238\ttotal: 4.71s\tremaining: 9.35s\n",
      "335:\tlearn: 0.1882942\ttotal: 4.72s\tremaining: 9.33s\n",
      "336:\tlearn: 0.1882143\ttotal: 4.73s\tremaining: 9.31s\n",
      "337:\tlearn: 0.1880485\ttotal: 4.75s\tremaining: 9.3s\n",
      "338:\tlearn: 0.1879681\ttotal: 4.76s\tremaining: 9.28s\n",
      "339:\tlearn: 0.1878851\ttotal: 4.77s\tremaining: 9.27s\n",
      "340:\tlearn: 0.1878243\ttotal: 4.79s\tremaining: 9.25s\n",
      "341:\tlearn: 0.1877222\ttotal: 4.8s\tremaining: 9.23s\n",
      "342:\tlearn: 0.1876075\ttotal: 4.81s\tremaining: 9.22s\n",
      "343:\tlearn: 0.1874320\ttotal: 4.83s\tremaining: 9.21s\n",
      "344:\tlearn: 0.1872017\ttotal: 4.84s\tremaining: 9.19s\n",
      "345:\tlearn: 0.1871390\ttotal: 4.85s\tremaining: 9.17s\n",
      "346:\tlearn: 0.1870630\ttotal: 4.87s\tremaining: 9.16s\n",
      "347:\tlearn: 0.1869284\ttotal: 4.88s\tremaining: 9.15s\n",
      "348:\tlearn: 0.1868720\ttotal: 4.89s\tremaining: 9.13s\n",
      "349:\tlearn: 0.1868196\ttotal: 4.91s\tremaining: 9.11s\n",
      "350:\tlearn: 0.1867965\ttotal: 4.92s\tremaining: 9.1s\n",
      "351:\tlearn: 0.1866468\ttotal: 4.93s\tremaining: 9.08s\n",
      "352:\tlearn: 0.1865912\ttotal: 4.95s\tremaining: 9.06s\n",
      "353:\tlearn: 0.1864968\ttotal: 4.96s\tremaining: 9.05s\n",
      "354:\tlearn: 0.1864212\ttotal: 4.97s\tremaining: 9.04s\n",
      "355:\tlearn: 0.1863175\ttotal: 4.99s\tremaining: 9.02s\n",
      "356:\tlearn: 0.1862700\ttotal: 5s\tremaining: 9.01s\n",
      "357:\tlearn: 0.1861959\ttotal: 5.02s\tremaining: 9s\n",
      "358:\tlearn: 0.1859359\ttotal: 5.03s\tremaining: 8.98s\n",
      "359:\tlearn: 0.1858679\ttotal: 5.04s\tremaining: 8.97s\n",
      "360:\tlearn: 0.1857831\ttotal: 5.06s\tremaining: 8.95s\n",
      "361:\tlearn: 0.1856949\ttotal: 5.07s\tremaining: 8.94s\n",
      "362:\tlearn: 0.1854936\ttotal: 5.08s\tremaining: 8.92s\n",
      "363:\tlearn: 0.1853595\ttotal: 5.1s\tremaining: 8.91s\n",
      "364:\tlearn: 0.1852154\ttotal: 5.11s\tremaining: 8.89s\n",
      "365:\tlearn: 0.1850100\ttotal: 5.12s\tremaining: 8.88s\n",
      "366:\tlearn: 0.1849485\ttotal: 5.14s\tremaining: 8.86s\n",
      "367:\tlearn: 0.1848648\ttotal: 5.15s\tremaining: 8.84s\n",
      "368:\tlearn: 0.1848141\ttotal: 5.16s\tremaining: 8.83s\n",
      "369:\tlearn: 0.1847287\ttotal: 5.18s\tremaining: 8.81s\n",
      "370:\tlearn: 0.1845078\ttotal: 5.2s\tremaining: 8.81s\n",
      "371:\tlearn: 0.1844152\ttotal: 5.21s\tremaining: 8.8s\n",
      "372:\tlearn: 0.1841576\ttotal: 5.23s\tremaining: 8.79s\n",
      "373:\tlearn: 0.1840924\ttotal: 5.24s\tremaining: 8.77s\n",
      "374:\tlearn: 0.1838261\ttotal: 5.25s\tremaining: 8.76s\n",
      "375:\tlearn: 0.1835643\ttotal: 5.27s\tremaining: 8.75s\n",
      "376:\tlearn: 0.1835056\ttotal: 5.28s\tremaining: 8.73s\n",
      "377:\tlearn: 0.1833844\ttotal: 5.29s\tremaining: 8.71s\n",
      "378:\tlearn: 0.1833437\ttotal: 5.31s\tremaining: 8.7s\n",
      "379:\tlearn: 0.1832890\ttotal: 5.32s\tremaining: 8.68s\n",
      "380:\tlearn: 0.1832214\ttotal: 5.33s\tremaining: 8.67s\n",
      "381:\tlearn: 0.1831632\ttotal: 5.35s\tremaining: 8.65s\n",
      "382:\tlearn: 0.1831240\ttotal: 5.36s\tremaining: 8.63s\n",
      "383:\tlearn: 0.1829561\ttotal: 5.37s\tremaining: 8.62s\n",
      "384:\tlearn: 0.1829024\ttotal: 5.39s\tremaining: 8.61s\n",
      "385:\tlearn: 0.1827690\ttotal: 5.4s\tremaining: 8.59s\n",
      "386:\tlearn: 0.1827232\ttotal: 5.42s\tremaining: 8.59s\n",
      "387:\tlearn: 0.1826292\ttotal: 5.44s\tremaining: 8.58s\n",
      "388:\tlearn: 0.1825951\ttotal: 5.45s\tremaining: 8.56s\n",
      "389:\tlearn: 0.1825032\ttotal: 5.46s\tremaining: 8.54s\n",
      "390:\tlearn: 0.1824646\ttotal: 5.47s\tremaining: 8.53s\n",
      "391:\tlearn: 0.1823938\ttotal: 5.49s\tremaining: 8.51s\n",
      "392:\tlearn: 0.1822680\ttotal: 5.5s\tremaining: 8.49s\n",
      "393:\tlearn: 0.1821884\ttotal: 5.51s\tremaining: 8.48s\n",
      "394:\tlearn: 0.1821407\ttotal: 5.53s\tremaining: 8.46s\n",
      "395:\tlearn: 0.1820863\ttotal: 5.54s\tremaining: 8.45s\n",
      "396:\tlearn: 0.1819970\ttotal: 5.55s\tremaining: 8.43s\n",
      "397:\tlearn: 0.1818990\ttotal: 5.56s\tremaining: 8.41s\n",
      "398:\tlearn: 0.1818112\ttotal: 5.58s\tremaining: 8.4s\n",
      "399:\tlearn: 0.1817555\ttotal: 5.59s\tremaining: 8.39s\n",
      "400:\tlearn: 0.1816152\ttotal: 5.61s\tremaining: 8.37s\n",
      "401:\tlearn: 0.1815764\ttotal: 5.62s\tremaining: 8.36s\n",
      "402:\tlearn: 0.1815047\ttotal: 5.63s\tremaining: 8.34s\n",
      "403:\tlearn: 0.1814035\ttotal: 5.64s\tremaining: 8.33s\n",
      "404:\tlearn: 0.1813090\ttotal: 5.66s\tremaining: 8.31s\n",
      "405:\tlearn: 0.1811957\ttotal: 5.67s\tremaining: 8.3s\n",
      "406:\tlearn: 0.1811152\ttotal: 5.68s\tremaining: 8.28s\n",
      "407:\tlearn: 0.1810467\ttotal: 5.7s\tremaining: 8.27s\n",
      "408:\tlearn: 0.1809706\ttotal: 5.71s\tremaining: 8.25s\n",
      "409:\tlearn: 0.1808628\ttotal: 5.72s\tremaining: 8.24s\n",
      "410:\tlearn: 0.1808296\ttotal: 5.74s\tremaining: 8.22s\n",
      "411:\tlearn: 0.1807565\ttotal: 5.75s\tremaining: 8.21s\n",
      "412:\tlearn: 0.1807023\ttotal: 5.76s\tremaining: 8.19s\n",
      "413:\tlearn: 0.1806194\ttotal: 5.78s\tremaining: 8.18s\n",
      "414:\tlearn: 0.1805455\ttotal: 5.79s\tremaining: 8.16s\n",
      "415:\tlearn: 0.1805028\ttotal: 5.8s\tremaining: 8.15s\n",
      "416:\tlearn: 0.1804559\ttotal: 5.82s\tremaining: 8.13s\n",
      "417:\tlearn: 0.1804227\ttotal: 5.83s\tremaining: 8.11s\n",
      "418:\tlearn: 0.1803242\ttotal: 5.84s\tremaining: 8.1s\n",
      "419:\tlearn: 0.1802351\ttotal: 5.85s\tremaining: 8.08s\n",
      "420:\tlearn: 0.1801300\ttotal: 5.87s\tremaining: 8.07s\n",
      "421:\tlearn: 0.1800784\ttotal: 5.88s\tremaining: 8.05s\n",
      "422:\tlearn: 0.1800381\ttotal: 5.89s\tremaining: 8.04s\n",
      "423:\tlearn: 0.1799558\ttotal: 5.91s\tremaining: 8.02s\n",
      "424:\tlearn: 0.1798722\ttotal: 5.92s\tremaining: 8.01s\n",
      "425:\tlearn: 0.1798170\ttotal: 5.93s\tremaining: 7.99s\n",
      "426:\tlearn: 0.1797411\ttotal: 5.94s\tremaining: 7.98s\n",
      "427:\tlearn: 0.1796367\ttotal: 5.96s\tremaining: 7.96s\n",
      "428:\tlearn: 0.1795937\ttotal: 5.97s\tremaining: 7.95s\n",
      "429:\tlearn: 0.1795056\ttotal: 5.99s\tremaining: 7.94s\n",
      "430:\tlearn: 0.1794166\ttotal: 6s\tremaining: 7.93s\n",
      "431:\tlearn: 0.1793462\ttotal: 6.02s\tremaining: 7.92s\n",
      "432:\tlearn: 0.1792765\ttotal: 6.03s\tremaining: 7.9s\n",
      "433:\tlearn: 0.1792345\ttotal: 6.04s\tremaining: 7.88s\n",
      "434:\tlearn: 0.1792078\ttotal: 6.06s\tremaining: 7.87s\n",
      "435:\tlearn: 0.1790357\ttotal: 6.07s\tremaining: 7.86s\n",
      "436:\tlearn: 0.1789843\ttotal: 6.09s\tremaining: 7.84s\n",
      "437:\tlearn: 0.1789235\ttotal: 6.1s\tremaining: 7.83s\n",
      "438:\tlearn: 0.1788146\ttotal: 6.11s\tremaining: 7.81s\n",
      "439:\tlearn: 0.1787673\ttotal: 6.13s\tremaining: 7.8s\n",
      "440:\tlearn: 0.1787053\ttotal: 6.14s\tremaining: 7.78s\n",
      "441:\tlearn: 0.1784944\ttotal: 6.15s\tremaining: 7.77s\n",
      "442:\tlearn: 0.1783968\ttotal: 6.17s\tremaining: 7.75s\n",
      "443:\tlearn: 0.1783531\ttotal: 6.18s\tremaining: 7.74s\n",
      "444:\tlearn: 0.1782809\ttotal: 6.2s\tremaining: 7.73s\n",
      "445:\tlearn: 0.1781712\ttotal: 6.21s\tremaining: 7.71s\n",
      "446:\tlearn: 0.1780890\ttotal: 6.22s\tremaining: 7.7s\n",
      "447:\tlearn: 0.1780108\ttotal: 6.24s\tremaining: 7.68s\n",
      "448:\tlearn: 0.1779276\ttotal: 6.25s\tremaining: 7.67s\n",
      "449:\tlearn: 0.1778777\ttotal: 6.26s\tremaining: 7.65s\n",
      "450:\tlearn: 0.1777547\ttotal: 6.27s\tremaining: 7.64s\n",
      "451:\tlearn: 0.1775431\ttotal: 6.29s\tremaining: 7.62s\n",
      "452:\tlearn: 0.1774857\ttotal: 6.3s\tremaining: 7.61s\n",
      "453:\tlearn: 0.1774080\ttotal: 6.31s\tremaining: 7.59s\n",
      "454:\tlearn: 0.1773425\ttotal: 6.33s\tremaining: 7.58s\n",
      "455:\tlearn: 0.1772637\ttotal: 6.34s\tremaining: 7.56s\n",
      "456:\tlearn: 0.1772042\ttotal: 6.35s\tremaining: 7.54s\n",
      "457:\tlearn: 0.1771127\ttotal: 6.36s\tremaining: 7.53s\n",
      "458:\tlearn: 0.1770322\ttotal: 6.38s\tremaining: 7.52s\n",
      "459:\tlearn: 0.1769305\ttotal: 6.39s\tremaining: 7.5s\n",
      "460:\tlearn: 0.1768777\ttotal: 6.41s\tremaining: 7.49s\n",
      "461:\tlearn: 0.1768067\ttotal: 6.42s\tremaining: 7.48s\n",
      "462:\tlearn: 0.1767671\ttotal: 6.43s\tremaining: 7.46s\n",
      "463:\tlearn: 0.1767185\ttotal: 6.45s\tremaining: 7.45s\n",
      "464:\tlearn: 0.1767005\ttotal: 6.46s\tremaining: 7.43s\n",
      "465:\tlearn: 0.1766424\ttotal: 6.47s\tremaining: 7.42s\n",
      "466:\tlearn: 0.1765920\ttotal: 6.48s\tremaining: 7.4s\n",
      "467:\tlearn: 0.1765041\ttotal: 6.5s\tremaining: 7.38s\n",
      "468:\tlearn: 0.1764574\ttotal: 6.51s\tremaining: 7.37s\n",
      "469:\tlearn: 0.1764117\ttotal: 6.52s\tremaining: 7.35s\n",
      "470:\tlearn: 0.1763813\ttotal: 6.53s\tremaining: 7.34s\n",
      "471:\tlearn: 0.1762894\ttotal: 6.55s\tremaining: 7.33s\n",
      "472:\tlearn: 0.1761942\ttotal: 6.56s\tremaining: 7.31s\n",
      "473:\tlearn: 0.1761261\ttotal: 6.58s\tremaining: 7.3s\n",
      "474:\tlearn: 0.1760810\ttotal: 6.59s\tremaining: 7.29s\n",
      "475:\tlearn: 0.1759476\ttotal: 6.6s\tremaining: 7.27s\n",
      "476:\tlearn: 0.1758277\ttotal: 6.62s\tremaining: 7.25s\n",
      "477:\tlearn: 0.1757929\ttotal: 6.63s\tremaining: 7.24s\n",
      "478:\tlearn: 0.1757536\ttotal: 6.64s\tremaining: 7.22s\n",
      "479:\tlearn: 0.1757157\ttotal: 6.66s\tremaining: 7.21s\n",
      "480:\tlearn: 0.1756629\ttotal: 6.67s\tremaining: 7.2s\n",
      "481:\tlearn: 0.1756211\ttotal: 6.68s\tremaining: 7.18s\n",
      "482:\tlearn: 0.1755404\ttotal: 6.7s\tremaining: 7.17s\n",
      "483:\tlearn: 0.1754676\ttotal: 6.71s\tremaining: 7.15s\n",
      "484:\tlearn: 0.1754458\ttotal: 6.72s\tremaining: 7.14s\n",
      "485:\tlearn: 0.1753762\ttotal: 6.73s\tremaining: 7.12s\n",
      "486:\tlearn: 0.1753285\ttotal: 6.75s\tremaining: 7.11s\n",
      "487:\tlearn: 0.1752729\ttotal: 6.76s\tremaining: 7.09s\n",
      "488:\tlearn: 0.1752179\ttotal: 6.78s\tremaining: 7.08s\n",
      "489:\tlearn: 0.1751368\ttotal: 6.79s\tremaining: 7.07s\n",
      "490:\tlearn: 0.1750529\ttotal: 6.8s\tremaining: 7.05s\n",
      "491:\tlearn: 0.1749863\ttotal: 6.82s\tremaining: 7.04s\n",
      "492:\tlearn: 0.1749155\ttotal: 6.83s\tremaining: 7.02s\n",
      "493:\tlearn: 0.1748804\ttotal: 6.84s\tremaining: 7.01s\n",
      "494:\tlearn: 0.1747982\ttotal: 6.85s\tremaining: 6.99s\n",
      "495:\tlearn: 0.1747585\ttotal: 6.87s\tremaining: 6.98s\n",
      "496:\tlearn: 0.1747134\ttotal: 6.88s\tremaining: 6.96s\n",
      "497:\tlearn: 0.1746614\ttotal: 6.89s\tremaining: 6.95s\n",
      "498:\tlearn: 0.1745827\ttotal: 6.9s\tremaining: 6.93s\n",
      "499:\tlearn: 0.1745114\ttotal: 6.92s\tremaining: 6.92s\n",
      "500:\tlearn: 0.1744611\ttotal: 6.93s\tremaining: 6.9s\n",
      "501:\tlearn: 0.1744032\ttotal: 6.94s\tremaining: 6.89s\n",
      "502:\tlearn: 0.1743493\ttotal: 6.96s\tremaining: 6.87s\n",
      "503:\tlearn: 0.1742937\ttotal: 6.97s\tremaining: 6.86s\n",
      "504:\tlearn: 0.1742535\ttotal: 6.99s\tremaining: 6.85s\n",
      "505:\tlearn: 0.1741576\ttotal: 7s\tremaining: 6.84s\n",
      "506:\tlearn: 0.1741013\ttotal: 7.02s\tremaining: 6.82s\n",
      "507:\tlearn: 0.1740612\ttotal: 7.03s\tremaining: 6.81s\n",
      "508:\tlearn: 0.1739966\ttotal: 7.04s\tremaining: 6.8s\n",
      "509:\tlearn: 0.1739538\ttotal: 7.06s\tremaining: 6.78s\n",
      "510:\tlearn: 0.1738686\ttotal: 7.07s\tremaining: 6.77s\n",
      "511:\tlearn: 0.1737624\ttotal: 7.09s\tremaining: 6.75s\n",
      "512:\tlearn: 0.1737219\ttotal: 7.1s\tremaining: 6.74s\n",
      "513:\tlearn: 0.1736742\ttotal: 7.11s\tremaining: 6.72s\n",
      "514:\tlearn: 0.1735818\ttotal: 7.13s\tremaining: 6.71s\n",
      "515:\tlearn: 0.1735142\ttotal: 7.14s\tremaining: 6.7s\n",
      "516:\tlearn: 0.1733840\ttotal: 7.16s\tremaining: 6.68s\n",
      "517:\tlearn: 0.1732786\ttotal: 7.17s\tremaining: 6.67s\n",
      "518:\tlearn: 0.1732349\ttotal: 7.18s\tremaining: 6.66s\n",
      "519:\tlearn: 0.1731769\ttotal: 7.2s\tremaining: 6.64s\n",
      "520:\tlearn: 0.1731296\ttotal: 7.21s\tremaining: 6.63s\n",
      "521:\tlearn: 0.1731176\ttotal: 7.22s\tremaining: 6.61s\n",
      "522:\tlearn: 0.1730747\ttotal: 7.23s\tremaining: 6.6s\n",
      "523:\tlearn: 0.1730482\ttotal: 7.25s\tremaining: 6.58s\n",
      "524:\tlearn: 0.1729660\ttotal: 7.26s\tremaining: 6.57s\n",
      "525:\tlearn: 0.1728240\ttotal: 7.27s\tremaining: 6.55s\n",
      "526:\tlearn: 0.1726617\ttotal: 7.29s\tremaining: 6.54s\n",
      "527:\tlearn: 0.1725300\ttotal: 7.3s\tremaining: 6.53s\n",
      "528:\tlearn: 0.1724515\ttotal: 7.31s\tremaining: 6.51s\n",
      "529:\tlearn: 0.1723622\ttotal: 7.33s\tremaining: 6.5s\n",
      "530:\tlearn: 0.1722962\ttotal: 7.34s\tremaining: 6.48s\n",
      "531:\tlearn: 0.1722233\ttotal: 7.35s\tremaining: 6.47s\n",
      "532:\tlearn: 0.1721489\ttotal: 7.37s\tremaining: 6.45s\n",
      "533:\tlearn: 0.1720414\ttotal: 7.38s\tremaining: 6.44s\n",
      "534:\tlearn: 0.1719959\ttotal: 7.39s\tremaining: 6.42s\n",
      "535:\tlearn: 0.1719425\ttotal: 7.41s\tremaining: 6.41s\n",
      "536:\tlearn: 0.1719170\ttotal: 7.42s\tremaining: 6.4s\n",
      "537:\tlearn: 0.1718710\ttotal: 7.43s\tremaining: 6.38s\n",
      "538:\tlearn: 0.1718211\ttotal: 7.44s\tremaining: 6.37s\n",
      "539:\tlearn: 0.1717408\ttotal: 7.46s\tremaining: 6.35s\n",
      "540:\tlearn: 0.1717124\ttotal: 7.47s\tremaining: 6.34s\n",
      "541:\tlearn: 0.1716878\ttotal: 7.48s\tremaining: 6.32s\n",
      "542:\tlearn: 0.1716171\ttotal: 7.5s\tremaining: 6.31s\n",
      "543:\tlearn: 0.1715967\ttotal: 7.51s\tremaining: 6.29s\n",
      "544:\tlearn: 0.1715623\ttotal: 7.52s\tremaining: 6.28s\n",
      "545:\tlearn: 0.1715074\ttotal: 7.53s\tremaining: 6.26s\n",
      "546:\tlearn: 0.1714481\ttotal: 7.55s\tremaining: 6.25s\n",
      "547:\tlearn: 0.1714304\ttotal: 7.56s\tremaining: 6.24s\n",
      "548:\tlearn: 0.1713923\ttotal: 7.58s\tremaining: 6.22s\n",
      "549:\tlearn: 0.1712754\ttotal: 7.59s\tremaining: 6.21s\n",
      "550:\tlearn: 0.1710967\ttotal: 7.61s\tremaining: 6.2s\n",
      "551:\tlearn: 0.1710073\ttotal: 7.62s\tremaining: 6.18s\n",
      "552:\tlearn: 0.1709294\ttotal: 7.63s\tremaining: 6.17s\n",
      "553:\tlearn: 0.1708276\ttotal: 7.65s\tremaining: 6.16s\n",
      "554:\tlearn: 0.1707884\ttotal: 7.66s\tremaining: 6.14s\n",
      "555:\tlearn: 0.1707076\ttotal: 7.67s\tremaining: 6.13s\n",
      "556:\tlearn: 0.1706522\ttotal: 7.68s\tremaining: 6.11s\n",
      "557:\tlearn: 0.1705217\ttotal: 7.7s\tremaining: 6.1s\n",
      "558:\tlearn: 0.1704256\ttotal: 7.71s\tremaining: 6.08s\n",
      "559:\tlearn: 0.1703426\ttotal: 7.73s\tremaining: 6.07s\n",
      "560:\tlearn: 0.1702973\ttotal: 7.74s\tremaining: 6.06s\n",
      "561:\tlearn: 0.1702535\ttotal: 7.75s\tremaining: 6.04s\n",
      "562:\tlearn: 0.1701653\ttotal: 7.77s\tremaining: 6.03s\n",
      "563:\tlearn: 0.1701230\ttotal: 7.78s\tremaining: 6.02s\n",
      "564:\tlearn: 0.1700970\ttotal: 7.8s\tremaining: 6s\n",
      "565:\tlearn: 0.1700271\ttotal: 7.81s\tremaining: 5.99s\n",
      "566:\tlearn: 0.1699944\ttotal: 7.82s\tremaining: 5.97s\n",
      "567:\tlearn: 0.1699545\ttotal: 7.83s\tremaining: 5.96s\n",
      "568:\tlearn: 0.1699148\ttotal: 7.85s\tremaining: 5.95s\n",
      "569:\tlearn: 0.1698340\ttotal: 7.86s\tremaining: 5.93s\n",
      "570:\tlearn: 0.1697798\ttotal: 7.88s\tremaining: 5.92s\n",
      "571:\tlearn: 0.1697146\ttotal: 7.89s\tremaining: 5.9s\n",
      "572:\tlearn: 0.1696631\ttotal: 7.9s\tremaining: 5.89s\n",
      "573:\tlearn: 0.1696276\ttotal: 7.92s\tremaining: 5.88s\n",
      "574:\tlearn: 0.1695498\ttotal: 7.93s\tremaining: 5.86s\n",
      "575:\tlearn: 0.1695179\ttotal: 7.95s\tremaining: 5.85s\n",
      "576:\tlearn: 0.1694351\ttotal: 7.96s\tremaining: 5.83s\n",
      "577:\tlearn: 0.1693857\ttotal: 7.97s\tremaining: 5.82s\n",
      "578:\tlearn: 0.1693721\ttotal: 7.99s\tremaining: 5.81s\n",
      "579:\tlearn: 0.1693500\ttotal: 8s\tremaining: 5.79s\n",
      "580:\tlearn: 0.1692821\ttotal: 8.02s\tremaining: 5.78s\n",
      "581:\tlearn: 0.1692230\ttotal: 8.03s\tremaining: 5.77s\n",
      "582:\tlearn: 0.1691676\ttotal: 8.04s\tremaining: 5.75s\n",
      "583:\tlearn: 0.1691084\ttotal: 8.05s\tremaining: 5.74s\n",
      "584:\tlearn: 0.1690393\ttotal: 8.07s\tremaining: 5.72s\n",
      "585:\tlearn: 0.1689693\ttotal: 8.08s\tremaining: 5.71s\n",
      "586:\tlearn: 0.1689295\ttotal: 8.09s\tremaining: 5.69s\n",
      "587:\tlearn: 0.1687452\ttotal: 8.11s\tremaining: 5.68s\n",
      "588:\tlearn: 0.1686951\ttotal: 8.12s\tremaining: 5.67s\n",
      "589:\tlearn: 0.1686810\ttotal: 8.13s\tremaining: 5.65s\n",
      "590:\tlearn: 0.1686336\ttotal: 8.15s\tremaining: 5.64s\n",
      "591:\tlearn: 0.1685807\ttotal: 8.16s\tremaining: 5.63s\n",
      "592:\tlearn: 0.1685655\ttotal: 8.18s\tremaining: 5.61s\n",
      "593:\tlearn: 0.1685133\ttotal: 8.19s\tremaining: 5.6s\n",
      "594:\tlearn: 0.1684399\ttotal: 8.2s\tremaining: 5.58s\n",
      "595:\tlearn: 0.1683715\ttotal: 8.22s\tremaining: 5.57s\n",
      "596:\tlearn: 0.1682994\ttotal: 8.23s\tremaining: 5.55s\n",
      "597:\tlearn: 0.1682666\ttotal: 8.24s\tremaining: 5.54s\n",
      "598:\tlearn: 0.1681050\ttotal: 8.26s\tremaining: 5.53s\n",
      "599:\tlearn: 0.1680729\ttotal: 8.27s\tremaining: 5.51s\n",
      "600:\tlearn: 0.1679642\ttotal: 8.28s\tremaining: 5.5s\n",
      "601:\tlearn: 0.1678799\ttotal: 8.3s\tremaining: 5.49s\n",
      "602:\tlearn: 0.1678188\ttotal: 8.31s\tremaining: 5.47s\n",
      "603:\tlearn: 0.1677232\ttotal: 8.33s\tremaining: 5.46s\n",
      "604:\tlearn: 0.1677005\ttotal: 8.34s\tremaining: 5.45s\n",
      "605:\tlearn: 0.1676277\ttotal: 8.35s\tremaining: 5.43s\n",
      "606:\tlearn: 0.1675443\ttotal: 8.37s\tremaining: 5.42s\n",
      "607:\tlearn: 0.1674671\ttotal: 8.38s\tremaining: 5.4s\n",
      "608:\tlearn: 0.1673695\ttotal: 8.4s\tremaining: 5.39s\n",
      "609:\tlearn: 0.1673159\ttotal: 8.41s\tremaining: 5.38s\n",
      "610:\tlearn: 0.1672460\ttotal: 8.43s\tremaining: 5.36s\n",
      "611:\tlearn: 0.1672080\ttotal: 8.44s\tremaining: 5.35s\n",
      "612:\tlearn: 0.1671842\ttotal: 8.45s\tremaining: 5.33s\n",
      "613:\tlearn: 0.1671135\ttotal: 8.46s\tremaining: 5.32s\n",
      "614:\tlearn: 0.1670603\ttotal: 8.48s\tremaining: 5.31s\n",
      "615:\tlearn: 0.1670342\ttotal: 8.49s\tremaining: 5.29s\n",
      "616:\tlearn: 0.1669846\ttotal: 8.51s\tremaining: 5.28s\n",
      "617:\tlearn: 0.1669494\ttotal: 8.52s\tremaining: 5.26s\n",
      "618:\tlearn: 0.1668987\ttotal: 8.53s\tremaining: 5.25s\n",
      "619:\tlearn: 0.1668494\ttotal: 8.55s\tremaining: 5.24s\n",
      "620:\tlearn: 0.1668207\ttotal: 8.56s\tremaining: 5.22s\n",
      "621:\tlearn: 0.1667501\ttotal: 8.57s\tremaining: 5.21s\n",
      "622:\tlearn: 0.1666830\ttotal: 8.59s\tremaining: 5.2s\n",
      "623:\tlearn: 0.1665999\ttotal: 8.6s\tremaining: 5.18s\n",
      "624:\tlearn: 0.1664668\ttotal: 8.62s\tremaining: 5.17s\n",
      "625:\tlearn: 0.1664260\ttotal: 8.63s\tremaining: 5.16s\n",
      "626:\tlearn: 0.1663522\ttotal: 8.64s\tremaining: 5.14s\n",
      "627:\tlearn: 0.1662914\ttotal: 8.66s\tremaining: 5.13s\n",
      "628:\tlearn: 0.1662403\ttotal: 8.67s\tremaining: 5.11s\n",
      "629:\tlearn: 0.1661999\ttotal: 8.68s\tremaining: 5.1s\n",
      "630:\tlearn: 0.1661530\ttotal: 8.7s\tremaining: 5.09s\n",
      "631:\tlearn: 0.1661215\ttotal: 8.71s\tremaining: 5.07s\n",
      "632:\tlearn: 0.1660776\ttotal: 8.72s\tremaining: 5.06s\n",
      "633:\tlearn: 0.1660444\ttotal: 8.74s\tremaining: 5.04s\n",
      "634:\tlearn: 0.1659756\ttotal: 8.75s\tremaining: 5.03s\n",
      "635:\tlearn: 0.1659313\ttotal: 8.77s\tremaining: 5.02s\n",
      "636:\tlearn: 0.1658963\ttotal: 8.78s\tremaining: 5s\n",
      "637:\tlearn: 0.1658637\ttotal: 8.79s\tremaining: 4.99s\n",
      "638:\tlearn: 0.1657991\ttotal: 8.81s\tremaining: 4.97s\n",
      "639:\tlearn: 0.1657725\ttotal: 8.82s\tremaining: 4.96s\n",
      "640:\tlearn: 0.1657414\ttotal: 8.83s\tremaining: 4.95s\n",
      "641:\tlearn: 0.1656480\ttotal: 8.85s\tremaining: 4.93s\n",
      "642:\tlearn: 0.1656114\ttotal: 8.86s\tremaining: 4.92s\n",
      "643:\tlearn: 0.1655814\ttotal: 8.87s\tremaining: 4.91s\n",
      "644:\tlearn: 0.1655287\ttotal: 8.89s\tremaining: 4.89s\n",
      "645:\tlearn: 0.1654724\ttotal: 8.9s\tremaining: 4.88s\n",
      "646:\tlearn: 0.1654084\ttotal: 8.92s\tremaining: 4.87s\n",
      "647:\tlearn: 0.1653705\ttotal: 8.93s\tremaining: 4.85s\n",
      "648:\tlearn: 0.1653124\ttotal: 8.95s\tremaining: 4.84s\n",
      "649:\tlearn: 0.1652724\ttotal: 8.96s\tremaining: 4.83s\n",
      "650:\tlearn: 0.1652216\ttotal: 8.98s\tremaining: 4.81s\n",
      "651:\tlearn: 0.1651636\ttotal: 8.99s\tremaining: 4.8s\n",
      "652:\tlearn: 0.1651162\ttotal: 9.01s\tremaining: 4.79s\n",
      "653:\tlearn: 0.1650731\ttotal: 9.02s\tremaining: 4.77s\n",
      "654:\tlearn: 0.1649642\ttotal: 9.03s\tremaining: 4.76s\n",
      "655:\tlearn: 0.1649377\ttotal: 9.04s\tremaining: 4.74s\n",
      "656:\tlearn: 0.1649038\ttotal: 9.06s\tremaining: 4.73s\n",
      "657:\tlearn: 0.1648691\ttotal: 9.07s\tremaining: 4.71s\n",
      "658:\tlearn: 0.1648269\ttotal: 9.09s\tremaining: 4.7s\n",
      "659:\tlearn: 0.1647577\ttotal: 9.1s\tremaining: 4.69s\n",
      "660:\tlearn: 0.1647213\ttotal: 9.11s\tremaining: 4.67s\n",
      "661:\tlearn: 0.1646163\ttotal: 9.13s\tremaining: 4.66s\n",
      "662:\tlearn: 0.1645846\ttotal: 9.14s\tremaining: 4.65s\n",
      "663:\tlearn: 0.1645273\ttotal: 9.16s\tremaining: 4.63s\n",
      "664:\tlearn: 0.1644332\ttotal: 9.17s\tremaining: 4.62s\n",
      "665:\tlearn: 0.1643789\ttotal: 9.19s\tremaining: 4.61s\n",
      "666:\tlearn: 0.1643258\ttotal: 9.2s\tremaining: 4.59s\n",
      "667:\tlearn: 0.1642496\ttotal: 9.21s\tremaining: 4.58s\n",
      "668:\tlearn: 0.1641837\ttotal: 9.22s\tremaining: 4.56s\n",
      "669:\tlearn: 0.1641297\ttotal: 9.24s\tremaining: 4.55s\n",
      "670:\tlearn: 0.1640980\ttotal: 9.25s\tremaining: 4.54s\n",
      "671:\tlearn: 0.1640727\ttotal: 9.26s\tremaining: 4.52s\n",
      "672:\tlearn: 0.1640024\ttotal: 9.28s\tremaining: 4.51s\n",
      "673:\tlearn: 0.1639458\ttotal: 9.29s\tremaining: 4.49s\n",
      "674:\tlearn: 0.1639062\ttotal: 9.3s\tremaining: 4.48s\n",
      "675:\tlearn: 0.1638350\ttotal: 9.31s\tremaining: 4.46s\n",
      "676:\tlearn: 0.1638008\ttotal: 9.33s\tremaining: 4.45s\n",
      "677:\tlearn: 0.1637712\ttotal: 9.34s\tremaining: 4.44s\n",
      "678:\tlearn: 0.1637286\ttotal: 9.35s\tremaining: 4.42s\n",
      "679:\tlearn: 0.1636817\ttotal: 9.39s\tremaining: 4.42s\n",
      "680:\tlearn: 0.1636201\ttotal: 9.41s\tremaining: 4.41s\n",
      "681:\tlearn: 0.1635329\ttotal: 9.42s\tremaining: 4.39s\n",
      "682:\tlearn: 0.1635124\ttotal: 9.43s\tremaining: 4.38s\n",
      "683:\tlearn: 0.1634918\ttotal: 9.44s\tremaining: 4.36s\n",
      "684:\tlearn: 0.1634442\ttotal: 9.46s\tremaining: 4.35s\n",
      "685:\tlearn: 0.1633850\ttotal: 9.47s\tremaining: 4.33s\n",
      "686:\tlearn: 0.1633080\ttotal: 9.48s\tremaining: 4.32s\n",
      "687:\tlearn: 0.1632523\ttotal: 9.5s\tremaining: 4.31s\n",
      "688:\tlearn: 0.1631722\ttotal: 9.51s\tremaining: 4.29s\n",
      "689:\tlearn: 0.1631272\ttotal: 9.53s\tremaining: 4.28s\n",
      "690:\tlearn: 0.1629653\ttotal: 9.54s\tremaining: 4.26s\n",
      "691:\tlearn: 0.1629001\ttotal: 9.55s\tremaining: 4.25s\n",
      "692:\tlearn: 0.1628538\ttotal: 9.56s\tremaining: 4.24s\n",
      "693:\tlearn: 0.1627951\ttotal: 9.58s\tremaining: 4.22s\n",
      "694:\tlearn: 0.1627368\ttotal: 9.59s\tremaining: 4.21s\n",
      "695:\tlearn: 0.1627114\ttotal: 9.6s\tremaining: 4.19s\n",
      "696:\tlearn: 0.1626542\ttotal: 9.62s\tremaining: 4.18s\n",
      "697:\tlearn: 0.1625885\ttotal: 9.63s\tremaining: 4.17s\n",
      "698:\tlearn: 0.1625358\ttotal: 9.64s\tremaining: 4.15s\n",
      "699:\tlearn: 0.1624908\ttotal: 9.66s\tremaining: 4.14s\n",
      "700:\tlearn: 0.1624564\ttotal: 9.67s\tremaining: 4.12s\n",
      "701:\tlearn: 0.1624250\ttotal: 9.68s\tremaining: 4.11s\n",
      "702:\tlearn: 0.1623547\ttotal: 9.69s\tremaining: 4.09s\n",
      "703:\tlearn: 0.1623182\ttotal: 9.71s\tremaining: 4.08s\n",
      "704:\tlearn: 0.1622416\ttotal: 9.72s\tremaining: 4.07s\n",
      "705:\tlearn: 0.1622165\ttotal: 9.73s\tremaining: 4.05s\n",
      "706:\tlearn: 0.1622023\ttotal: 9.75s\tremaining: 4.04s\n",
      "707:\tlearn: 0.1621356\ttotal: 9.76s\tremaining: 4.03s\n",
      "708:\tlearn: 0.1620832\ttotal: 9.78s\tremaining: 4.01s\n",
      "709:\tlearn: 0.1620382\ttotal: 9.79s\tremaining: 4s\n",
      "710:\tlearn: 0.1619745\ttotal: 9.8s\tremaining: 3.98s\n",
      "711:\tlearn: 0.1619495\ttotal: 9.81s\tremaining: 3.97s\n",
      "712:\tlearn: 0.1619067\ttotal: 9.83s\tremaining: 3.96s\n",
      "713:\tlearn: 0.1618450\ttotal: 9.84s\tremaining: 3.94s\n",
      "714:\tlearn: 0.1618066\ttotal: 9.85s\tremaining: 3.93s\n",
      "715:\tlearn: 0.1617716\ttotal: 9.87s\tremaining: 3.91s\n",
      "716:\tlearn: 0.1617480\ttotal: 9.88s\tremaining: 3.9s\n",
      "717:\tlearn: 0.1617103\ttotal: 9.89s\tremaining: 3.89s\n",
      "718:\tlearn: 0.1615895\ttotal: 9.91s\tremaining: 3.87s\n",
      "719:\tlearn: 0.1615425\ttotal: 9.92s\tremaining: 3.86s\n",
      "720:\tlearn: 0.1615144\ttotal: 9.94s\tremaining: 3.85s\n",
      "721:\tlearn: 0.1614670\ttotal: 9.95s\tremaining: 3.83s\n",
      "722:\tlearn: 0.1614101\ttotal: 9.97s\tremaining: 3.82s\n",
      "723:\tlearn: 0.1613682\ttotal: 9.98s\tremaining: 3.81s\n",
      "724:\tlearn: 0.1613189\ttotal: 10s\tremaining: 3.79s\n",
      "725:\tlearn: 0.1612602\ttotal: 10s\tremaining: 3.78s\n",
      "726:\tlearn: 0.1611801\ttotal: 10s\tremaining: 3.77s\n",
      "727:\tlearn: 0.1611465\ttotal: 10s\tremaining: 3.75s\n",
      "728:\tlearn: 0.1611042\ttotal: 10.1s\tremaining: 3.74s\n",
      "729:\tlearn: 0.1610768\ttotal: 10.1s\tremaining: 3.73s\n",
      "730:\tlearn: 0.1610311\ttotal: 10.1s\tremaining: 3.71s\n",
      "731:\tlearn: 0.1610053\ttotal: 10.1s\tremaining: 3.7s\n",
      "732:\tlearn: 0.1609750\ttotal: 10.1s\tremaining: 3.69s\n",
      "733:\tlearn: 0.1609405\ttotal: 10.1s\tremaining: 3.67s\n",
      "734:\tlearn: 0.1608803\ttotal: 10.1s\tremaining: 3.66s\n",
      "735:\tlearn: 0.1608147\ttotal: 10.2s\tremaining: 3.65s\n",
      "736:\tlearn: 0.1607540\ttotal: 10.2s\tremaining: 3.63s\n",
      "737:\tlearn: 0.1606987\ttotal: 10.2s\tremaining: 3.62s\n",
      "738:\tlearn: 0.1606087\ttotal: 10.2s\tremaining: 3.6s\n",
      "739:\tlearn: 0.1605571\ttotal: 10.2s\tremaining: 3.59s\n",
      "740:\tlearn: 0.1605190\ttotal: 10.2s\tremaining: 3.58s\n",
      "741:\tlearn: 0.1604453\ttotal: 10.2s\tremaining: 3.56s\n",
      "742:\tlearn: 0.1603746\ttotal: 10.3s\tremaining: 3.55s\n",
      "743:\tlearn: 0.1602677\ttotal: 10.3s\tremaining: 3.53s\n",
      "744:\tlearn: 0.1602391\ttotal: 10.3s\tremaining: 3.52s\n",
      "745:\tlearn: 0.1602131\ttotal: 10.3s\tremaining: 3.51s\n",
      "746:\tlearn: 0.1601710\ttotal: 10.3s\tremaining: 3.49s\n",
      "747:\tlearn: 0.1601225\ttotal: 10.3s\tremaining: 3.48s\n",
      "748:\tlearn: 0.1600592\ttotal: 10.3s\tremaining: 3.46s\n",
      "749:\tlearn: 0.1600142\ttotal: 10.4s\tremaining: 3.45s\n",
      "750:\tlearn: 0.1599949\ttotal: 10.4s\tremaining: 3.44s\n",
      "751:\tlearn: 0.1599510\ttotal: 10.4s\tremaining: 3.42s\n",
      "752:\tlearn: 0.1599054\ttotal: 10.4s\tremaining: 3.41s\n",
      "753:\tlearn: 0.1598350\ttotal: 10.4s\tremaining: 3.4s\n",
      "754:\tlearn: 0.1597719\ttotal: 10.4s\tremaining: 3.38s\n",
      "755:\tlearn: 0.1597376\ttotal: 10.4s\tremaining: 3.37s\n",
      "756:\tlearn: 0.1597064\ttotal: 10.4s\tremaining: 3.35s\n",
      "757:\tlearn: 0.1596380\ttotal: 10.5s\tremaining: 3.34s\n",
      "758:\tlearn: 0.1596156\ttotal: 10.5s\tremaining: 3.33s\n",
      "759:\tlearn: 0.1595896\ttotal: 10.5s\tremaining: 3.31s\n",
      "760:\tlearn: 0.1595484\ttotal: 10.5s\tremaining: 3.3s\n",
      "761:\tlearn: 0.1594933\ttotal: 10.5s\tremaining: 3.29s\n",
      "762:\tlearn: 0.1594438\ttotal: 10.5s\tremaining: 3.27s\n",
      "763:\tlearn: 0.1593703\ttotal: 10.5s\tremaining: 3.26s\n",
      "764:\tlearn: 0.1593181\ttotal: 10.6s\tremaining: 3.24s\n",
      "765:\tlearn: 0.1592783\ttotal: 10.6s\tremaining: 3.23s\n",
      "766:\tlearn: 0.1592420\ttotal: 10.6s\tremaining: 3.21s\n",
      "767:\tlearn: 0.1592058\ttotal: 10.6s\tremaining: 3.2s\n",
      "768:\tlearn: 0.1591750\ttotal: 10.6s\tremaining: 3.19s\n",
      "769:\tlearn: 0.1591473\ttotal: 10.6s\tremaining: 3.17s\n",
      "770:\tlearn: 0.1591271\ttotal: 10.6s\tremaining: 3.16s\n",
      "771:\tlearn: 0.1590574\ttotal: 10.7s\tremaining: 3.15s\n",
      "772:\tlearn: 0.1590024\ttotal: 10.7s\tremaining: 3.13s\n",
      "773:\tlearn: 0.1589486\ttotal: 10.7s\tremaining: 3.12s\n",
      "774:\tlearn: 0.1589065\ttotal: 10.7s\tremaining: 3.1s\n",
      "775:\tlearn: 0.1588324\ttotal: 10.7s\tremaining: 3.09s\n",
      "776:\tlearn: 0.1587711\ttotal: 10.7s\tremaining: 3.08s\n",
      "777:\tlearn: 0.1587203\ttotal: 10.7s\tremaining: 3.06s\n",
      "778:\tlearn: 0.1586720\ttotal: 10.8s\tremaining: 3.05s\n",
      "779:\tlearn: 0.1585915\ttotal: 10.8s\tremaining: 3.04s\n",
      "780:\tlearn: 0.1585594\ttotal: 10.8s\tremaining: 3.02s\n",
      "781:\tlearn: 0.1585400\ttotal: 10.8s\tremaining: 3.01s\n",
      "782:\tlearn: 0.1585122\ttotal: 10.8s\tremaining: 2.99s\n",
      "783:\tlearn: 0.1584606\ttotal: 10.8s\tremaining: 2.98s\n",
      "784:\tlearn: 0.1584074\ttotal: 10.8s\tremaining: 2.97s\n",
      "785:\tlearn: 0.1583760\ttotal: 10.8s\tremaining: 2.95s\n",
      "786:\tlearn: 0.1583462\ttotal: 10.9s\tremaining: 2.94s\n",
      "787:\tlearn: 0.1583050\ttotal: 10.9s\tremaining: 2.92s\n",
      "788:\tlearn: 0.1582816\ttotal: 10.9s\tremaining: 2.91s\n",
      "789:\tlearn: 0.1581912\ttotal: 10.9s\tremaining: 2.9s\n",
      "790:\tlearn: 0.1580880\ttotal: 10.9s\tremaining: 2.88s\n",
      "791:\tlearn: 0.1580282\ttotal: 10.9s\tremaining: 2.87s\n",
      "792:\tlearn: 0.1579636\ttotal: 10.9s\tremaining: 2.86s\n",
      "793:\tlearn: 0.1579170\ttotal: 11s\tremaining: 2.84s\n",
      "794:\tlearn: 0.1578809\ttotal: 11s\tremaining: 2.83s\n",
      "795:\tlearn: 0.1578374\ttotal: 11s\tremaining: 2.81s\n",
      "796:\tlearn: 0.1577969\ttotal: 11s\tremaining: 2.8s\n",
      "797:\tlearn: 0.1577559\ttotal: 11s\tremaining: 2.79s\n",
      "798:\tlearn: 0.1576891\ttotal: 11s\tremaining: 2.77s\n",
      "799:\tlearn: 0.1576567\ttotal: 11s\tremaining: 2.76s\n",
      "800:\tlearn: 0.1576309\ttotal: 11s\tremaining: 2.74s\n",
      "801:\tlearn: 0.1575825\ttotal: 11.1s\tremaining: 2.73s\n",
      "802:\tlearn: 0.1575374\ttotal: 11.1s\tremaining: 2.72s\n",
      "803:\tlearn: 0.1574869\ttotal: 11.1s\tremaining: 2.7s\n",
      "804:\tlearn: 0.1574296\ttotal: 11.1s\tremaining: 2.69s\n",
      "805:\tlearn: 0.1573394\ttotal: 11.1s\tremaining: 2.67s\n",
      "806:\tlearn: 0.1572784\ttotal: 11.1s\tremaining: 2.66s\n",
      "807:\tlearn: 0.1572319\ttotal: 11.1s\tremaining: 2.65s\n",
      "808:\tlearn: 0.1571893\ttotal: 11.2s\tremaining: 2.63s\n",
      "809:\tlearn: 0.1571282\ttotal: 11.2s\tremaining: 2.62s\n",
      "810:\tlearn: 0.1570895\ttotal: 11.2s\tremaining: 2.61s\n",
      "811:\tlearn: 0.1570336\ttotal: 11.2s\tremaining: 2.59s\n",
      "812:\tlearn: 0.1570090\ttotal: 11.2s\tremaining: 2.58s\n",
      "813:\tlearn: 0.1569808\ttotal: 11.2s\tremaining: 2.56s\n",
      "814:\tlearn: 0.1569419\ttotal: 11.2s\tremaining: 2.55s\n",
      "815:\tlearn: 0.1568905\ttotal: 11.2s\tremaining: 2.54s\n",
      "816:\tlearn: 0.1568497\ttotal: 11.3s\tremaining: 2.52s\n",
      "817:\tlearn: 0.1567800\ttotal: 11.3s\tremaining: 2.51s\n",
      "818:\tlearn: 0.1567297\ttotal: 11.3s\tremaining: 2.49s\n",
      "819:\tlearn: 0.1567031\ttotal: 11.3s\tremaining: 2.48s\n",
      "820:\tlearn: 0.1566540\ttotal: 11.3s\tremaining: 2.47s\n",
      "821:\tlearn: 0.1565999\ttotal: 11.3s\tremaining: 2.45s\n",
      "822:\tlearn: 0.1565555\ttotal: 11.4s\tremaining: 2.44s\n",
      "823:\tlearn: 0.1565052\ttotal: 11.4s\tremaining: 2.43s\n",
      "824:\tlearn: 0.1564775\ttotal: 11.4s\tremaining: 2.42s\n",
      "825:\tlearn: 0.1564238\ttotal: 11.4s\tremaining: 2.41s\n",
      "826:\tlearn: 0.1563844\ttotal: 11.4s\tremaining: 2.39s\n",
      "827:\tlearn: 0.1563539\ttotal: 11.5s\tremaining: 2.38s\n",
      "828:\tlearn: 0.1563191\ttotal: 11.5s\tremaining: 2.36s\n",
      "829:\tlearn: 0.1562828\ttotal: 11.5s\tremaining: 2.35s\n",
      "830:\tlearn: 0.1562722\ttotal: 11.5s\tremaining: 2.34s\n",
      "831:\tlearn: 0.1562280\ttotal: 11.5s\tremaining: 2.32s\n",
      "832:\tlearn: 0.1561841\ttotal: 11.5s\tremaining: 2.31s\n",
      "833:\tlearn: 0.1561523\ttotal: 11.5s\tremaining: 2.29s\n",
      "834:\tlearn: 0.1561307\ttotal: 11.5s\tremaining: 2.28s\n",
      "835:\tlearn: 0.1560679\ttotal: 11.6s\tremaining: 2.27s\n",
      "836:\tlearn: 0.1560197\ttotal: 11.6s\tremaining: 2.25s\n",
      "837:\tlearn: 0.1559755\ttotal: 11.6s\tremaining: 2.24s\n",
      "838:\tlearn: 0.1559184\ttotal: 11.6s\tremaining: 2.23s\n",
      "839:\tlearn: 0.1558668\ttotal: 11.6s\tremaining: 2.21s\n",
      "840:\tlearn: 0.1558064\ttotal: 11.6s\tremaining: 2.2s\n",
      "841:\tlearn: 0.1557697\ttotal: 11.7s\tremaining: 2.19s\n",
      "842:\tlearn: 0.1557428\ttotal: 11.7s\tremaining: 2.17s\n",
      "843:\tlearn: 0.1556481\ttotal: 11.7s\tremaining: 2.16s\n",
      "844:\tlearn: 0.1556170\ttotal: 11.7s\tremaining: 2.15s\n",
      "845:\tlearn: 0.1555677\ttotal: 11.7s\tremaining: 2.13s\n",
      "846:\tlearn: 0.1555236\ttotal: 11.7s\tremaining: 2.12s\n",
      "847:\tlearn: 0.1554749\ttotal: 11.7s\tremaining: 2.1s\n",
      "848:\tlearn: 0.1553961\ttotal: 11.8s\tremaining: 2.09s\n",
      "849:\tlearn: 0.1553468\ttotal: 11.8s\tremaining: 2.08s\n",
      "850:\tlearn: 0.1553158\ttotal: 11.8s\tremaining: 2.06s\n",
      "851:\tlearn: 0.1552793\ttotal: 11.8s\tremaining: 2.05s\n",
      "852:\tlearn: 0.1552451\ttotal: 11.8s\tremaining: 2.03s\n",
      "853:\tlearn: 0.1551853\ttotal: 11.8s\tremaining: 2.02s\n",
      "854:\tlearn: 0.1551554\ttotal: 11.8s\tremaining: 2.01s\n",
      "855:\tlearn: 0.1550858\ttotal: 11.8s\tremaining: 1.99s\n",
      "856:\tlearn: 0.1550257\ttotal: 11.9s\tremaining: 1.98s\n",
      "857:\tlearn: 0.1549844\ttotal: 11.9s\tremaining: 1.96s\n",
      "858:\tlearn: 0.1549267\ttotal: 11.9s\tremaining: 1.95s\n",
      "859:\tlearn: 0.1548827\ttotal: 11.9s\tremaining: 1.94s\n",
      "860:\tlearn: 0.1548326\ttotal: 11.9s\tremaining: 1.92s\n",
      "861:\tlearn: 0.1547775\ttotal: 11.9s\tremaining: 1.91s\n",
      "862:\tlearn: 0.1547399\ttotal: 11.9s\tremaining: 1.9s\n",
      "863:\tlearn: 0.1546934\ttotal: 12s\tremaining: 1.88s\n",
      "864:\tlearn: 0.1546631\ttotal: 12s\tremaining: 1.87s\n",
      "865:\tlearn: 0.1546268\ttotal: 12s\tremaining: 1.85s\n",
      "866:\tlearn: 0.1545902\ttotal: 12s\tremaining: 1.84s\n",
      "867:\tlearn: 0.1545513\ttotal: 12s\tremaining: 1.82s\n",
      "868:\tlearn: 0.1545403\ttotal: 12s\tremaining: 1.81s\n",
      "869:\tlearn: 0.1544822\ttotal: 12s\tremaining: 1.8s\n",
      "870:\tlearn: 0.1544359\ttotal: 12s\tremaining: 1.78s\n",
      "871:\tlearn: 0.1544018\ttotal: 12.1s\tremaining: 1.77s\n",
      "872:\tlearn: 0.1543472\ttotal: 12.1s\tremaining: 1.76s\n",
      "873:\tlearn: 0.1543101\ttotal: 12.1s\tremaining: 1.74s\n",
      "874:\tlearn: 0.1542457\ttotal: 12.1s\tremaining: 1.73s\n",
      "875:\tlearn: 0.1542039\ttotal: 12.1s\tremaining: 1.72s\n",
      "876:\tlearn: 0.1541553\ttotal: 12.1s\tremaining: 1.7s\n",
      "877:\tlearn: 0.1541169\ttotal: 12.1s\tremaining: 1.69s\n",
      "878:\tlearn: 0.1540801\ttotal: 12.2s\tremaining: 1.67s\n",
      "879:\tlearn: 0.1540609\ttotal: 12.2s\tremaining: 1.66s\n",
      "880:\tlearn: 0.1539992\ttotal: 12.2s\tremaining: 1.65s\n",
      "881:\tlearn: 0.1539421\ttotal: 12.2s\tremaining: 1.63s\n",
      "882:\tlearn: 0.1539157\ttotal: 12.2s\tremaining: 1.62s\n",
      "883:\tlearn: 0.1538833\ttotal: 12.2s\tremaining: 1.6s\n",
      "884:\tlearn: 0.1538249\ttotal: 12.2s\tremaining: 1.59s\n",
      "885:\tlearn: 0.1537595\ttotal: 12.2s\tremaining: 1.58s\n",
      "886:\tlearn: 0.1537301\ttotal: 12.3s\tremaining: 1.56s\n",
      "887:\tlearn: 0.1536842\ttotal: 12.3s\tremaining: 1.55s\n",
      "888:\tlearn: 0.1536485\ttotal: 12.3s\tremaining: 1.53s\n",
      "889:\tlearn: 0.1535767\ttotal: 12.3s\tremaining: 1.52s\n",
      "890:\tlearn: 0.1535428\ttotal: 12.3s\tremaining: 1.51s\n",
      "891:\tlearn: 0.1535137\ttotal: 12.3s\tremaining: 1.49s\n",
      "892:\tlearn: 0.1534742\ttotal: 12.4s\tremaining: 1.48s\n",
      "893:\tlearn: 0.1534079\ttotal: 12.4s\tremaining: 1.47s\n",
      "894:\tlearn: 0.1533435\ttotal: 12.4s\tremaining: 1.45s\n",
      "895:\tlearn: 0.1532768\ttotal: 12.4s\tremaining: 1.44s\n",
      "896:\tlearn: 0.1531955\ttotal: 12.4s\tremaining: 1.42s\n",
      "897:\tlearn: 0.1531564\ttotal: 12.4s\tremaining: 1.41s\n",
      "898:\tlearn: 0.1531313\ttotal: 12.4s\tremaining: 1.4s\n",
      "899:\tlearn: 0.1530525\ttotal: 12.4s\tremaining: 1.38s\n",
      "900:\tlearn: 0.1530130\ttotal: 12.5s\tremaining: 1.37s\n",
      "901:\tlearn: 0.1529962\ttotal: 12.5s\tremaining: 1.35s\n",
      "902:\tlearn: 0.1529518\ttotal: 12.5s\tremaining: 1.34s\n",
      "903:\tlearn: 0.1529024\ttotal: 12.5s\tremaining: 1.33s\n",
      "904:\tlearn: 0.1528869\ttotal: 12.5s\tremaining: 1.31s\n",
      "905:\tlearn: 0.1528448\ttotal: 12.5s\tremaining: 1.3s\n",
      "906:\tlearn: 0.1527872\ttotal: 12.5s\tremaining: 1.29s\n",
      "907:\tlearn: 0.1527593\ttotal: 12.6s\tremaining: 1.27s\n",
      "908:\tlearn: 0.1527352\ttotal: 12.6s\tremaining: 1.26s\n",
      "909:\tlearn: 0.1527204\ttotal: 12.6s\tremaining: 1.24s\n",
      "910:\tlearn: 0.1526890\ttotal: 12.6s\tremaining: 1.23s\n",
      "911:\tlearn: 0.1526545\ttotal: 12.6s\tremaining: 1.22s\n",
      "912:\tlearn: 0.1526196\ttotal: 12.6s\tremaining: 1.2s\n",
      "913:\tlearn: 0.1525663\ttotal: 12.6s\tremaining: 1.19s\n",
      "914:\tlearn: 0.1525107\ttotal: 12.6s\tremaining: 1.17s\n",
      "915:\tlearn: 0.1524734\ttotal: 12.7s\tremaining: 1.16s\n",
      "916:\tlearn: 0.1524290\ttotal: 12.7s\tremaining: 1.15s\n",
      "917:\tlearn: 0.1523996\ttotal: 12.7s\tremaining: 1.13s\n",
      "918:\tlearn: 0.1523581\ttotal: 12.7s\tremaining: 1.12s\n",
      "919:\tlearn: 0.1523003\ttotal: 12.7s\tremaining: 1.1s\n",
      "920:\tlearn: 0.1522768\ttotal: 12.7s\tremaining: 1.09s\n",
      "921:\tlearn: 0.1522309\ttotal: 12.7s\tremaining: 1.08s\n",
      "922:\tlearn: 0.1521730\ttotal: 12.8s\tremaining: 1.06s\n",
      "923:\tlearn: 0.1521270\ttotal: 12.8s\tremaining: 1.05s\n",
      "924:\tlearn: 0.1520908\ttotal: 12.8s\tremaining: 1.04s\n",
      "925:\tlearn: 0.1520443\ttotal: 12.8s\tremaining: 1.02s\n",
      "926:\tlearn: 0.1519934\ttotal: 12.8s\tremaining: 1.01s\n",
      "927:\tlearn: 0.1519650\ttotal: 12.8s\tremaining: 994ms\n",
      "928:\tlearn: 0.1518631\ttotal: 12.8s\tremaining: 981ms\n",
      "929:\tlearn: 0.1518031\ttotal: 12.8s\tremaining: 967ms\n",
      "930:\tlearn: 0.1517821\ttotal: 12.9s\tremaining: 953ms\n",
      "931:\tlearn: 0.1517681\ttotal: 12.9s\tremaining: 939ms\n",
      "932:\tlearn: 0.1517130\ttotal: 12.9s\tremaining: 925ms\n",
      "933:\tlearn: 0.1517030\ttotal: 12.9s\tremaining: 911ms\n",
      "934:\tlearn: 0.1516475\ttotal: 12.9s\tremaining: 898ms\n",
      "935:\tlearn: 0.1516309\ttotal: 12.9s\tremaining: 884ms\n",
      "936:\tlearn: 0.1515776\ttotal: 12.9s\tremaining: 870ms\n",
      "937:\tlearn: 0.1515388\ttotal: 13s\tremaining: 856ms\n",
      "938:\tlearn: 0.1513833\ttotal: 13s\tremaining: 843ms\n",
      "939:\tlearn: 0.1513080\ttotal: 13s\tremaining: 829ms\n",
      "940:\tlearn: 0.1512631\ttotal: 13s\tremaining: 815ms\n",
      "941:\tlearn: 0.1512359\ttotal: 13s\tremaining: 801ms\n",
      "942:\tlearn: 0.1512019\ttotal: 13s\tremaining: 787ms\n",
      "943:\tlearn: 0.1511496\ttotal: 13.1s\tremaining: 775ms\n",
      "944:\tlearn: 0.1511221\ttotal: 13.1s\tremaining: 761ms\n",
      "945:\tlearn: 0.1511012\ttotal: 13.1s\tremaining: 748ms\n",
      "946:\tlearn: 0.1510566\ttotal: 13.1s\tremaining: 734ms\n",
      "947:\tlearn: 0.1510380\ttotal: 13.1s\tremaining: 720ms\n",
      "948:\tlearn: 0.1509861\ttotal: 13.1s\tremaining: 706ms\n",
      "949:\tlearn: 0.1509618\ttotal: 13.1s\tremaining: 692ms\n",
      "950:\tlearn: 0.1509285\ttotal: 13.2s\tremaining: 678ms\n",
      "951:\tlearn: 0.1508855\ttotal: 13.2s\tremaining: 664ms\n",
      "952:\tlearn: 0.1508710\ttotal: 13.2s\tremaining: 650ms\n",
      "953:\tlearn: 0.1508537\ttotal: 13.2s\tremaining: 636ms\n",
      "954:\tlearn: 0.1507501\ttotal: 13.2s\tremaining: 622ms\n",
      "955:\tlearn: 0.1507211\ttotal: 13.2s\tremaining: 609ms\n",
      "956:\tlearn: 0.1506534\ttotal: 13.2s\tremaining: 595ms\n",
      "957:\tlearn: 0.1506175\ttotal: 13.3s\tremaining: 581ms\n",
      "958:\tlearn: 0.1506053\ttotal: 13.3s\tremaining: 567ms\n",
      "959:\tlearn: 0.1505562\ttotal: 13.3s\tremaining: 553ms\n",
      "960:\tlearn: 0.1505264\ttotal: 13.3s\tremaining: 539ms\n",
      "961:\tlearn: 0.1505120\ttotal: 13.3s\tremaining: 526ms\n",
      "962:\tlearn: 0.1504620\ttotal: 13.3s\tremaining: 512ms\n",
      "963:\tlearn: 0.1503838\ttotal: 13.3s\tremaining: 498ms\n",
      "964:\tlearn: 0.1503519\ttotal: 13.4s\tremaining: 484ms\n",
      "965:\tlearn: 0.1503183\ttotal: 13.4s\tremaining: 470ms\n",
      "966:\tlearn: 0.1502918\ttotal: 13.4s\tremaining: 457ms\n",
      "967:\tlearn: 0.1502718\ttotal: 13.4s\tremaining: 443ms\n",
      "968:\tlearn: 0.1502076\ttotal: 13.4s\tremaining: 429ms\n",
      "969:\tlearn: 0.1501933\ttotal: 13.4s\tremaining: 415ms\n",
      "970:\tlearn: 0.1501556\ttotal: 13.4s\tremaining: 401ms\n",
      "971:\tlearn: 0.1501248\ttotal: 13.4s\tremaining: 387ms\n",
      "972:\tlearn: 0.1500641\ttotal: 13.5s\tremaining: 373ms\n",
      "973:\tlearn: 0.1500439\ttotal: 13.5s\tremaining: 360ms\n",
      "974:\tlearn: 0.1500247\ttotal: 13.5s\tremaining: 346ms\n",
      "975:\tlearn: 0.1499875\ttotal: 13.5s\tremaining: 332ms\n",
      "976:\tlearn: 0.1499392\ttotal: 13.5s\tremaining: 318ms\n",
      "977:\tlearn: 0.1498767\ttotal: 13.5s\tremaining: 304ms\n",
      "978:\tlearn: 0.1498354\ttotal: 13.5s\tremaining: 290ms\n",
      "979:\tlearn: 0.1498001\ttotal: 13.6s\tremaining: 277ms\n",
      "980:\tlearn: 0.1497561\ttotal: 13.6s\tremaining: 263ms\n",
      "981:\tlearn: 0.1497058\ttotal: 13.6s\tremaining: 249ms\n",
      "982:\tlearn: 0.1496759\ttotal: 13.6s\tremaining: 235ms\n",
      "983:\tlearn: 0.1496143\ttotal: 13.6s\tremaining: 221ms\n",
      "984:\tlearn: 0.1495684\ttotal: 13.6s\tremaining: 207ms\n",
      "985:\tlearn: 0.1495240\ttotal: 13.6s\tremaining: 194ms\n",
      "986:\tlearn: 0.1494858\ttotal: 13.6s\tremaining: 180ms\n",
      "987:\tlearn: 0.1494404\ttotal: 13.7s\tremaining: 166ms\n",
      "988:\tlearn: 0.1494159\ttotal: 13.7s\tremaining: 152ms\n",
      "989:\tlearn: 0.1494041\ttotal: 13.7s\tremaining: 138ms\n",
      "990:\tlearn: 0.1493507\ttotal: 13.7s\tremaining: 124ms\n",
      "991:\tlearn: 0.1493139\ttotal: 13.7s\tremaining: 111ms\n",
      "992:\tlearn: 0.1492708\ttotal: 13.7s\tremaining: 96.8ms\n",
      "993:\tlearn: 0.1492252\ttotal: 13.7s\tremaining: 82.9ms\n",
      "994:\tlearn: 0.1491918\ttotal: 13.8s\tremaining: 69.1ms\n",
      "995:\tlearn: 0.1491711\ttotal: 13.8s\tremaining: 55.3ms\n",
      "996:\tlearn: 0.1491249\ttotal: 13.8s\tremaining: 41.5ms\n",
      "997:\tlearn: 0.1490970\ttotal: 13.8s\tremaining: 27.6ms\n",
      "998:\tlearn: 0.1490401\ttotal: 13.8s\tremaining: 13.8ms\n",
      "999:\tlearn: 0.1489802\ttotal: 13.8s\tremaining: 0us\n",
      "Learning rate set to 0.070522\n",
      "0:\tlearn: 0.6517385\ttotal: 42.2ms\tremaining: 42.2s\n",
      "1:\tlearn: 0.6139475\ttotal: 56.9ms\tremaining: 28.4s\n",
      "2:\tlearn: 0.5919356\ttotal: 71.3ms\tremaining: 23.7s\n",
      "3:\tlearn: 0.5723391\ttotal: 84.5ms\tremaining: 21s\n",
      "4:\tlearn: 0.5572293\ttotal: 99ms\tremaining: 19.7s\n",
      "5:\tlearn: 0.5453298\ttotal: 113ms\tremaining: 18.7s\n",
      "6:\tlearn: 0.5099769\ttotal: 126ms\tremaining: 17.9s\n",
      "7:\tlearn: 0.5019940\ttotal: 139ms\tremaining: 17.3s\n",
      "8:\tlearn: 0.4852628\ttotal: 153ms\tremaining: 16.8s\n",
      "9:\tlearn: 0.4799570\ttotal: 165ms\tremaining: 16.3s\n",
      "10:\tlearn: 0.4724552\ttotal: 177ms\tremaining: 15.9s\n",
      "11:\tlearn: 0.4605916\ttotal: 191ms\tremaining: 15.7s\n",
      "12:\tlearn: 0.4570600\ttotal: 203ms\tremaining: 15.4s\n",
      "13:\tlearn: 0.4470236\ttotal: 218ms\tremaining: 15.3s\n",
      "14:\tlearn: 0.4398896\ttotal: 232ms\tremaining: 15.2s\n",
      "15:\tlearn: 0.4353691\ttotal: 246ms\tremaining: 15.1s\n",
      "16:\tlearn: 0.4313656\ttotal: 258ms\tremaining: 14.9s\n",
      "17:\tlearn: 0.4236416\ttotal: 273ms\tremaining: 14.9s\n",
      "18:\tlearn: 0.4211338\ttotal: 292ms\tremaining: 15.1s\n",
      "19:\tlearn: 0.4148331\ttotal: 309ms\tremaining: 15.1s\n",
      "20:\tlearn: 0.4099860\ttotal: 326ms\tremaining: 15.2s\n",
      "21:\tlearn: 0.4076479\ttotal: 338ms\tremaining: 15s\n",
      "22:\tlearn: 0.4044061\ttotal: 351ms\tremaining: 14.9s\n",
      "23:\tlearn: 0.3910635\ttotal: 363ms\tremaining: 14.8s\n",
      "24:\tlearn: 0.3851209\ttotal: 377ms\tremaining: 14.7s\n",
      "25:\tlearn: 0.3804809\ttotal: 391ms\tremaining: 14.6s\n",
      "26:\tlearn: 0.3763790\ttotal: 403ms\tremaining: 14.5s\n",
      "27:\tlearn: 0.3719932\ttotal: 416ms\tremaining: 14.5s\n",
      "28:\tlearn: 0.3672612\ttotal: 431ms\tremaining: 14.4s\n",
      "29:\tlearn: 0.3646415\ttotal: 444ms\tremaining: 14.4s\n",
      "30:\tlearn: 0.3605190\ttotal: 456ms\tremaining: 14.3s\n",
      "31:\tlearn: 0.3585691\ttotal: 469ms\tremaining: 14.2s\n",
      "32:\tlearn: 0.3565173\ttotal: 482ms\tremaining: 14.1s\n",
      "33:\tlearn: 0.3532838\ttotal: 495ms\tremaining: 14.1s\n",
      "34:\tlearn: 0.3504861\ttotal: 507ms\tremaining: 14s\n",
      "35:\tlearn: 0.3487777\ttotal: 519ms\tremaining: 13.9s\n",
      "36:\tlearn: 0.3435301\ttotal: 533ms\tremaining: 13.9s\n",
      "37:\tlearn: 0.3402812\ttotal: 548ms\tremaining: 13.9s\n",
      "38:\tlearn: 0.3378757\ttotal: 560ms\tremaining: 13.8s\n",
      "39:\tlearn: 0.3363769\ttotal: 572ms\tremaining: 13.7s\n",
      "40:\tlearn: 0.3349307\ttotal: 585ms\tremaining: 13.7s\n",
      "41:\tlearn: 0.3316013\ttotal: 601ms\tremaining: 13.7s\n",
      "42:\tlearn: 0.3307784\ttotal: 617ms\tremaining: 13.7s\n",
      "43:\tlearn: 0.3271273\ttotal: 631ms\tremaining: 13.7s\n",
      "44:\tlearn: 0.3229616\ttotal: 645ms\tremaining: 13.7s\n",
      "45:\tlearn: 0.3208215\ttotal: 658ms\tremaining: 13.6s\n",
      "46:\tlearn: 0.3195820\ttotal: 672ms\tremaining: 13.6s\n",
      "47:\tlearn: 0.3156788\ttotal: 685ms\tremaining: 13.6s\n",
      "48:\tlearn: 0.3114219\ttotal: 699ms\tremaining: 13.6s\n",
      "49:\tlearn: 0.3082701\ttotal: 713ms\tremaining: 13.5s\n",
      "50:\tlearn: 0.3065109\ttotal: 728ms\tremaining: 13.5s\n",
      "51:\tlearn: 0.3029613\ttotal: 742ms\tremaining: 13.5s\n",
      "52:\tlearn: 0.3003139\ttotal: 755ms\tremaining: 13.5s\n",
      "53:\tlearn: 0.2980970\ttotal: 768ms\tremaining: 13.4s\n",
      "54:\tlearn: 0.2971064\ttotal: 781ms\tremaining: 13.4s\n",
      "55:\tlearn: 0.2960976\ttotal: 794ms\tremaining: 13.4s\n",
      "56:\tlearn: 0.2949384\ttotal: 808ms\tremaining: 13.4s\n",
      "57:\tlearn: 0.2938986\ttotal: 821ms\tremaining: 13.3s\n",
      "58:\tlearn: 0.2924581\ttotal: 837ms\tremaining: 13.3s\n",
      "59:\tlearn: 0.2915721\ttotal: 850ms\tremaining: 13.3s\n",
      "60:\tlearn: 0.2901546\ttotal: 862ms\tremaining: 13.3s\n",
      "61:\tlearn: 0.2859398\ttotal: 876ms\tremaining: 13.2s\n",
      "62:\tlearn: 0.2837729\ttotal: 888ms\tremaining: 13.2s\n",
      "63:\tlearn: 0.2832281\ttotal: 902ms\tremaining: 13.2s\n",
      "64:\tlearn: 0.2819120\ttotal: 914ms\tremaining: 13.2s\n",
      "65:\tlearn: 0.2798831\ttotal: 927ms\tremaining: 13.1s\n",
      "66:\tlearn: 0.2787408\ttotal: 939ms\tremaining: 13.1s\n",
      "67:\tlearn: 0.2771383\ttotal: 953ms\tremaining: 13.1s\n",
      "68:\tlearn: 0.2759343\ttotal: 965ms\tremaining: 13s\n",
      "69:\tlearn: 0.2730226\ttotal: 979ms\tremaining: 13s\n",
      "70:\tlearn: 0.2716978\ttotal: 994ms\tremaining: 13s\n",
      "71:\tlearn: 0.2693411\ttotal: 1.01s\tremaining: 13s\n",
      "72:\tlearn: 0.2682754\ttotal: 1.03s\tremaining: 13s\n",
      "73:\tlearn: 0.2661680\ttotal: 1.04s\tremaining: 13s\n",
      "74:\tlearn: 0.2658188\ttotal: 1.05s\tremaining: 13s\n",
      "75:\tlearn: 0.2651150\ttotal: 1.07s\tremaining: 13s\n",
      "76:\tlearn: 0.2643018\ttotal: 1.08s\tremaining: 13s\n",
      "77:\tlearn: 0.2640128\ttotal: 1.09s\tremaining: 12.9s\n",
      "78:\tlearn: 0.2637349\ttotal: 1.11s\tremaining: 12.9s\n",
      "79:\tlearn: 0.2630723\ttotal: 1.12s\tremaining: 12.9s\n",
      "80:\tlearn: 0.2623878\ttotal: 1.13s\tremaining: 12.9s\n",
      "81:\tlearn: 0.2602345\ttotal: 1.15s\tremaining: 12.9s\n",
      "82:\tlearn: 0.2592805\ttotal: 1.16s\tremaining: 12.8s\n",
      "83:\tlearn: 0.2581234\ttotal: 1.17s\tremaining: 12.8s\n",
      "84:\tlearn: 0.2572823\ttotal: 1.19s\tremaining: 12.8s\n",
      "85:\tlearn: 0.2568084\ttotal: 1.2s\tremaining: 12.8s\n",
      "86:\tlearn: 0.2562110\ttotal: 1.22s\tremaining: 12.8s\n",
      "87:\tlearn: 0.2553664\ttotal: 1.23s\tremaining: 12.7s\n",
      "88:\tlearn: 0.2546224\ttotal: 1.25s\tremaining: 12.8s\n",
      "89:\tlearn: 0.2541531\ttotal: 1.27s\tremaining: 12.8s\n",
      "90:\tlearn: 0.2528304\ttotal: 1.28s\tremaining: 12.8s\n",
      "91:\tlearn: 0.2516933\ttotal: 1.3s\tremaining: 12.8s\n",
      "92:\tlearn: 0.2509899\ttotal: 1.31s\tremaining: 12.8s\n",
      "93:\tlearn: 0.2495935\ttotal: 1.32s\tremaining: 12.7s\n",
      "94:\tlearn: 0.2492886\ttotal: 1.33s\tremaining: 12.7s\n",
      "95:\tlearn: 0.2490411\ttotal: 1.35s\tremaining: 12.7s\n",
      "96:\tlearn: 0.2485289\ttotal: 1.36s\tremaining: 12.7s\n",
      "97:\tlearn: 0.2468831\ttotal: 1.37s\tremaining: 12.7s\n",
      "98:\tlearn: 0.2457392\ttotal: 1.39s\tremaining: 12.6s\n",
      "99:\tlearn: 0.2454035\ttotal: 1.4s\tremaining: 12.6s\n",
      "100:\tlearn: 0.2450114\ttotal: 1.42s\tremaining: 12.6s\n",
      "101:\tlearn: 0.2441232\ttotal: 1.43s\tremaining: 12.6s\n",
      "102:\tlearn: 0.2437168\ttotal: 1.44s\tremaining: 12.6s\n",
      "103:\tlearn: 0.2433953\ttotal: 1.45s\tremaining: 12.5s\n",
      "104:\tlearn: 0.2431240\ttotal: 1.47s\tremaining: 12.5s\n",
      "105:\tlearn: 0.2426658\ttotal: 1.48s\tremaining: 12.5s\n",
      "106:\tlearn: 0.2424785\ttotal: 1.49s\tremaining: 12.4s\n",
      "107:\tlearn: 0.2417499\ttotal: 1.5s\tremaining: 12.4s\n",
      "108:\tlearn: 0.2413800\ttotal: 1.52s\tremaining: 12.4s\n",
      "109:\tlearn: 0.2407854\ttotal: 1.53s\tremaining: 12.4s\n",
      "110:\tlearn: 0.2405516\ttotal: 1.54s\tremaining: 12.4s\n",
      "111:\tlearn: 0.2403727\ttotal: 1.56s\tremaining: 12.3s\n",
      "112:\tlearn: 0.2400186\ttotal: 1.57s\tremaining: 12.3s\n",
      "113:\tlearn: 0.2397196\ttotal: 1.58s\tremaining: 12.3s\n",
      "114:\tlearn: 0.2390937\ttotal: 1.6s\tremaining: 12.3s\n",
      "115:\tlearn: 0.2387346\ttotal: 1.61s\tremaining: 12.3s\n",
      "116:\tlearn: 0.2386138\ttotal: 1.63s\tremaining: 12.3s\n",
      "117:\tlearn: 0.2384436\ttotal: 1.64s\tremaining: 12.3s\n",
      "118:\tlearn: 0.2382321\ttotal: 1.66s\tremaining: 12.3s\n",
      "119:\tlearn: 0.2377719\ttotal: 1.67s\tremaining: 12.2s\n",
      "120:\tlearn: 0.2371593\ttotal: 1.68s\tremaining: 12.2s\n",
      "121:\tlearn: 0.2368713\ttotal: 1.7s\tremaining: 12.2s\n",
      "122:\tlearn: 0.2367284\ttotal: 1.71s\tremaining: 12.2s\n",
      "123:\tlearn: 0.2365728\ttotal: 1.72s\tremaining: 12.2s\n",
      "124:\tlearn: 0.2363865\ttotal: 1.74s\tremaining: 12.2s\n",
      "125:\tlearn: 0.2360657\ttotal: 1.75s\tremaining: 12.1s\n",
      "126:\tlearn: 0.2355548\ttotal: 1.76s\tremaining: 12.1s\n",
      "127:\tlearn: 0.2350825\ttotal: 1.77s\tremaining: 12.1s\n",
      "128:\tlearn: 0.2339279\ttotal: 1.79s\tremaining: 12.1s\n",
      "129:\tlearn: 0.2335551\ttotal: 1.8s\tremaining: 12.1s\n",
      "130:\tlearn: 0.2330900\ttotal: 1.82s\tremaining: 12.1s\n",
      "131:\tlearn: 0.2328888\ttotal: 1.83s\tremaining: 12s\n",
      "132:\tlearn: 0.2318067\ttotal: 1.84s\tremaining: 12s\n",
      "133:\tlearn: 0.2309193\ttotal: 1.86s\tremaining: 12s\n",
      "134:\tlearn: 0.2304848\ttotal: 1.87s\tremaining: 12s\n",
      "135:\tlearn: 0.2299906\ttotal: 1.89s\tremaining: 12s\n",
      "136:\tlearn: 0.2297147\ttotal: 1.9s\tremaining: 12s\n",
      "137:\tlearn: 0.2295726\ttotal: 1.91s\tremaining: 11.9s\n",
      "138:\tlearn: 0.2288682\ttotal: 1.93s\tremaining: 11.9s\n",
      "139:\tlearn: 0.2282837\ttotal: 1.94s\tremaining: 11.9s\n",
      "140:\tlearn: 0.2279738\ttotal: 1.95s\tremaining: 11.9s\n",
      "141:\tlearn: 0.2275445\ttotal: 1.96s\tremaining: 11.9s\n",
      "142:\tlearn: 0.2274075\ttotal: 1.98s\tremaining: 11.9s\n",
      "143:\tlearn: 0.2268820\ttotal: 2s\tremaining: 11.9s\n",
      "144:\tlearn: 0.2263053\ttotal: 2.01s\tremaining: 11.9s\n",
      "145:\tlearn: 0.2259891\ttotal: 2.03s\tremaining: 11.9s\n",
      "146:\tlearn: 0.2258368\ttotal: 2.04s\tremaining: 11.9s\n",
      "147:\tlearn: 0.2253091\ttotal: 2.06s\tremaining: 11.8s\n",
      "148:\tlearn: 0.2251661\ttotal: 2.07s\tremaining: 11.8s\n",
      "149:\tlearn: 0.2248310\ttotal: 2.08s\tremaining: 11.8s\n",
      "150:\tlearn: 0.2246183\ttotal: 2.09s\tremaining: 11.8s\n",
      "151:\tlearn: 0.2242154\ttotal: 2.11s\tremaining: 11.8s\n",
      "152:\tlearn: 0.2241013\ttotal: 2.12s\tremaining: 11.7s\n",
      "153:\tlearn: 0.2238251\ttotal: 2.13s\tremaining: 11.7s\n",
      "154:\tlearn: 0.2236769\ttotal: 2.15s\tremaining: 11.7s\n",
      "155:\tlearn: 0.2232391\ttotal: 2.16s\tremaining: 11.7s\n",
      "156:\tlearn: 0.2231212\ttotal: 2.17s\tremaining: 11.7s\n",
      "157:\tlearn: 0.2229211\ttotal: 2.19s\tremaining: 11.7s\n",
      "158:\tlearn: 0.2222567\ttotal: 2.2s\tremaining: 11.6s\n",
      "159:\tlearn: 0.2216878\ttotal: 2.22s\tremaining: 11.6s\n",
      "160:\tlearn: 0.2213245\ttotal: 2.23s\tremaining: 11.6s\n",
      "161:\tlearn: 0.2212056\ttotal: 2.24s\tremaining: 11.6s\n",
      "162:\tlearn: 0.2208374\ttotal: 2.26s\tremaining: 11.6s\n",
      "163:\tlearn: 0.2205861\ttotal: 2.27s\tremaining: 11.6s\n",
      "164:\tlearn: 0.2200733\ttotal: 2.29s\tremaining: 11.6s\n",
      "165:\tlearn: 0.2198723\ttotal: 2.3s\tremaining: 11.5s\n",
      "166:\tlearn: 0.2196895\ttotal: 2.31s\tremaining: 11.5s\n",
      "167:\tlearn: 0.2192762\ttotal: 2.32s\tremaining: 11.5s\n",
      "168:\tlearn: 0.2191109\ttotal: 2.34s\tremaining: 11.5s\n",
      "169:\tlearn: 0.2188361\ttotal: 2.35s\tremaining: 11.5s\n",
      "170:\tlearn: 0.2187297\ttotal: 2.36s\tremaining: 11.5s\n",
      "171:\tlearn: 0.2183475\ttotal: 2.38s\tremaining: 11.4s\n",
      "172:\tlearn: 0.2181930\ttotal: 2.39s\tremaining: 11.4s\n",
      "173:\tlearn: 0.2179609\ttotal: 2.4s\tremaining: 11.4s\n",
      "174:\tlearn: 0.2176623\ttotal: 2.41s\tremaining: 11.4s\n",
      "175:\tlearn: 0.2169229\ttotal: 2.43s\tremaining: 11.4s\n",
      "176:\tlearn: 0.2164157\ttotal: 2.44s\tremaining: 11.4s\n",
      "177:\tlearn: 0.2161865\ttotal: 2.46s\tremaining: 11.3s\n",
      "178:\tlearn: 0.2158162\ttotal: 2.47s\tremaining: 11.3s\n",
      "179:\tlearn: 0.2156195\ttotal: 2.48s\tremaining: 11.3s\n",
      "180:\tlearn: 0.2155041\ttotal: 2.49s\tremaining: 11.3s\n",
      "181:\tlearn: 0.2154196\ttotal: 2.5s\tremaining: 11.3s\n",
      "182:\tlearn: 0.2148294\ttotal: 2.52s\tremaining: 11.2s\n",
      "183:\tlearn: 0.2147170\ttotal: 2.53s\tremaining: 11.2s\n",
      "184:\tlearn: 0.2145812\ttotal: 2.54s\tremaining: 11.2s\n",
      "185:\tlearn: 0.2144411\ttotal: 2.56s\tremaining: 11.2s\n",
      "186:\tlearn: 0.2142265\ttotal: 2.57s\tremaining: 11.2s\n",
      "187:\tlearn: 0.2137176\ttotal: 2.59s\tremaining: 11.2s\n",
      "188:\tlearn: 0.2133664\ttotal: 2.6s\tremaining: 11.2s\n",
      "189:\tlearn: 0.2131223\ttotal: 2.62s\tremaining: 11.2s\n",
      "190:\tlearn: 0.2128231\ttotal: 2.63s\tremaining: 11.2s\n",
      "191:\tlearn: 0.2126670\ttotal: 2.65s\tremaining: 11.1s\n",
      "192:\tlearn: 0.2117012\ttotal: 2.66s\tremaining: 11.1s\n",
      "193:\tlearn: 0.2116013\ttotal: 2.67s\tremaining: 11.1s\n",
      "194:\tlearn: 0.2112713\ttotal: 2.69s\tremaining: 11.1s\n",
      "195:\tlearn: 0.2110126\ttotal: 2.7s\tremaining: 11.1s\n",
      "196:\tlearn: 0.2103557\ttotal: 2.71s\tremaining: 11.1s\n",
      "197:\tlearn: 0.2102414\ttotal: 2.73s\tremaining: 11.1s\n",
      "198:\tlearn: 0.2099348\ttotal: 2.74s\tremaining: 11s\n",
      "199:\tlearn: 0.2094445\ttotal: 2.75s\tremaining: 11s\n",
      "200:\tlearn: 0.2089650\ttotal: 2.77s\tremaining: 11s\n",
      "201:\tlearn: 0.2086303\ttotal: 2.78s\tremaining: 11s\n",
      "202:\tlearn: 0.2083155\ttotal: 2.79s\tremaining: 11s\n",
      "203:\tlearn: 0.2080462\ttotal: 2.81s\tremaining: 11s\n",
      "204:\tlearn: 0.2075996\ttotal: 2.82s\tremaining: 10.9s\n",
      "205:\tlearn: 0.2073820\ttotal: 2.83s\tremaining: 10.9s\n",
      "206:\tlearn: 0.2072686\ttotal: 2.85s\tremaining: 10.9s\n",
      "207:\tlearn: 0.2071769\ttotal: 2.86s\tremaining: 10.9s\n",
      "208:\tlearn: 0.2068121\ttotal: 2.87s\tremaining: 10.9s\n",
      "209:\tlearn: 0.2065753\ttotal: 2.88s\tremaining: 10.9s\n",
      "210:\tlearn: 0.2063457\ttotal: 2.9s\tremaining: 10.8s\n",
      "211:\tlearn: 0.2061301\ttotal: 2.91s\tremaining: 10.8s\n",
      "212:\tlearn: 0.2053925\ttotal: 2.92s\tremaining: 10.8s\n",
      "213:\tlearn: 0.2050752\ttotal: 2.94s\tremaining: 10.8s\n",
      "214:\tlearn: 0.2045953\ttotal: 2.95s\tremaining: 10.8s\n",
      "215:\tlearn: 0.2041920\ttotal: 2.96s\tremaining: 10.8s\n",
      "216:\tlearn: 0.2036222\ttotal: 2.98s\tremaining: 10.7s\n",
      "217:\tlearn: 0.2035664\ttotal: 2.99s\tremaining: 10.7s\n",
      "218:\tlearn: 0.2034829\ttotal: 3.01s\tremaining: 10.7s\n",
      "219:\tlearn: 0.2031573\ttotal: 3.02s\tremaining: 10.7s\n",
      "220:\tlearn: 0.2030696\ttotal: 3.04s\tremaining: 10.7s\n",
      "221:\tlearn: 0.2028426\ttotal: 3.05s\tremaining: 10.7s\n",
      "222:\tlearn: 0.2027063\ttotal: 3.06s\tremaining: 10.7s\n",
      "223:\tlearn: 0.2026027\ttotal: 3.08s\tremaining: 10.7s\n",
      "224:\tlearn: 0.2024002\ttotal: 3.09s\tremaining: 10.6s\n",
      "225:\tlearn: 0.2022172\ttotal: 3.1s\tremaining: 10.6s\n",
      "226:\tlearn: 0.2020206\ttotal: 3.12s\tremaining: 10.6s\n",
      "227:\tlearn: 0.2017324\ttotal: 3.13s\tremaining: 10.6s\n",
      "228:\tlearn: 0.2012671\ttotal: 3.14s\tremaining: 10.6s\n",
      "229:\tlearn: 0.2010164\ttotal: 3.16s\tremaining: 10.6s\n",
      "230:\tlearn: 0.2008809\ttotal: 3.17s\tremaining: 10.6s\n",
      "231:\tlearn: 0.2005263\ttotal: 3.18s\tremaining: 10.5s\n",
      "232:\tlearn: 0.2003649\ttotal: 3.2s\tremaining: 10.5s\n",
      "233:\tlearn: 0.2002641\ttotal: 3.21s\tremaining: 10.5s\n",
      "234:\tlearn: 0.1996894\ttotal: 3.23s\tremaining: 10.5s\n",
      "235:\tlearn: 0.1996053\ttotal: 3.24s\tremaining: 10.5s\n",
      "236:\tlearn: 0.1994817\ttotal: 3.25s\tremaining: 10.5s\n",
      "237:\tlearn: 0.1992674\ttotal: 3.27s\tremaining: 10.5s\n",
      "238:\tlearn: 0.1989832\ttotal: 3.28s\tremaining: 10.5s\n",
      "239:\tlearn: 0.1988999\ttotal: 3.29s\tremaining: 10.4s\n",
      "240:\tlearn: 0.1988017\ttotal: 3.31s\tremaining: 10.4s\n",
      "241:\tlearn: 0.1987176\ttotal: 3.32s\tremaining: 10.4s\n",
      "242:\tlearn: 0.1986203\ttotal: 3.33s\tremaining: 10.4s\n",
      "243:\tlearn: 0.1986203\ttotal: 3.35s\tremaining: 10.4s\n",
      "244:\tlearn: 0.1985630\ttotal: 3.36s\tremaining: 10.3s\n",
      "245:\tlearn: 0.1984578\ttotal: 3.37s\tremaining: 10.3s\n",
      "246:\tlearn: 0.1983723\ttotal: 3.38s\tremaining: 10.3s\n",
      "247:\tlearn: 0.1982932\ttotal: 3.4s\tremaining: 10.3s\n",
      "248:\tlearn: 0.1982190\ttotal: 3.41s\tremaining: 10.3s\n",
      "249:\tlearn: 0.1981785\ttotal: 3.43s\tremaining: 10.3s\n",
      "250:\tlearn: 0.1980754\ttotal: 3.44s\tremaining: 10.3s\n",
      "251:\tlearn: 0.1978152\ttotal: 3.45s\tremaining: 10.3s\n",
      "252:\tlearn: 0.1974591\ttotal: 3.47s\tremaining: 10.2s\n",
      "253:\tlearn: 0.1973823\ttotal: 3.48s\tremaining: 10.2s\n",
      "254:\tlearn: 0.1971616\ttotal: 3.49s\tremaining: 10.2s\n",
      "255:\tlearn: 0.1970001\ttotal: 3.51s\tremaining: 10.2s\n",
      "256:\tlearn: 0.1968586\ttotal: 3.52s\tremaining: 10.2s\n",
      "257:\tlearn: 0.1966875\ttotal: 3.53s\tremaining: 10.2s\n",
      "258:\tlearn: 0.1966864\ttotal: 3.54s\tremaining: 10.1s\n",
      "259:\tlearn: 0.1965874\ttotal: 3.56s\tremaining: 10.1s\n",
      "260:\tlearn: 0.1963365\ttotal: 3.58s\tremaining: 10.1s\n",
      "261:\tlearn: 0.1960773\ttotal: 3.59s\tremaining: 10.1s\n",
      "262:\tlearn: 0.1959218\ttotal: 3.6s\tremaining: 10.1s\n",
      "263:\tlearn: 0.1957808\ttotal: 3.62s\tremaining: 10.1s\n",
      "264:\tlearn: 0.1956267\ttotal: 3.63s\tremaining: 10.1s\n",
      "265:\tlearn: 0.1953935\ttotal: 3.64s\tremaining: 10.1s\n",
      "266:\tlearn: 0.1950705\ttotal: 3.66s\tremaining: 10s\n",
      "267:\tlearn: 0.1949311\ttotal: 3.67s\tremaining: 10s\n",
      "268:\tlearn: 0.1947002\ttotal: 3.69s\tremaining: 10s\n",
      "269:\tlearn: 0.1946024\ttotal: 3.7s\tremaining: 10s\n",
      "270:\tlearn: 0.1945508\ttotal: 3.71s\tremaining: 9.99s\n",
      "271:\tlearn: 0.1944502\ttotal: 3.73s\tremaining: 9.97s\n",
      "272:\tlearn: 0.1943549\ttotal: 3.74s\tremaining: 9.96s\n",
      "273:\tlearn: 0.1942305\ttotal: 3.75s\tremaining: 9.94s\n",
      "274:\tlearn: 0.1940891\ttotal: 3.77s\tremaining: 9.93s\n",
      "275:\tlearn: 0.1938923\ttotal: 3.8s\tremaining: 9.97s\n",
      "276:\tlearn: 0.1938858\ttotal: 3.82s\tremaining: 9.97s\n",
      "277:\tlearn: 0.1936696\ttotal: 3.83s\tremaining: 9.96s\n",
      "278:\tlearn: 0.1935418\ttotal: 3.85s\tremaining: 9.95s\n",
      "279:\tlearn: 0.1933215\ttotal: 3.86s\tremaining: 9.93s\n",
      "280:\tlearn: 0.1932404\ttotal: 3.88s\tremaining: 9.92s\n",
      "281:\tlearn: 0.1931530\ttotal: 3.89s\tremaining: 9.9s\n",
      "282:\tlearn: 0.1930423\ttotal: 3.9s\tremaining: 9.89s\n",
      "283:\tlearn: 0.1929559\ttotal: 3.92s\tremaining: 9.87s\n",
      "284:\tlearn: 0.1927867\ttotal: 3.93s\tremaining: 9.85s\n",
      "285:\tlearn: 0.1926686\ttotal: 3.94s\tremaining: 9.84s\n",
      "286:\tlearn: 0.1926557\ttotal: 3.95s\tremaining: 9.82s\n",
      "287:\tlearn: 0.1925713\ttotal: 3.96s\tremaining: 9.8s\n",
      "288:\tlearn: 0.1924851\ttotal: 3.98s\tremaining: 9.79s\n",
      "289:\tlearn: 0.1924149\ttotal: 4s\tremaining: 9.78s\n",
      "290:\tlearn: 0.1922474\ttotal: 4.01s\tremaining: 9.77s\n",
      "291:\tlearn: 0.1920723\ttotal: 4.02s\tremaining: 9.76s\n",
      "292:\tlearn: 0.1919994\ttotal: 4.04s\tremaining: 9.74s\n",
      "293:\tlearn: 0.1919313\ttotal: 4.05s\tremaining: 9.73s\n",
      "294:\tlearn: 0.1917880\ttotal: 4.06s\tremaining: 9.71s\n",
      "295:\tlearn: 0.1916192\ttotal: 4.08s\tremaining: 9.69s\n",
      "296:\tlearn: 0.1915499\ttotal: 4.09s\tremaining: 9.68s\n",
      "297:\tlearn: 0.1914165\ttotal: 4.1s\tremaining: 9.66s\n",
      "298:\tlearn: 0.1913070\ttotal: 4.12s\tremaining: 9.65s\n",
      "299:\tlearn: 0.1911674\ttotal: 4.13s\tremaining: 9.63s\n",
      "300:\tlearn: 0.1910676\ttotal: 4.14s\tremaining: 9.61s\n",
      "301:\tlearn: 0.1910591\ttotal: 4.15s\tremaining: 9.6s\n",
      "302:\tlearn: 0.1910583\ttotal: 4.16s\tremaining: 9.57s\n",
      "303:\tlearn: 0.1909763\ttotal: 4.17s\tremaining: 9.56s\n",
      "304:\tlearn: 0.1908564\ttotal: 4.19s\tremaining: 9.54s\n",
      "305:\tlearn: 0.1907115\ttotal: 4.2s\tremaining: 9.53s\n",
      "306:\tlearn: 0.1903145\ttotal: 4.22s\tremaining: 9.52s\n",
      "307:\tlearn: 0.1901345\ttotal: 4.23s\tremaining: 9.51s\n",
      "308:\tlearn: 0.1900621\ttotal: 4.25s\tremaining: 9.5s\n",
      "309:\tlearn: 0.1899636\ttotal: 4.26s\tremaining: 9.48s\n",
      "310:\tlearn: 0.1898851\ttotal: 4.27s\tremaining: 9.47s\n",
      "311:\tlearn: 0.1897634\ttotal: 4.29s\tremaining: 9.45s\n",
      "312:\tlearn: 0.1897627\ttotal: 4.3s\tremaining: 9.43s\n",
      "313:\tlearn: 0.1894200\ttotal: 4.31s\tremaining: 9.42s\n",
      "314:\tlearn: 0.1890820\ttotal: 4.32s\tremaining: 9.4s\n",
      "315:\tlearn: 0.1890098\ttotal: 4.34s\tremaining: 9.39s\n",
      "316:\tlearn: 0.1888952\ttotal: 4.35s\tremaining: 9.38s\n",
      "317:\tlearn: 0.1888291\ttotal: 4.36s\tremaining: 9.36s\n",
      "318:\tlearn: 0.1887815\ttotal: 4.38s\tremaining: 9.34s\n",
      "319:\tlearn: 0.1885205\ttotal: 4.39s\tremaining: 9.33s\n",
      "320:\tlearn: 0.1885205\ttotal: 4.4s\tremaining: 9.31s\n",
      "321:\tlearn: 0.1883512\ttotal: 4.42s\tremaining: 9.3s\n",
      "322:\tlearn: 0.1883512\ttotal: 4.42s\tremaining: 9.28s\n",
      "323:\tlearn: 0.1883434\ttotal: 4.44s\tremaining: 9.26s\n",
      "324:\tlearn: 0.1882632\ttotal: 4.45s\tremaining: 9.24s\n",
      "325:\tlearn: 0.1882127\ttotal: 4.47s\tremaining: 9.24s\n",
      "326:\tlearn: 0.1881262\ttotal: 4.48s\tremaining: 9.22s\n",
      "327:\tlearn: 0.1879611\ttotal: 4.5s\tremaining: 9.21s\n",
      "328:\tlearn: 0.1879182\ttotal: 4.51s\tremaining: 9.19s\n",
      "329:\tlearn: 0.1878130\ttotal: 4.52s\tremaining: 9.18s\n",
      "330:\tlearn: 0.1877297\ttotal: 4.54s\tremaining: 9.17s\n",
      "331:\tlearn: 0.1876096\ttotal: 4.55s\tremaining: 9.15s\n",
      "332:\tlearn: 0.1875175\ttotal: 4.56s\tremaining: 9.14s\n",
      "333:\tlearn: 0.1874672\ttotal: 4.58s\tremaining: 9.13s\n",
      "334:\tlearn: 0.1872690\ttotal: 4.59s\tremaining: 9.11s\n",
      "335:\tlearn: 0.1871674\ttotal: 4.6s\tremaining: 9.1s\n",
      "336:\tlearn: 0.1869105\ttotal: 4.62s\tremaining: 9.09s\n",
      "337:\tlearn: 0.1868447\ttotal: 4.63s\tremaining: 9.07s\n",
      "338:\tlearn: 0.1865744\ttotal: 4.65s\tremaining: 9.06s\n",
      "339:\tlearn: 0.1864778\ttotal: 4.66s\tremaining: 9.05s\n",
      "340:\tlearn: 0.1863415\ttotal: 4.67s\tremaining: 9.04s\n",
      "341:\tlearn: 0.1862317\ttotal: 4.69s\tremaining: 9.02s\n",
      "342:\tlearn: 0.1862315\ttotal: 4.7s\tremaining: 9s\n",
      "343:\tlearn: 0.1861571\ttotal: 4.71s\tremaining: 8.99s\n",
      "344:\tlearn: 0.1860848\ttotal: 4.73s\tremaining: 8.97s\n",
      "345:\tlearn: 0.1860171\ttotal: 4.74s\tremaining: 8.96s\n",
      "346:\tlearn: 0.1860165\ttotal: 4.75s\tremaining: 8.94s\n",
      "347:\tlearn: 0.1859303\ttotal: 4.76s\tremaining: 8.93s\n",
      "348:\tlearn: 0.1858894\ttotal: 4.78s\tremaining: 8.91s\n",
      "349:\tlearn: 0.1857829\ttotal: 4.79s\tremaining: 8.9s\n",
      "350:\tlearn: 0.1856856\ttotal: 4.8s\tremaining: 8.88s\n",
      "351:\tlearn: 0.1856047\ttotal: 4.82s\tremaining: 8.87s\n",
      "352:\tlearn: 0.1855158\ttotal: 4.83s\tremaining: 8.86s\n",
      "353:\tlearn: 0.1854222\ttotal: 4.85s\tremaining: 8.85s\n",
      "354:\tlearn: 0.1852927\ttotal: 4.86s\tremaining: 8.83s\n",
      "355:\tlearn: 0.1850590\ttotal: 4.87s\tremaining: 8.82s\n",
      "356:\tlearn: 0.1849857\ttotal: 4.89s\tremaining: 8.8s\n",
      "357:\tlearn: 0.1848334\ttotal: 4.9s\tremaining: 8.79s\n",
      "358:\tlearn: 0.1847215\ttotal: 4.91s\tremaining: 8.78s\n",
      "359:\tlearn: 0.1846440\ttotal: 4.93s\tremaining: 8.76s\n",
      "360:\tlearn: 0.1844941\ttotal: 4.94s\tremaining: 8.75s\n",
      "361:\tlearn: 0.1844327\ttotal: 4.96s\tremaining: 8.74s\n",
      "362:\tlearn: 0.1843693\ttotal: 4.97s\tremaining: 8.72s\n",
      "363:\tlearn: 0.1842152\ttotal: 4.98s\tremaining: 8.71s\n",
      "364:\tlearn: 0.1841090\ttotal: 5s\tremaining: 8.69s\n",
      "365:\tlearn: 0.1840139\ttotal: 5.01s\tremaining: 8.68s\n",
      "366:\tlearn: 0.1839595\ttotal: 5.02s\tremaining: 8.66s\n",
      "367:\tlearn: 0.1837970\ttotal: 5.04s\tremaining: 8.65s\n",
      "368:\tlearn: 0.1835778\ttotal: 5.05s\tremaining: 8.64s\n",
      "369:\tlearn: 0.1835125\ttotal: 5.07s\tremaining: 8.63s\n",
      "370:\tlearn: 0.1834197\ttotal: 5.08s\tremaining: 8.61s\n",
      "371:\tlearn: 0.1833812\ttotal: 5.09s\tremaining: 8.6s\n",
      "372:\tlearn: 0.1833100\ttotal: 5.1s\tremaining: 8.58s\n",
      "373:\tlearn: 0.1832251\ttotal: 5.12s\tremaining: 8.56s\n",
      "374:\tlearn: 0.1831498\ttotal: 5.13s\tremaining: 8.55s\n",
      "375:\tlearn: 0.1830873\ttotal: 5.14s\tremaining: 8.53s\n",
      "376:\tlearn: 0.1829294\ttotal: 5.16s\tremaining: 8.52s\n",
      "377:\tlearn: 0.1828581\ttotal: 5.17s\tremaining: 8.5s\n",
      "378:\tlearn: 0.1828288\ttotal: 5.18s\tremaining: 8.49s\n",
      "379:\tlearn: 0.1827637\ttotal: 5.19s\tremaining: 8.47s\n",
      "380:\tlearn: 0.1826814\ttotal: 5.21s\tremaining: 8.46s\n",
      "381:\tlearn: 0.1826131\ttotal: 5.22s\tremaining: 8.44s\n",
      "382:\tlearn: 0.1825337\ttotal: 5.23s\tremaining: 8.43s\n",
      "383:\tlearn: 0.1824595\ttotal: 5.25s\tremaining: 8.42s\n",
      "384:\tlearn: 0.1824589\ttotal: 5.26s\tremaining: 8.4s\n",
      "385:\tlearn: 0.1823852\ttotal: 5.27s\tremaining: 8.39s\n",
      "386:\tlearn: 0.1823204\ttotal: 5.28s\tremaining: 8.37s\n",
      "387:\tlearn: 0.1821234\ttotal: 5.3s\tremaining: 8.36s\n",
      "388:\tlearn: 0.1819978\ttotal: 5.31s\tremaining: 8.34s\n",
      "389:\tlearn: 0.1819482\ttotal: 5.32s\tremaining: 8.33s\n",
      "390:\tlearn: 0.1818566\ttotal: 5.34s\tremaining: 8.31s\n",
      "391:\tlearn: 0.1817959\ttotal: 5.35s\tremaining: 8.3s\n",
      "392:\tlearn: 0.1817512\ttotal: 5.36s\tremaining: 8.28s\n",
      "393:\tlearn: 0.1816663\ttotal: 5.38s\tremaining: 8.27s\n",
      "394:\tlearn: 0.1815249\ttotal: 5.39s\tremaining: 8.26s\n",
      "395:\tlearn: 0.1814610\ttotal: 5.41s\tremaining: 8.25s\n",
      "396:\tlearn: 0.1813976\ttotal: 5.42s\tremaining: 8.23s\n",
      "397:\tlearn: 0.1812990\ttotal: 5.44s\tremaining: 8.22s\n",
      "398:\tlearn: 0.1812523\ttotal: 5.45s\tremaining: 8.21s\n",
      "399:\tlearn: 0.1812200\ttotal: 5.46s\tremaining: 8.2s\n",
      "400:\tlearn: 0.1811284\ttotal: 5.48s\tremaining: 8.19s\n",
      "401:\tlearn: 0.1810424\ttotal: 5.49s\tremaining: 8.17s\n",
      "402:\tlearn: 0.1809981\ttotal: 5.51s\tremaining: 8.16s\n",
      "403:\tlearn: 0.1809355\ttotal: 5.52s\tremaining: 8.14s\n",
      "404:\tlearn: 0.1808299\ttotal: 5.53s\tremaining: 8.13s\n",
      "405:\tlearn: 0.1807722\ttotal: 5.55s\tremaining: 8.12s\n",
      "406:\tlearn: 0.1806514\ttotal: 5.56s\tremaining: 8.11s\n",
      "407:\tlearn: 0.1806376\ttotal: 5.57s\tremaining: 8.09s\n",
      "408:\tlearn: 0.1805698\ttotal: 5.59s\tremaining: 8.08s\n",
      "409:\tlearn: 0.1805111\ttotal: 5.6s\tremaining: 8.06s\n",
      "410:\tlearn: 0.1804356\ttotal: 5.62s\tremaining: 8.05s\n",
      "411:\tlearn: 0.1803781\ttotal: 5.63s\tremaining: 8.03s\n",
      "412:\tlearn: 0.1803776\ttotal: 5.64s\tremaining: 8.02s\n",
      "413:\tlearn: 0.1803167\ttotal: 5.65s\tremaining: 8s\n",
      "414:\tlearn: 0.1802242\ttotal: 5.67s\tremaining: 7.99s\n",
      "415:\tlearn: 0.1801364\ttotal: 5.68s\tremaining: 7.97s\n",
      "416:\tlearn: 0.1799962\ttotal: 5.69s\tremaining: 7.96s\n",
      "417:\tlearn: 0.1799437\ttotal: 5.71s\tremaining: 7.94s\n",
      "418:\tlearn: 0.1798626\ttotal: 5.72s\tremaining: 7.93s\n",
      "419:\tlearn: 0.1797577\ttotal: 5.73s\tremaining: 7.91s\n",
      "420:\tlearn: 0.1797067\ttotal: 5.74s\tremaining: 7.9s\n",
      "421:\tlearn: 0.1796517\ttotal: 5.76s\tremaining: 7.88s\n",
      "422:\tlearn: 0.1795949\ttotal: 5.77s\tremaining: 7.87s\n",
      "423:\tlearn: 0.1794674\ttotal: 5.78s\tremaining: 7.86s\n",
      "424:\tlearn: 0.1793673\ttotal: 5.79s\tremaining: 7.84s\n",
      "425:\tlearn: 0.1793122\ttotal: 5.81s\tremaining: 7.83s\n",
      "426:\tlearn: 0.1791953\ttotal: 5.82s\tremaining: 7.81s\n",
      "427:\tlearn: 0.1791514\ttotal: 5.83s\tremaining: 7.8s\n",
      "428:\tlearn: 0.1791146\ttotal: 5.85s\tremaining: 7.78s\n",
      "429:\tlearn: 0.1789868\ttotal: 5.86s\tremaining: 7.77s\n",
      "430:\tlearn: 0.1788299\ttotal: 5.88s\tremaining: 7.76s\n",
      "431:\tlearn: 0.1787543\ttotal: 5.89s\tremaining: 7.74s\n",
      "432:\tlearn: 0.1786693\ttotal: 5.9s\tremaining: 7.73s\n",
      "433:\tlearn: 0.1786327\ttotal: 5.92s\tremaining: 7.71s\n",
      "434:\tlearn: 0.1785427\ttotal: 5.93s\tremaining: 7.7s\n",
      "435:\tlearn: 0.1784855\ttotal: 5.94s\tremaining: 7.69s\n",
      "436:\tlearn: 0.1783892\ttotal: 5.96s\tremaining: 7.67s\n",
      "437:\tlearn: 0.1783214\ttotal: 5.97s\tremaining: 7.67s\n",
      "438:\tlearn: 0.1782358\ttotal: 5.99s\tremaining: 7.65s\n",
      "439:\tlearn: 0.1781910\ttotal: 6s\tremaining: 7.64s\n",
      "440:\tlearn: 0.1781176\ttotal: 6.01s\tremaining: 7.62s\n",
      "441:\tlearn: 0.1780164\ttotal: 6.03s\tremaining: 7.61s\n",
      "442:\tlearn: 0.1779829\ttotal: 6.04s\tremaining: 7.6s\n",
      "443:\tlearn: 0.1779209\ttotal: 6.06s\tremaining: 7.58s\n",
      "444:\tlearn: 0.1778574\ttotal: 6.07s\tremaining: 7.57s\n",
      "445:\tlearn: 0.1777086\ttotal: 6.08s\tremaining: 7.56s\n",
      "446:\tlearn: 0.1776575\ttotal: 6.1s\tremaining: 7.54s\n",
      "447:\tlearn: 0.1776079\ttotal: 6.11s\tremaining: 7.53s\n",
      "448:\tlearn: 0.1776079\ttotal: 6.12s\tremaining: 7.51s\n",
      "449:\tlearn: 0.1775437\ttotal: 6.13s\tremaining: 7.5s\n",
      "450:\tlearn: 0.1773719\ttotal: 6.15s\tremaining: 7.48s\n",
      "451:\tlearn: 0.1772953\ttotal: 6.16s\tremaining: 7.47s\n",
      "452:\tlearn: 0.1772403\ttotal: 6.18s\tremaining: 7.46s\n",
      "453:\tlearn: 0.1771839\ttotal: 6.19s\tremaining: 7.44s\n",
      "454:\tlearn: 0.1770869\ttotal: 6.21s\tremaining: 7.43s\n",
      "455:\tlearn: 0.1770281\ttotal: 6.22s\tremaining: 7.42s\n",
      "456:\tlearn: 0.1769539\ttotal: 6.24s\tremaining: 7.41s\n",
      "457:\tlearn: 0.1768443\ttotal: 6.25s\tremaining: 7.39s\n",
      "458:\tlearn: 0.1768379\ttotal: 6.26s\tremaining: 7.38s\n",
      "459:\tlearn: 0.1767808\ttotal: 6.27s\tremaining: 7.36s\n",
      "460:\tlearn: 0.1767303\ttotal: 6.29s\tremaining: 7.35s\n",
      "461:\tlearn: 0.1766393\ttotal: 6.3s\tremaining: 7.33s\n",
      "462:\tlearn: 0.1765345\ttotal: 6.31s\tremaining: 7.32s\n",
      "463:\tlearn: 0.1764642\ttotal: 6.33s\tremaining: 7.31s\n",
      "464:\tlearn: 0.1764286\ttotal: 6.34s\tremaining: 7.29s\n",
      "465:\tlearn: 0.1763668\ttotal: 6.35s\tremaining: 7.28s\n",
      "466:\tlearn: 0.1763017\ttotal: 6.36s\tremaining: 7.26s\n",
      "467:\tlearn: 0.1762499\ttotal: 6.38s\tremaining: 7.25s\n",
      "468:\tlearn: 0.1761472\ttotal: 6.39s\tremaining: 7.23s\n",
      "469:\tlearn: 0.1761023\ttotal: 6.4s\tremaining: 7.22s\n",
      "470:\tlearn: 0.1760428\ttotal: 6.42s\tremaining: 7.21s\n",
      "471:\tlearn: 0.1760428\ttotal: 6.43s\tremaining: 7.19s\n",
      "472:\tlearn: 0.1759714\ttotal: 6.44s\tremaining: 7.18s\n",
      "473:\tlearn: 0.1758808\ttotal: 6.45s\tremaining: 7.16s\n",
      "474:\tlearn: 0.1758770\ttotal: 6.46s\tremaining: 7.15s\n",
      "475:\tlearn: 0.1758299\ttotal: 6.48s\tremaining: 7.13s\n",
      "476:\tlearn: 0.1758215\ttotal: 6.49s\tremaining: 7.12s\n",
      "477:\tlearn: 0.1757577\ttotal: 6.5s\tremaining: 7.1s\n",
      "478:\tlearn: 0.1757076\ttotal: 6.52s\tremaining: 7.09s\n",
      "479:\tlearn: 0.1756384\ttotal: 6.53s\tremaining: 7.08s\n",
      "480:\tlearn: 0.1755493\ttotal: 6.55s\tremaining: 7.06s\n",
      "481:\tlearn: 0.1754600\ttotal: 6.56s\tremaining: 7.05s\n",
      "482:\tlearn: 0.1754146\ttotal: 6.58s\tremaining: 7.04s\n",
      "483:\tlearn: 0.1753326\ttotal: 6.59s\tremaining: 7.02s\n",
      "484:\tlearn: 0.1752825\ttotal: 6.6s\tremaining: 7.01s\n",
      "485:\tlearn: 0.1752104\ttotal: 6.61s\tremaining: 7s\n",
      "486:\tlearn: 0.1751378\ttotal: 6.63s\tremaining: 6.98s\n",
      "487:\tlearn: 0.1750722\ttotal: 6.64s\tremaining: 6.97s\n",
      "488:\tlearn: 0.1749949\ttotal: 6.66s\tremaining: 6.96s\n",
      "489:\tlearn: 0.1749635\ttotal: 6.67s\tremaining: 6.94s\n",
      "490:\tlearn: 0.1748948\ttotal: 6.68s\tremaining: 6.93s\n",
      "491:\tlearn: 0.1748481\ttotal: 6.7s\tremaining: 6.91s\n",
      "492:\tlearn: 0.1747909\ttotal: 6.71s\tremaining: 6.9s\n",
      "493:\tlearn: 0.1747342\ttotal: 6.72s\tremaining: 6.88s\n",
      "494:\tlearn: 0.1746776\ttotal: 6.73s\tremaining: 6.87s\n",
      "495:\tlearn: 0.1746216\ttotal: 6.75s\tremaining: 6.86s\n",
      "496:\tlearn: 0.1745597\ttotal: 6.76s\tremaining: 6.84s\n",
      "497:\tlearn: 0.1745088\ttotal: 6.77s\tremaining: 6.83s\n",
      "498:\tlearn: 0.1744703\ttotal: 6.79s\tremaining: 6.82s\n",
      "499:\tlearn: 0.1743824\ttotal: 6.8s\tremaining: 6.8s\n",
      "500:\tlearn: 0.1743057\ttotal: 6.82s\tremaining: 6.79s\n",
      "501:\tlearn: 0.1742361\ttotal: 6.83s\tremaining: 6.77s\n",
      "502:\tlearn: 0.1741639\ttotal: 6.84s\tremaining: 6.76s\n",
      "503:\tlearn: 0.1740477\ttotal: 6.86s\tremaining: 6.75s\n",
      "504:\tlearn: 0.1739916\ttotal: 6.87s\tremaining: 6.73s\n",
      "505:\tlearn: 0.1739236\ttotal: 6.88s\tremaining: 6.72s\n",
      "506:\tlearn: 0.1737666\ttotal: 6.9s\tremaining: 6.71s\n",
      "507:\tlearn: 0.1736501\ttotal: 6.91s\tremaining: 6.69s\n",
      "508:\tlearn: 0.1735404\ttotal: 6.92s\tremaining: 6.68s\n",
      "509:\tlearn: 0.1734782\ttotal: 6.94s\tremaining: 6.67s\n",
      "510:\tlearn: 0.1733994\ttotal: 6.95s\tremaining: 6.66s\n",
      "511:\tlearn: 0.1733528\ttotal: 6.97s\tremaining: 6.64s\n",
      "512:\tlearn: 0.1732893\ttotal: 6.98s\tremaining: 6.63s\n",
      "513:\tlearn: 0.1732421\ttotal: 7s\tremaining: 6.61s\n",
      "514:\tlearn: 0.1731734\ttotal: 7.01s\tremaining: 6.6s\n",
      "515:\tlearn: 0.1731211\ttotal: 7.02s\tremaining: 6.59s\n",
      "516:\tlearn: 0.1730778\ttotal: 7.04s\tremaining: 6.57s\n",
      "517:\tlearn: 0.1730526\ttotal: 7.05s\tremaining: 6.56s\n",
      "518:\tlearn: 0.1729999\ttotal: 7.06s\tremaining: 6.54s\n",
      "519:\tlearn: 0.1729571\ttotal: 7.07s\tremaining: 6.53s\n",
      "520:\tlearn: 0.1729472\ttotal: 7.08s\tremaining: 6.51s\n",
      "521:\tlearn: 0.1728902\ttotal: 7.1s\tremaining: 6.5s\n",
      "522:\tlearn: 0.1727906\ttotal: 7.11s\tremaining: 6.49s\n",
      "523:\tlearn: 0.1727288\ttotal: 7.12s\tremaining: 6.47s\n",
      "524:\tlearn: 0.1726452\ttotal: 7.14s\tremaining: 6.46s\n",
      "525:\tlearn: 0.1725903\ttotal: 7.15s\tremaining: 6.44s\n",
      "526:\tlearn: 0.1725408\ttotal: 7.16s\tremaining: 6.43s\n",
      "527:\tlearn: 0.1724994\ttotal: 7.18s\tremaining: 6.42s\n",
      "528:\tlearn: 0.1724488\ttotal: 7.19s\tremaining: 6.4s\n",
      "529:\tlearn: 0.1723818\ttotal: 7.2s\tremaining: 6.39s\n",
      "530:\tlearn: 0.1723129\ttotal: 7.22s\tremaining: 6.38s\n",
      "531:\tlearn: 0.1722713\ttotal: 7.23s\tremaining: 6.36s\n",
      "532:\tlearn: 0.1722210\ttotal: 7.25s\tremaining: 6.35s\n",
      "533:\tlearn: 0.1721804\ttotal: 7.26s\tremaining: 6.34s\n",
      "534:\tlearn: 0.1721318\ttotal: 7.27s\tremaining: 6.32s\n",
      "535:\tlearn: 0.1720519\ttotal: 7.29s\tremaining: 6.31s\n",
      "536:\tlearn: 0.1720223\ttotal: 7.3s\tremaining: 6.29s\n",
      "537:\tlearn: 0.1719566\ttotal: 7.31s\tremaining: 6.28s\n",
      "538:\tlearn: 0.1718952\ttotal: 7.32s\tremaining: 6.26s\n",
      "539:\tlearn: 0.1718949\ttotal: 7.33s\tremaining: 6.25s\n",
      "540:\tlearn: 0.1718648\ttotal: 7.35s\tremaining: 6.23s\n",
      "541:\tlearn: 0.1717656\ttotal: 7.36s\tremaining: 6.22s\n",
      "542:\tlearn: 0.1716870\ttotal: 7.38s\tremaining: 6.21s\n",
      "543:\tlearn: 0.1716569\ttotal: 7.39s\tremaining: 6.19s\n",
      "544:\tlearn: 0.1714868\ttotal: 7.4s\tremaining: 6.18s\n",
      "545:\tlearn: 0.1714140\ttotal: 7.42s\tremaining: 6.17s\n",
      "546:\tlearn: 0.1713553\ttotal: 7.43s\tremaining: 6.16s\n",
      "547:\tlearn: 0.1713091\ttotal: 7.45s\tremaining: 6.14s\n",
      "548:\tlearn: 0.1712447\ttotal: 7.46s\tremaining: 6.13s\n",
      "549:\tlearn: 0.1711676\ttotal: 7.47s\tremaining: 6.11s\n",
      "550:\tlearn: 0.1711015\ttotal: 7.48s\tremaining: 6.1s\n",
      "551:\tlearn: 0.1710631\ttotal: 7.5s\tremaining: 6.08s\n",
      "552:\tlearn: 0.1710010\ttotal: 7.51s\tremaining: 6.07s\n",
      "553:\tlearn: 0.1707837\ttotal: 7.53s\tremaining: 6.06s\n",
      "554:\tlearn: 0.1707162\ttotal: 7.54s\tremaining: 6.05s\n",
      "555:\tlearn: 0.1706843\ttotal: 7.56s\tremaining: 6.03s\n",
      "556:\tlearn: 0.1706304\ttotal: 7.57s\tremaining: 6.02s\n",
      "557:\tlearn: 0.1705869\ttotal: 7.58s\tremaining: 6.01s\n",
      "558:\tlearn: 0.1705082\ttotal: 7.6s\tremaining: 5.99s\n",
      "559:\tlearn: 0.1704652\ttotal: 7.61s\tremaining: 5.98s\n",
      "560:\tlearn: 0.1703938\ttotal: 7.63s\tremaining: 5.97s\n",
      "561:\tlearn: 0.1703635\ttotal: 7.64s\tremaining: 5.95s\n",
      "562:\tlearn: 0.1703149\ttotal: 7.65s\tremaining: 5.94s\n",
      "563:\tlearn: 0.1702635\ttotal: 7.67s\tremaining: 5.93s\n",
      "564:\tlearn: 0.1702182\ttotal: 7.68s\tremaining: 5.91s\n",
      "565:\tlearn: 0.1701585\ttotal: 7.69s\tremaining: 5.9s\n",
      "566:\tlearn: 0.1701177\ttotal: 7.7s\tremaining: 5.88s\n",
      "567:\tlearn: 0.1700262\ttotal: 7.72s\tremaining: 5.87s\n",
      "568:\tlearn: 0.1699453\ttotal: 7.73s\tremaining: 5.86s\n",
      "569:\tlearn: 0.1698904\ttotal: 7.74s\tremaining: 5.84s\n",
      "570:\tlearn: 0.1698412\ttotal: 7.76s\tremaining: 5.83s\n",
      "571:\tlearn: 0.1697919\ttotal: 7.77s\tremaining: 5.81s\n",
      "572:\tlearn: 0.1696633\ttotal: 7.78s\tremaining: 5.8s\n",
      "573:\tlearn: 0.1696060\ttotal: 7.79s\tremaining: 5.79s\n",
      "574:\tlearn: 0.1695537\ttotal: 7.81s\tremaining: 5.77s\n",
      "575:\tlearn: 0.1695255\ttotal: 7.82s\tremaining: 5.76s\n",
      "576:\tlearn: 0.1694986\ttotal: 7.84s\tremaining: 5.75s\n",
      "577:\tlearn: 0.1694431\ttotal: 7.85s\tremaining: 5.73s\n",
      "578:\tlearn: 0.1694159\ttotal: 7.86s\tremaining: 5.72s\n",
      "579:\tlearn: 0.1693601\ttotal: 7.88s\tremaining: 5.7s\n",
      "580:\tlearn: 0.1693133\ttotal: 7.89s\tremaining: 5.69s\n",
      "581:\tlearn: 0.1692639\ttotal: 7.9s\tremaining: 5.67s\n",
      "582:\tlearn: 0.1691670\ttotal: 7.92s\tremaining: 5.66s\n",
      "583:\tlearn: 0.1691114\ttotal: 7.93s\tremaining: 5.65s\n",
      "584:\tlearn: 0.1690891\ttotal: 7.95s\tremaining: 5.64s\n",
      "585:\tlearn: 0.1690204\ttotal: 7.96s\tremaining: 5.62s\n",
      "586:\tlearn: 0.1689803\ttotal: 7.97s\tremaining: 5.61s\n",
      "587:\tlearn: 0.1689232\ttotal: 7.98s\tremaining: 5.59s\n",
      "588:\tlearn: 0.1688429\ttotal: 8s\tremaining: 5.58s\n",
      "589:\tlearn: 0.1687944\ttotal: 8.01s\tremaining: 5.57s\n",
      "590:\tlearn: 0.1687562\ttotal: 8.03s\tremaining: 5.55s\n",
      "591:\tlearn: 0.1687001\ttotal: 8.04s\tremaining: 5.54s\n",
      "592:\tlearn: 0.1686222\ttotal: 8.05s\tremaining: 5.53s\n",
      "593:\tlearn: 0.1685195\ttotal: 8.07s\tremaining: 5.51s\n",
      "594:\tlearn: 0.1684875\ttotal: 8.08s\tremaining: 5.5s\n",
      "595:\tlearn: 0.1684034\ttotal: 8.09s\tremaining: 5.48s\n",
      "596:\tlearn: 0.1683152\ttotal: 8.11s\tremaining: 5.47s\n",
      "597:\tlearn: 0.1682867\ttotal: 8.12s\tremaining: 5.46s\n",
      "598:\tlearn: 0.1682493\ttotal: 8.13s\tremaining: 5.44s\n",
      "599:\tlearn: 0.1682075\ttotal: 8.14s\tremaining: 5.43s\n",
      "600:\tlearn: 0.1681643\ttotal: 8.15s\tremaining: 5.41s\n",
      "601:\tlearn: 0.1680892\ttotal: 8.17s\tremaining: 5.4s\n",
      "602:\tlearn: 0.1680177\ttotal: 8.18s\tremaining: 5.39s\n",
      "603:\tlearn: 0.1679343\ttotal: 8.2s\tremaining: 5.37s\n",
      "604:\tlearn: 0.1678751\ttotal: 8.21s\tremaining: 5.36s\n",
      "605:\tlearn: 0.1678319\ttotal: 8.22s\tremaining: 5.35s\n",
      "606:\tlearn: 0.1677582\ttotal: 8.24s\tremaining: 5.33s\n",
      "607:\tlearn: 0.1676848\ttotal: 8.25s\tremaining: 5.32s\n",
      "608:\tlearn: 0.1676478\ttotal: 8.26s\tremaining: 5.3s\n",
      "609:\tlearn: 0.1675837\ttotal: 8.28s\tremaining: 5.29s\n",
      "610:\tlearn: 0.1673377\ttotal: 8.29s\tremaining: 5.28s\n",
      "611:\tlearn: 0.1672828\ttotal: 8.3s\tremaining: 5.26s\n",
      "612:\tlearn: 0.1672395\ttotal: 8.31s\tremaining: 5.25s\n",
      "613:\tlearn: 0.1671707\ttotal: 8.33s\tremaining: 5.23s\n",
      "614:\tlearn: 0.1669664\ttotal: 8.34s\tremaining: 5.22s\n",
      "615:\tlearn: 0.1668891\ttotal: 8.35s\tremaining: 5.21s\n",
      "616:\tlearn: 0.1668275\ttotal: 8.37s\tremaining: 5.19s\n",
      "617:\tlearn: 0.1667805\ttotal: 8.38s\tremaining: 5.18s\n",
      "618:\tlearn: 0.1666731\ttotal: 8.39s\tremaining: 5.17s\n",
      "619:\tlearn: 0.1666268\ttotal: 8.41s\tremaining: 5.15s\n",
      "620:\tlearn: 0.1664783\ttotal: 8.42s\tremaining: 5.14s\n",
      "621:\tlearn: 0.1663933\ttotal: 8.44s\tremaining: 5.13s\n",
      "622:\tlearn: 0.1663588\ttotal: 8.45s\tremaining: 5.11s\n",
      "623:\tlearn: 0.1662947\ttotal: 8.46s\tremaining: 5.1s\n",
      "624:\tlearn: 0.1662567\ttotal: 8.47s\tremaining: 5.08s\n",
      "625:\tlearn: 0.1662294\ttotal: 8.48s\tremaining: 5.07s\n",
      "626:\tlearn: 0.1661896\ttotal: 8.5s\tremaining: 5.06s\n",
      "627:\tlearn: 0.1661094\ttotal: 8.52s\tremaining: 5.04s\n",
      "628:\tlearn: 0.1660509\ttotal: 8.53s\tremaining: 5.03s\n",
      "629:\tlearn: 0.1659930\ttotal: 8.54s\tremaining: 5.02s\n",
      "630:\tlearn: 0.1659501\ttotal: 8.56s\tremaining: 5s\n",
      "631:\tlearn: 0.1658961\ttotal: 8.57s\tremaining: 4.99s\n",
      "632:\tlearn: 0.1658384\ttotal: 8.58s\tremaining: 4.98s\n",
      "633:\tlearn: 0.1657817\ttotal: 8.6s\tremaining: 4.96s\n",
      "634:\tlearn: 0.1656675\ttotal: 8.61s\tremaining: 4.95s\n",
      "635:\tlearn: 0.1656207\ttotal: 8.63s\tremaining: 4.94s\n",
      "636:\tlearn: 0.1655483\ttotal: 8.64s\tremaining: 4.92s\n",
      "637:\tlearn: 0.1654765\ttotal: 8.65s\tremaining: 4.91s\n",
      "638:\tlearn: 0.1654370\ttotal: 8.67s\tremaining: 4.9s\n",
      "639:\tlearn: 0.1654002\ttotal: 8.68s\tremaining: 4.88s\n",
      "640:\tlearn: 0.1653591\ttotal: 8.69s\tremaining: 4.87s\n",
      "641:\tlearn: 0.1652838\ttotal: 8.71s\tremaining: 4.85s\n",
      "642:\tlearn: 0.1652336\ttotal: 8.72s\tremaining: 4.84s\n",
      "643:\tlearn: 0.1652030\ttotal: 8.73s\tremaining: 4.83s\n",
      "644:\tlearn: 0.1651520\ttotal: 8.74s\tremaining: 4.81s\n",
      "645:\tlearn: 0.1650908\ttotal: 8.76s\tremaining: 4.8s\n",
      "646:\tlearn: 0.1650026\ttotal: 8.77s\tremaining: 4.79s\n",
      "647:\tlearn: 0.1648628\ttotal: 8.79s\tremaining: 4.77s\n",
      "648:\tlearn: 0.1647948\ttotal: 8.8s\tremaining: 4.76s\n",
      "649:\tlearn: 0.1647547\ttotal: 8.81s\tremaining: 4.75s\n",
      "650:\tlearn: 0.1646939\ttotal: 8.83s\tremaining: 4.73s\n",
      "651:\tlearn: 0.1646392\ttotal: 8.84s\tremaining: 4.72s\n",
      "652:\tlearn: 0.1645898\ttotal: 8.85s\tremaining: 4.7s\n",
      "653:\tlearn: 0.1645334\ttotal: 8.87s\tremaining: 4.69s\n",
      "654:\tlearn: 0.1644644\ttotal: 8.88s\tremaining: 4.67s\n",
      "655:\tlearn: 0.1644422\ttotal: 8.89s\tremaining: 4.66s\n",
      "656:\tlearn: 0.1643910\ttotal: 8.9s\tremaining: 4.65s\n",
      "657:\tlearn: 0.1643115\ttotal: 8.92s\tremaining: 4.63s\n",
      "658:\tlearn: 0.1642486\ttotal: 8.93s\tremaining: 4.62s\n",
      "659:\tlearn: 0.1641697\ttotal: 8.95s\tremaining: 4.61s\n",
      "660:\tlearn: 0.1641312\ttotal: 8.96s\tremaining: 4.59s\n",
      "661:\tlearn: 0.1640680\ttotal: 8.97s\tremaining: 4.58s\n",
      "662:\tlearn: 0.1640255\ttotal: 8.98s\tremaining: 4.57s\n",
      "663:\tlearn: 0.1639628\ttotal: 9s\tremaining: 4.55s\n",
      "664:\tlearn: 0.1638377\ttotal: 9.01s\tremaining: 4.54s\n",
      "665:\tlearn: 0.1637997\ttotal: 9.03s\tremaining: 4.53s\n",
      "666:\tlearn: 0.1637349\ttotal: 9.04s\tremaining: 4.51s\n",
      "667:\tlearn: 0.1636804\ttotal: 9.05s\tremaining: 4.5s\n",
      "668:\tlearn: 0.1636192\ttotal: 9.07s\tremaining: 4.49s\n",
      "669:\tlearn: 0.1635887\ttotal: 9.08s\tremaining: 4.47s\n",
      "670:\tlearn: 0.1635572\ttotal: 9.09s\tremaining: 4.46s\n",
      "671:\tlearn: 0.1634879\ttotal: 9.11s\tremaining: 4.44s\n",
      "672:\tlearn: 0.1634124\ttotal: 9.12s\tremaining: 4.43s\n",
      "673:\tlearn: 0.1633701\ttotal: 9.13s\tremaining: 4.42s\n",
      "674:\tlearn: 0.1633204\ttotal: 9.15s\tremaining: 4.4s\n",
      "675:\tlearn: 0.1632912\ttotal: 9.16s\tremaining: 4.39s\n",
      "676:\tlearn: 0.1632228\ttotal: 9.17s\tremaining: 4.38s\n",
      "677:\tlearn: 0.1631716\ttotal: 9.19s\tremaining: 4.36s\n",
      "678:\tlearn: 0.1631276\ttotal: 9.2s\tremaining: 4.35s\n",
      "679:\tlearn: 0.1629581\ttotal: 9.21s\tremaining: 4.34s\n",
      "680:\tlearn: 0.1629062\ttotal: 9.23s\tremaining: 4.32s\n",
      "681:\tlearn: 0.1628932\ttotal: 9.24s\tremaining: 4.31s\n",
      "682:\tlearn: 0.1628333\ttotal: 9.26s\tremaining: 4.3s\n",
      "683:\tlearn: 0.1627622\ttotal: 9.27s\tremaining: 4.28s\n",
      "684:\tlearn: 0.1627199\ttotal: 9.28s\tremaining: 4.27s\n",
      "685:\tlearn: 0.1626436\ttotal: 9.3s\tremaining: 4.25s\n",
      "686:\tlearn: 0.1625567\ttotal: 9.31s\tremaining: 4.24s\n",
      "687:\tlearn: 0.1624998\ttotal: 9.32s\tremaining: 4.23s\n",
      "688:\tlearn: 0.1624411\ttotal: 9.34s\tremaining: 4.21s\n",
      "689:\tlearn: 0.1623905\ttotal: 9.35s\tremaining: 4.2s\n",
      "690:\tlearn: 0.1623628\ttotal: 9.36s\tremaining: 4.19s\n",
      "691:\tlearn: 0.1623065\ttotal: 9.38s\tremaining: 4.17s\n",
      "692:\tlearn: 0.1622720\ttotal: 9.39s\tremaining: 4.16s\n",
      "693:\tlearn: 0.1622231\ttotal: 9.4s\tremaining: 4.15s\n",
      "694:\tlearn: 0.1621952\ttotal: 9.42s\tremaining: 4.13s\n",
      "695:\tlearn: 0.1621422\ttotal: 9.43s\tremaining: 4.12s\n",
      "696:\tlearn: 0.1620816\ttotal: 9.44s\tremaining: 4.11s\n",
      "697:\tlearn: 0.1620442\ttotal: 9.46s\tremaining: 4.09s\n",
      "698:\tlearn: 0.1619727\ttotal: 9.47s\tremaining: 4.08s\n",
      "699:\tlearn: 0.1619320\ttotal: 9.49s\tremaining: 4.07s\n",
      "700:\tlearn: 0.1618859\ttotal: 9.5s\tremaining: 4.05s\n",
      "701:\tlearn: 0.1617438\ttotal: 9.52s\tremaining: 4.04s\n",
      "702:\tlearn: 0.1616973\ttotal: 9.53s\tremaining: 4.03s\n",
      "703:\tlearn: 0.1616625\ttotal: 9.55s\tremaining: 4.01s\n",
      "704:\tlearn: 0.1616175\ttotal: 9.56s\tremaining: 4s\n",
      "705:\tlearn: 0.1615775\ttotal: 9.57s\tremaining: 3.99s\n",
      "706:\tlearn: 0.1615289\ttotal: 9.59s\tremaining: 3.97s\n",
      "707:\tlearn: 0.1614531\ttotal: 9.6s\tremaining: 3.96s\n",
      "708:\tlearn: 0.1614274\ttotal: 9.61s\tremaining: 3.95s\n",
      "709:\tlearn: 0.1613471\ttotal: 9.63s\tremaining: 3.93s\n",
      "710:\tlearn: 0.1613084\ttotal: 9.64s\tremaining: 3.92s\n",
      "711:\tlearn: 0.1612668\ttotal: 9.65s\tremaining: 3.9s\n",
      "712:\tlearn: 0.1612195\ttotal: 9.67s\tremaining: 3.89s\n",
      "713:\tlearn: 0.1611974\ttotal: 9.68s\tremaining: 3.88s\n",
      "714:\tlearn: 0.1611627\ttotal: 9.7s\tremaining: 3.86s\n",
      "715:\tlearn: 0.1611130\ttotal: 9.71s\tremaining: 3.85s\n",
      "716:\tlearn: 0.1610486\ttotal: 9.72s\tremaining: 3.84s\n",
      "717:\tlearn: 0.1609914\ttotal: 9.73s\tremaining: 3.82s\n",
      "718:\tlearn: 0.1609527\ttotal: 9.75s\tremaining: 3.81s\n",
      "719:\tlearn: 0.1609157\ttotal: 9.76s\tremaining: 3.79s\n",
      "720:\tlearn: 0.1608214\ttotal: 9.77s\tremaining: 3.78s\n",
      "721:\tlearn: 0.1607734\ttotal: 9.79s\tremaining: 3.77s\n",
      "722:\tlearn: 0.1607151\ttotal: 9.8s\tremaining: 3.75s\n",
      "723:\tlearn: 0.1606571\ttotal: 9.81s\tremaining: 3.74s\n",
      "724:\tlearn: 0.1606283\ttotal: 9.83s\tremaining: 3.73s\n",
      "725:\tlearn: 0.1605956\ttotal: 9.84s\tremaining: 3.71s\n",
      "726:\tlearn: 0.1605537\ttotal: 9.85s\tremaining: 3.7s\n",
      "727:\tlearn: 0.1605370\ttotal: 9.87s\tremaining: 3.69s\n",
      "728:\tlearn: 0.1604675\ttotal: 9.88s\tremaining: 3.67s\n",
      "729:\tlearn: 0.1604214\ttotal: 9.89s\tremaining: 3.66s\n",
      "730:\tlearn: 0.1603466\ttotal: 9.91s\tremaining: 3.65s\n",
      "731:\tlearn: 0.1603042\ttotal: 9.92s\tremaining: 3.63s\n",
      "732:\tlearn: 0.1602431\ttotal: 9.94s\tremaining: 3.62s\n",
      "733:\tlearn: 0.1602082\ttotal: 9.95s\tremaining: 3.6s\n",
      "734:\tlearn: 0.1601629\ttotal: 9.96s\tremaining: 3.59s\n",
      "735:\tlearn: 0.1601174\ttotal: 9.97s\tremaining: 3.58s\n",
      "736:\tlearn: 0.1600736\ttotal: 9.99s\tremaining: 3.56s\n",
      "737:\tlearn: 0.1600275\ttotal: 10s\tremaining: 3.55s\n",
      "738:\tlearn: 0.1599743\ttotal: 10s\tremaining: 3.54s\n",
      "739:\tlearn: 0.1598886\ttotal: 10s\tremaining: 3.52s\n",
      "740:\tlearn: 0.1598349\ttotal: 10s\tremaining: 3.51s\n",
      "741:\tlearn: 0.1598129\ttotal: 10.1s\tremaining: 3.5s\n",
      "742:\tlearn: 0.1596753\ttotal: 10.1s\tremaining: 3.48s\n",
      "743:\tlearn: 0.1596427\ttotal: 10.1s\tremaining: 3.47s\n",
      "744:\tlearn: 0.1595888\ttotal: 10.1s\tremaining: 3.45s\n",
      "745:\tlearn: 0.1595606\ttotal: 10.1s\tremaining: 3.44s\n",
      "746:\tlearn: 0.1595212\ttotal: 10.1s\tremaining: 3.43s\n",
      "747:\tlearn: 0.1594630\ttotal: 10.1s\tremaining: 3.41s\n",
      "748:\tlearn: 0.1594255\ttotal: 10.1s\tremaining: 3.4s\n",
      "749:\tlearn: 0.1593689\ttotal: 10.2s\tremaining: 3.39s\n",
      "750:\tlearn: 0.1593402\ttotal: 10.2s\tremaining: 3.37s\n",
      "751:\tlearn: 0.1592687\ttotal: 10.2s\tremaining: 3.36s\n",
      "752:\tlearn: 0.1592489\ttotal: 10.2s\tremaining: 3.35s\n",
      "753:\tlearn: 0.1592267\ttotal: 10.2s\tremaining: 3.33s\n",
      "754:\tlearn: 0.1591805\ttotal: 10.2s\tremaining: 3.32s\n",
      "755:\tlearn: 0.1591384\ttotal: 10.2s\tremaining: 3.3s\n",
      "756:\tlearn: 0.1590828\ttotal: 10.3s\tremaining: 3.29s\n",
      "757:\tlearn: 0.1590267\ttotal: 10.3s\tremaining: 3.28s\n",
      "758:\tlearn: 0.1589675\ttotal: 10.3s\tremaining: 3.27s\n",
      "759:\tlearn: 0.1589191\ttotal: 10.3s\tremaining: 3.26s\n",
      "760:\tlearn: 0.1588922\ttotal: 10.3s\tremaining: 3.25s\n",
      "761:\tlearn: 0.1588265\ttotal: 10.4s\tremaining: 3.23s\n",
      "762:\tlearn: 0.1587813\ttotal: 10.4s\tremaining: 3.22s\n",
      "763:\tlearn: 0.1587531\ttotal: 10.4s\tremaining: 3.21s\n",
      "764:\tlearn: 0.1587530\ttotal: 10.4s\tremaining: 3.19s\n",
      "765:\tlearn: 0.1587186\ttotal: 10.4s\tremaining: 3.18s\n",
      "766:\tlearn: 0.1586788\ttotal: 10.4s\tremaining: 3.17s\n",
      "767:\tlearn: 0.1586226\ttotal: 10.4s\tremaining: 3.15s\n",
      "768:\tlearn: 0.1586076\ttotal: 10.4s\tremaining: 3.14s\n",
      "769:\tlearn: 0.1585899\ttotal: 10.5s\tremaining: 3.12s\n",
      "770:\tlearn: 0.1585526\ttotal: 10.5s\tremaining: 3.11s\n",
      "771:\tlearn: 0.1584939\ttotal: 10.5s\tremaining: 3.1s\n",
      "772:\tlearn: 0.1584393\ttotal: 10.5s\tremaining: 3.08s\n",
      "773:\tlearn: 0.1583794\ttotal: 10.5s\tremaining: 3.07s\n",
      "774:\tlearn: 0.1583302\ttotal: 10.5s\tremaining: 3.06s\n",
      "775:\tlearn: 0.1582646\ttotal: 10.5s\tremaining: 3.04s\n",
      "776:\tlearn: 0.1582220\ttotal: 10.6s\tremaining: 3.03s\n",
      "777:\tlearn: 0.1581975\ttotal: 10.6s\tremaining: 3.02s\n",
      "778:\tlearn: 0.1581536\ttotal: 10.6s\tremaining: 3s\n",
      "779:\tlearn: 0.1581055\ttotal: 10.6s\tremaining: 2.99s\n",
      "780:\tlearn: 0.1580360\ttotal: 10.6s\tremaining: 2.98s\n",
      "781:\tlearn: 0.1579949\ttotal: 10.6s\tremaining: 2.96s\n",
      "782:\tlearn: 0.1579549\ttotal: 10.6s\tremaining: 2.95s\n",
      "783:\tlearn: 0.1579063\ttotal: 10.7s\tremaining: 2.94s\n",
      "784:\tlearn: 0.1578573\ttotal: 10.7s\tremaining: 2.92s\n",
      "785:\tlearn: 0.1577964\ttotal: 10.7s\tremaining: 2.91s\n",
      "786:\tlearn: 0.1577471\ttotal: 10.7s\tremaining: 2.89s\n",
      "787:\tlearn: 0.1576739\ttotal: 10.7s\tremaining: 2.88s\n",
      "788:\tlearn: 0.1576016\ttotal: 10.7s\tremaining: 2.87s\n",
      "789:\tlearn: 0.1574942\ttotal: 10.7s\tremaining: 2.85s\n",
      "790:\tlearn: 0.1574221\ttotal: 10.7s\tremaining: 2.84s\n",
      "791:\tlearn: 0.1573578\ttotal: 10.8s\tremaining: 2.83s\n",
      "792:\tlearn: 0.1572871\ttotal: 10.8s\tremaining: 2.81s\n",
      "793:\tlearn: 0.1572450\ttotal: 10.8s\tremaining: 2.8s\n",
      "794:\tlearn: 0.1572142\ttotal: 10.8s\tremaining: 2.79s\n",
      "795:\tlearn: 0.1571711\ttotal: 10.8s\tremaining: 2.77s\n",
      "796:\tlearn: 0.1571127\ttotal: 10.8s\tremaining: 2.76s\n",
      "797:\tlearn: 0.1570845\ttotal: 10.8s\tremaining: 2.75s\n",
      "798:\tlearn: 0.1570150\ttotal: 10.9s\tremaining: 2.73s\n",
      "799:\tlearn: 0.1569555\ttotal: 10.9s\tremaining: 2.72s\n",
      "800:\tlearn: 0.1569071\ttotal: 10.9s\tremaining: 2.7s\n",
      "801:\tlearn: 0.1568645\ttotal: 10.9s\tremaining: 2.69s\n",
      "802:\tlearn: 0.1568085\ttotal: 10.9s\tremaining: 2.68s\n",
      "803:\tlearn: 0.1567754\ttotal: 10.9s\tremaining: 2.66s\n",
      "804:\tlearn: 0.1567257\ttotal: 10.9s\tremaining: 2.65s\n",
      "805:\tlearn: 0.1566143\ttotal: 11s\tremaining: 2.64s\n",
      "806:\tlearn: 0.1565579\ttotal: 11s\tremaining: 2.62s\n",
      "807:\tlearn: 0.1564274\ttotal: 11s\tremaining: 2.61s\n",
      "808:\tlearn: 0.1563581\ttotal: 11s\tremaining: 2.6s\n",
      "809:\tlearn: 0.1562778\ttotal: 11s\tremaining: 2.58s\n",
      "810:\tlearn: 0.1562384\ttotal: 11s\tremaining: 2.57s\n",
      "811:\tlearn: 0.1561867\ttotal: 11s\tremaining: 2.56s\n",
      "812:\tlearn: 0.1560222\ttotal: 11.1s\tremaining: 2.54s\n",
      "813:\tlearn: 0.1559837\ttotal: 11.1s\tremaining: 2.53s\n",
      "814:\tlearn: 0.1559583\ttotal: 11.1s\tremaining: 2.52s\n",
      "815:\tlearn: 0.1559056\ttotal: 11.1s\tremaining: 2.5s\n",
      "816:\tlearn: 0.1558740\ttotal: 11.1s\tremaining: 2.49s\n",
      "817:\tlearn: 0.1558411\ttotal: 11.1s\tremaining: 2.47s\n",
      "818:\tlearn: 0.1558385\ttotal: 11.1s\tremaining: 2.46s\n",
      "819:\tlearn: 0.1558084\ttotal: 11.1s\tremaining: 2.45s\n",
      "820:\tlearn: 0.1557563\ttotal: 11.2s\tremaining: 2.43s\n",
      "821:\tlearn: 0.1557418\ttotal: 11.2s\tremaining: 2.42s\n",
      "822:\tlearn: 0.1557073\ttotal: 11.2s\tremaining: 2.4s\n",
      "823:\tlearn: 0.1556814\ttotal: 11.2s\tremaining: 2.39s\n",
      "824:\tlearn: 0.1556295\ttotal: 11.2s\tremaining: 2.38s\n",
      "825:\tlearn: 0.1555876\ttotal: 11.2s\tremaining: 2.36s\n",
      "826:\tlearn: 0.1555481\ttotal: 11.2s\tremaining: 2.35s\n",
      "827:\tlearn: 0.1554849\ttotal: 11.3s\tremaining: 2.34s\n",
      "828:\tlearn: 0.1554590\ttotal: 11.3s\tremaining: 2.32s\n",
      "829:\tlearn: 0.1554222\ttotal: 11.3s\tremaining: 2.31s\n",
      "830:\tlearn: 0.1553610\ttotal: 11.3s\tremaining: 2.3s\n",
      "831:\tlearn: 0.1553199\ttotal: 11.3s\tremaining: 2.28s\n",
      "832:\tlearn: 0.1552911\ttotal: 11.3s\tremaining: 2.27s\n",
      "833:\tlearn: 0.1552455\ttotal: 11.3s\tremaining: 2.25s\n",
      "834:\tlearn: 0.1552005\ttotal: 11.3s\tremaining: 2.24s\n",
      "835:\tlearn: 0.1551456\ttotal: 11.4s\tremaining: 2.23s\n",
      "836:\tlearn: 0.1551203\ttotal: 11.4s\tremaining: 2.21s\n",
      "837:\tlearn: 0.1550917\ttotal: 11.4s\tremaining: 2.2s\n",
      "838:\tlearn: 0.1550619\ttotal: 11.4s\tremaining: 2.19s\n",
      "839:\tlearn: 0.1550026\ttotal: 11.4s\tremaining: 2.17s\n",
      "840:\tlearn: 0.1549689\ttotal: 11.4s\tremaining: 2.16s\n",
      "841:\tlearn: 0.1549350\ttotal: 11.4s\tremaining: 2.15s\n",
      "842:\tlearn: 0.1548969\ttotal: 11.4s\tremaining: 2.13s\n",
      "843:\tlearn: 0.1548352\ttotal: 11.5s\tremaining: 2.12s\n",
      "844:\tlearn: 0.1547634\ttotal: 11.5s\tremaining: 2.1s\n",
      "845:\tlearn: 0.1547189\ttotal: 11.5s\tremaining: 2.09s\n",
      "846:\tlearn: 0.1546826\ttotal: 11.5s\tremaining: 2.08s\n",
      "847:\tlearn: 0.1546226\ttotal: 11.5s\tremaining: 2.06s\n",
      "848:\tlearn: 0.1545750\ttotal: 11.5s\tremaining: 2.05s\n",
      "849:\tlearn: 0.1545159\ttotal: 11.5s\tremaining: 2.04s\n",
      "850:\tlearn: 0.1544762\ttotal: 11.6s\tremaining: 2.02s\n",
      "851:\tlearn: 0.1544312\ttotal: 11.6s\tremaining: 2.01s\n",
      "852:\tlearn: 0.1542981\ttotal: 11.6s\tremaining: 2s\n",
      "853:\tlearn: 0.1542571\ttotal: 11.6s\tremaining: 1.98s\n",
      "854:\tlearn: 0.1542230\ttotal: 11.6s\tremaining: 1.97s\n",
      "855:\tlearn: 0.1541621\ttotal: 11.6s\tremaining: 1.96s\n",
      "856:\tlearn: 0.1541149\ttotal: 11.6s\tremaining: 1.94s\n",
      "857:\tlearn: 0.1540894\ttotal: 11.7s\tremaining: 1.93s\n",
      "858:\tlearn: 0.1540556\ttotal: 11.7s\tremaining: 1.92s\n",
      "859:\tlearn: 0.1539830\ttotal: 11.7s\tremaining: 1.9s\n",
      "860:\tlearn: 0.1539445\ttotal: 11.7s\tremaining: 1.89s\n",
      "861:\tlearn: 0.1538978\ttotal: 11.7s\tremaining: 1.88s\n",
      "862:\tlearn: 0.1538518\ttotal: 11.7s\tremaining: 1.86s\n",
      "863:\tlearn: 0.1538155\ttotal: 11.7s\tremaining: 1.85s\n",
      "864:\tlearn: 0.1537849\ttotal: 11.8s\tremaining: 1.83s\n",
      "865:\tlearn: 0.1537643\ttotal: 11.8s\tremaining: 1.82s\n",
      "866:\tlearn: 0.1537160\ttotal: 11.8s\tremaining: 1.81s\n",
      "867:\tlearn: 0.1536713\ttotal: 11.8s\tremaining: 1.79s\n",
      "868:\tlearn: 0.1536130\ttotal: 11.8s\tremaining: 1.78s\n",
      "869:\tlearn: 0.1535711\ttotal: 11.8s\tremaining: 1.77s\n",
      "870:\tlearn: 0.1535355\ttotal: 11.8s\tremaining: 1.75s\n",
      "871:\tlearn: 0.1535103\ttotal: 11.8s\tremaining: 1.74s\n",
      "872:\tlearn: 0.1534550\ttotal: 11.9s\tremaining: 1.73s\n",
      "873:\tlearn: 0.1534126\ttotal: 11.9s\tremaining: 1.71s\n",
      "874:\tlearn: 0.1533811\ttotal: 11.9s\tremaining: 1.7s\n",
      "875:\tlearn: 0.1533345\ttotal: 11.9s\tremaining: 1.69s\n",
      "876:\tlearn: 0.1533220\ttotal: 11.9s\tremaining: 1.67s\n",
      "877:\tlearn: 0.1532608\ttotal: 11.9s\tremaining: 1.66s\n",
      "878:\tlearn: 0.1532263\ttotal: 11.9s\tremaining: 1.64s\n",
      "879:\tlearn: 0.1531950\ttotal: 12s\tremaining: 1.63s\n",
      "880:\tlearn: 0.1531306\ttotal: 12s\tremaining: 1.62s\n",
      "881:\tlearn: 0.1530736\ttotal: 12s\tremaining: 1.6s\n",
      "882:\tlearn: 0.1530110\ttotal: 12s\tremaining: 1.59s\n",
      "883:\tlearn: 0.1529456\ttotal: 12s\tremaining: 1.58s\n",
      "884:\tlearn: 0.1529158\ttotal: 12s\tremaining: 1.56s\n",
      "885:\tlearn: 0.1528747\ttotal: 12s\tremaining: 1.55s\n",
      "886:\tlearn: 0.1528713\ttotal: 12.1s\tremaining: 1.53s\n",
      "887:\tlearn: 0.1528434\ttotal: 12.1s\tremaining: 1.52s\n",
      "888:\tlearn: 0.1527930\ttotal: 12.1s\tremaining: 1.51s\n",
      "889:\tlearn: 0.1527472\ttotal: 12.1s\tremaining: 1.49s\n",
      "890:\tlearn: 0.1526964\ttotal: 12.1s\tremaining: 1.48s\n",
      "891:\tlearn: 0.1526845\ttotal: 12.1s\tremaining: 1.47s\n",
      "892:\tlearn: 0.1526486\ttotal: 12.1s\tremaining: 1.45s\n",
      "893:\tlearn: 0.1526018\ttotal: 12.1s\tremaining: 1.44s\n",
      "894:\tlearn: 0.1525769\ttotal: 12.2s\tremaining: 1.43s\n",
      "895:\tlearn: 0.1525327\ttotal: 12.2s\tremaining: 1.41s\n",
      "896:\tlearn: 0.1524907\ttotal: 12.2s\tremaining: 1.4s\n",
      "897:\tlearn: 0.1524348\ttotal: 12.2s\tremaining: 1.39s\n",
      "898:\tlearn: 0.1524265\ttotal: 12.2s\tremaining: 1.37s\n",
      "899:\tlearn: 0.1523915\ttotal: 12.2s\tremaining: 1.36s\n",
      "900:\tlearn: 0.1523230\ttotal: 12.2s\tremaining: 1.34s\n",
      "901:\tlearn: 0.1522985\ttotal: 12.3s\tremaining: 1.33s\n",
      "902:\tlearn: 0.1522361\ttotal: 12.3s\tremaining: 1.32s\n",
      "903:\tlearn: 0.1521983\ttotal: 12.3s\tremaining: 1.3s\n",
      "904:\tlearn: 0.1521424\ttotal: 12.3s\tremaining: 1.29s\n",
      "905:\tlearn: 0.1520767\ttotal: 12.3s\tremaining: 1.28s\n",
      "906:\tlearn: 0.1520291\ttotal: 12.3s\tremaining: 1.26s\n",
      "907:\tlearn: 0.1519948\ttotal: 12.3s\tremaining: 1.25s\n",
      "908:\tlearn: 0.1519560\ttotal: 12.3s\tremaining: 1.24s\n",
      "909:\tlearn: 0.1519196\ttotal: 12.4s\tremaining: 1.22s\n",
      "910:\tlearn: 0.1518965\ttotal: 12.4s\tremaining: 1.21s\n",
      "911:\tlearn: 0.1518305\ttotal: 12.4s\tremaining: 1.2s\n",
      "912:\tlearn: 0.1517585\ttotal: 12.4s\tremaining: 1.18s\n",
      "913:\tlearn: 0.1517031\ttotal: 12.4s\tremaining: 1.17s\n",
      "914:\tlearn: 0.1516156\ttotal: 12.4s\tremaining: 1.15s\n",
      "915:\tlearn: 0.1515543\ttotal: 12.4s\tremaining: 1.14s\n",
      "916:\tlearn: 0.1515061\ttotal: 12.5s\tremaining: 1.13s\n",
      "917:\tlearn: 0.1514600\ttotal: 12.5s\tremaining: 1.11s\n",
      "918:\tlearn: 0.1514378\ttotal: 12.5s\tremaining: 1.1s\n",
      "919:\tlearn: 0.1514115\ttotal: 12.5s\tremaining: 1.09s\n",
      "920:\tlearn: 0.1513934\ttotal: 12.5s\tremaining: 1.07s\n",
      "921:\tlearn: 0.1513643\ttotal: 12.5s\tremaining: 1.06s\n",
      "922:\tlearn: 0.1512858\ttotal: 12.5s\tremaining: 1.04s\n",
      "923:\tlearn: 0.1511968\ttotal: 12.5s\tremaining: 1.03s\n",
      "924:\tlearn: 0.1511427\ttotal: 12.6s\tremaining: 1.02s\n",
      "925:\tlearn: 0.1510839\ttotal: 12.6s\tremaining: 1s\n",
      "926:\tlearn: 0.1510537\ttotal: 12.6s\tremaining: 992ms\n",
      "927:\tlearn: 0.1510002\ttotal: 12.6s\tremaining: 978ms\n",
      "928:\tlearn: 0.1509862\ttotal: 12.6s\tremaining: 964ms\n",
      "929:\tlearn: 0.1509657\ttotal: 12.6s\tremaining: 951ms\n",
      "930:\tlearn: 0.1509220\ttotal: 12.6s\tremaining: 937ms\n",
      "931:\tlearn: 0.1508367\ttotal: 12.7s\tremaining: 924ms\n",
      "932:\tlearn: 0.1508113\ttotal: 12.7s\tremaining: 910ms\n",
      "933:\tlearn: 0.1507751\ttotal: 12.7s\tremaining: 896ms\n",
      "934:\tlearn: 0.1507453\ttotal: 12.7s\tremaining: 883ms\n",
      "935:\tlearn: 0.1507215\ttotal: 12.7s\tremaining: 869ms\n",
      "936:\tlearn: 0.1506970\ttotal: 12.7s\tremaining: 856ms\n",
      "937:\tlearn: 0.1506640\ttotal: 12.7s\tremaining: 842ms\n",
      "938:\tlearn: 0.1505873\ttotal: 12.8s\tremaining: 829ms\n",
      "939:\tlearn: 0.1505395\ttotal: 12.8s\tremaining: 815ms\n",
      "940:\tlearn: 0.1504778\ttotal: 12.8s\tremaining: 801ms\n",
      "941:\tlearn: 0.1504292\ttotal: 12.8s\tremaining: 788ms\n",
      "942:\tlearn: 0.1503701\ttotal: 12.8s\tremaining: 774ms\n",
      "943:\tlearn: 0.1503422\ttotal: 12.8s\tremaining: 761ms\n",
      "944:\tlearn: 0.1503207\ttotal: 12.8s\tremaining: 747ms\n",
      "945:\tlearn: 0.1502847\ttotal: 12.8s\tremaining: 733ms\n",
      "946:\tlearn: 0.1502413\ttotal: 12.9s\tremaining: 720ms\n",
      "947:\tlearn: 0.1502012\ttotal: 12.9s\tremaining: 706ms\n",
      "948:\tlearn: 0.1501389\ttotal: 12.9s\tremaining: 693ms\n",
      "949:\tlearn: 0.1500866\ttotal: 12.9s\tremaining: 679ms\n",
      "950:\tlearn: 0.1500480\ttotal: 12.9s\tremaining: 666ms\n",
      "951:\tlearn: 0.1500198\ttotal: 12.9s\tremaining: 652ms\n",
      "952:\tlearn: 0.1499943\ttotal: 12.9s\tremaining: 638ms\n",
      "953:\tlearn: 0.1499638\ttotal: 13s\tremaining: 625ms\n",
      "954:\tlearn: 0.1499013\ttotal: 13s\tremaining: 611ms\n",
      "955:\tlearn: 0.1498860\ttotal: 13s\tremaining: 598ms\n",
      "956:\tlearn: 0.1498512\ttotal: 13s\tremaining: 584ms\n",
      "957:\tlearn: 0.1498130\ttotal: 13s\tremaining: 570ms\n",
      "958:\tlearn: 0.1497499\ttotal: 13s\tremaining: 557ms\n",
      "959:\tlearn: 0.1497095\ttotal: 13s\tremaining: 543ms\n",
      "960:\tlearn: 0.1496671\ttotal: 13.1s\tremaining: 530ms\n",
      "961:\tlearn: 0.1496352\ttotal: 13.1s\tremaining: 516ms\n",
      "962:\tlearn: 0.1495933\ttotal: 13.1s\tremaining: 502ms\n",
      "963:\tlearn: 0.1495369\ttotal: 13.1s\tremaining: 489ms\n",
      "964:\tlearn: 0.1494998\ttotal: 13.1s\tremaining: 475ms\n",
      "965:\tlearn: 0.1494419\ttotal: 13.1s\tremaining: 462ms\n",
      "966:\tlearn: 0.1494109\ttotal: 13.2s\tremaining: 449ms\n",
      "967:\tlearn: 0.1493902\ttotal: 13.2s\tremaining: 435ms\n",
      "968:\tlearn: 0.1493421\ttotal: 13.2s\tremaining: 422ms\n",
      "969:\tlearn: 0.1493043\ttotal: 13.2s\tremaining: 408ms\n",
      "970:\tlearn: 0.1492617\ttotal: 13.2s\tremaining: 395ms\n",
      "971:\tlearn: 0.1492234\ttotal: 13.2s\tremaining: 381ms\n",
      "972:\tlearn: 0.1491965\ttotal: 13.2s\tremaining: 368ms\n",
      "973:\tlearn: 0.1491547\ttotal: 13.3s\tremaining: 354ms\n",
      "974:\tlearn: 0.1491400\ttotal: 13.3s\tremaining: 340ms\n",
      "975:\tlearn: 0.1491084\ttotal: 13.3s\tremaining: 327ms\n",
      "976:\tlearn: 0.1490935\ttotal: 13.3s\tremaining: 313ms\n",
      "977:\tlearn: 0.1490576\ttotal: 13.3s\tremaining: 300ms\n",
      "978:\tlearn: 0.1490210\ttotal: 13.3s\tremaining: 286ms\n",
      "979:\tlearn: 0.1489686\ttotal: 13.3s\tremaining: 272ms\n",
      "980:\tlearn: 0.1489203\ttotal: 13.4s\tremaining: 259ms\n",
      "981:\tlearn: 0.1488671\ttotal: 13.4s\tremaining: 245ms\n",
      "982:\tlearn: 0.1487854\ttotal: 13.4s\tremaining: 232ms\n",
      "983:\tlearn: 0.1487472\ttotal: 13.4s\tremaining: 218ms\n",
      "984:\tlearn: 0.1486955\ttotal: 13.4s\tremaining: 204ms\n",
      "985:\tlearn: 0.1486689\ttotal: 13.4s\tremaining: 191ms\n",
      "986:\tlearn: 0.1486346\ttotal: 13.5s\tremaining: 177ms\n",
      "987:\tlearn: 0.1486120\ttotal: 13.5s\tremaining: 164ms\n",
      "988:\tlearn: 0.1485454\ttotal: 13.5s\tremaining: 150ms\n",
      "989:\tlearn: 0.1485158\ttotal: 13.5s\tremaining: 136ms\n",
      "990:\tlearn: 0.1484689\ttotal: 13.5s\tremaining: 123ms\n",
      "991:\tlearn: 0.1484279\ttotal: 13.5s\tremaining: 109ms\n",
      "992:\tlearn: 0.1483818\ttotal: 13.6s\tremaining: 95.6ms\n",
      "993:\tlearn: 0.1483407\ttotal: 13.6s\tremaining: 82ms\n",
      "994:\tlearn: 0.1482923\ttotal: 13.6s\tremaining: 68.3ms\n",
      "995:\tlearn: 0.1482794\ttotal: 13.6s\tremaining: 54.7ms\n",
      "996:\tlearn: 0.1482345\ttotal: 13.6s\tremaining: 41ms\n",
      "997:\tlearn: 0.1482098\ttotal: 13.6s\tremaining: 27.3ms\n",
      "998:\tlearn: 0.1481682\ttotal: 13.7s\tremaining: 13.7ms\n",
      "999:\tlearn: 0.1481330\ttotal: 13.7s\tremaining: 0us\n",
      "Learning rate set to 0.070522\n",
      "0:\tlearn: 0.6535625\ttotal: 39ms\tremaining: 39s\n",
      "1:\tlearn: 0.6325994\ttotal: 53.4ms\tremaining: 26.6s\n",
      "2:\tlearn: 0.6109513\ttotal: 68.2ms\tremaining: 22.7s\n",
      "3:\tlearn: 0.5635048\ttotal: 82.4ms\tremaining: 20.5s\n",
      "4:\tlearn: 0.5497613\ttotal: 96.1ms\tremaining: 19.1s\n",
      "5:\tlearn: 0.5342040\ttotal: 110ms\tremaining: 18.2s\n",
      "6:\tlearn: 0.5152179\ttotal: 124ms\tremaining: 17.6s\n",
      "7:\tlearn: 0.5038516\ttotal: 137ms\tremaining: 17s\n",
      "8:\tlearn: 0.4786038\ttotal: 152ms\tremaining: 16.8s\n",
      "9:\tlearn: 0.4720508\ttotal: 166ms\tremaining: 16.4s\n",
      "10:\tlearn: 0.4644568\ttotal: 178ms\tremaining: 16s\n",
      "11:\tlearn: 0.4577788\ttotal: 192ms\tremaining: 15.8s\n",
      "12:\tlearn: 0.4525149\ttotal: 205ms\tremaining: 15.6s\n",
      "13:\tlearn: 0.4478479\ttotal: 219ms\tremaining: 15.4s\n",
      "14:\tlearn: 0.4365348\ttotal: 234ms\tremaining: 15.4s\n",
      "15:\tlearn: 0.4317142\ttotal: 248ms\tremaining: 15.2s\n",
      "16:\tlearn: 0.4222909\ttotal: 263ms\tremaining: 15.2s\n",
      "17:\tlearn: 0.4198695\ttotal: 277ms\tremaining: 15.1s\n",
      "18:\tlearn: 0.4164852\ttotal: 291ms\tremaining: 15s\n",
      "19:\tlearn: 0.4124181\ttotal: 305ms\tremaining: 14.9s\n",
      "20:\tlearn: 0.4090802\ttotal: 319ms\tremaining: 14.9s\n",
      "21:\tlearn: 0.4052663\ttotal: 333ms\tremaining: 14.8s\n",
      "22:\tlearn: 0.4021287\ttotal: 347ms\tremaining: 14.8s\n",
      "23:\tlearn: 0.4004576\ttotal: 361ms\tremaining: 14.7s\n",
      "24:\tlearn: 0.3949709\ttotal: 374ms\tremaining: 14.6s\n",
      "25:\tlearn: 0.3891493\ttotal: 389ms\tremaining: 14.6s\n",
      "26:\tlearn: 0.3851584\ttotal: 402ms\tremaining: 14.5s\n",
      "27:\tlearn: 0.3766716\ttotal: 415ms\tremaining: 14.4s\n",
      "28:\tlearn: 0.3720436\ttotal: 428ms\tremaining: 14.3s\n",
      "29:\tlearn: 0.3681925\ttotal: 445ms\tremaining: 14.4s\n",
      "30:\tlearn: 0.3659005\ttotal: 458ms\tremaining: 14.3s\n",
      "31:\tlearn: 0.3616246\ttotal: 472ms\tremaining: 14.3s\n",
      "32:\tlearn: 0.3558884\ttotal: 486ms\tremaining: 14.2s\n",
      "33:\tlearn: 0.3516078\ttotal: 499ms\tremaining: 14.2s\n",
      "34:\tlearn: 0.3477065\ttotal: 511ms\tremaining: 14.1s\n",
      "35:\tlearn: 0.3459015\ttotal: 523ms\tremaining: 14s\n",
      "36:\tlearn: 0.3436352\ttotal: 537ms\tremaining: 14s\n",
      "37:\tlearn: 0.3425579\ttotal: 550ms\tremaining: 13.9s\n",
      "38:\tlearn: 0.3410332\ttotal: 563ms\tremaining: 13.9s\n",
      "39:\tlearn: 0.3371660\ttotal: 575ms\tremaining: 13.8s\n",
      "40:\tlearn: 0.3345922\ttotal: 588ms\tremaining: 13.8s\n",
      "41:\tlearn: 0.3326087\ttotal: 602ms\tremaining: 13.7s\n",
      "42:\tlearn: 0.3287133\ttotal: 615ms\tremaining: 13.7s\n",
      "43:\tlearn: 0.3234200\ttotal: 628ms\tremaining: 13.6s\n",
      "44:\tlearn: 0.3214666\ttotal: 643ms\tremaining: 13.6s\n",
      "45:\tlearn: 0.3201230\ttotal: 669ms\tremaining: 13.9s\n",
      "46:\tlearn: 0.3158056\ttotal: 703ms\tremaining: 14.2s\n",
      "47:\tlearn: 0.3132499\ttotal: 717ms\tremaining: 14.2s\n",
      "48:\tlearn: 0.3119160\ttotal: 737ms\tremaining: 14.3s\n",
      "49:\tlearn: 0.3097007\ttotal: 752ms\tremaining: 14.3s\n",
      "50:\tlearn: 0.3085342\ttotal: 766ms\tremaining: 14.2s\n",
      "51:\tlearn: 0.3047303\ttotal: 778ms\tremaining: 14.2s\n",
      "52:\tlearn: 0.3033036\ttotal: 792ms\tremaining: 14.1s\n",
      "53:\tlearn: 0.3001290\ttotal: 806ms\tremaining: 14.1s\n",
      "54:\tlearn: 0.2991161\ttotal: 819ms\tremaining: 14.1s\n",
      "55:\tlearn: 0.2974841\ttotal: 833ms\tremaining: 14s\n",
      "56:\tlearn: 0.2966124\ttotal: 847ms\tremaining: 14s\n",
      "57:\tlearn: 0.2952165\ttotal: 862ms\tremaining: 14s\n",
      "58:\tlearn: 0.2933524\ttotal: 876ms\tremaining: 14s\n",
      "59:\tlearn: 0.2928184\ttotal: 888ms\tremaining: 13.9s\n",
      "60:\tlearn: 0.2915074\ttotal: 901ms\tremaining: 13.9s\n",
      "61:\tlearn: 0.2894329\ttotal: 915ms\tremaining: 13.8s\n",
      "62:\tlearn: 0.2876365\ttotal: 927ms\tremaining: 13.8s\n",
      "63:\tlearn: 0.2869052\ttotal: 941ms\tremaining: 13.8s\n",
      "64:\tlearn: 0.2846327\ttotal: 954ms\tremaining: 13.7s\n",
      "65:\tlearn: 0.2833092\ttotal: 968ms\tremaining: 13.7s\n",
      "66:\tlearn: 0.2823951\ttotal: 980ms\tremaining: 13.7s\n",
      "67:\tlearn: 0.2810866\ttotal: 1.02s\tremaining: 14s\n",
      "68:\tlearn: 0.2802213\ttotal: 1.04s\tremaining: 14s\n",
      "69:\tlearn: 0.2786265\ttotal: 1.05s\tremaining: 14s\n",
      "70:\tlearn: 0.2776853\ttotal: 1.07s\tremaining: 13.9s\n",
      "71:\tlearn: 0.2744136\ttotal: 1.08s\tremaining: 13.9s\n",
      "72:\tlearn: 0.2739103\ttotal: 1.09s\tremaining: 13.9s\n",
      "73:\tlearn: 0.2718549\ttotal: 1.11s\tremaining: 13.9s\n",
      "74:\tlearn: 0.2705957\ttotal: 1.12s\tremaining: 13.8s\n",
      "75:\tlearn: 0.2697998\ttotal: 1.13s\tremaining: 13.8s\n",
      "76:\tlearn: 0.2667310\ttotal: 1.15s\tremaining: 13.7s\n",
      "77:\tlearn: 0.2653244\ttotal: 1.16s\tremaining: 13.7s\n",
      "78:\tlearn: 0.2649037\ttotal: 1.17s\tremaining: 13.7s\n",
      "79:\tlearn: 0.2645797\ttotal: 1.19s\tremaining: 13.6s\n",
      "80:\tlearn: 0.2638890\ttotal: 1.2s\tremaining: 13.6s\n",
      "81:\tlearn: 0.2616545\ttotal: 1.21s\tremaining: 13.6s\n",
      "82:\tlearn: 0.2613897\ttotal: 1.23s\tremaining: 13.5s\n",
      "83:\tlearn: 0.2603108\ttotal: 1.24s\tremaining: 13.5s\n",
      "84:\tlearn: 0.2597744\ttotal: 1.25s\tremaining: 13.5s\n",
      "85:\tlearn: 0.2595471\ttotal: 1.27s\tremaining: 13.5s\n",
      "86:\tlearn: 0.2587538\ttotal: 1.28s\tremaining: 13.5s\n",
      "87:\tlearn: 0.2574395\ttotal: 1.3s\tremaining: 13.5s\n",
      "88:\tlearn: 0.2559865\ttotal: 1.31s\tremaining: 13.4s\n",
      "89:\tlearn: 0.2545199\ttotal: 1.32s\tremaining: 13.4s\n",
      "90:\tlearn: 0.2530090\ttotal: 1.34s\tremaining: 13.4s\n",
      "91:\tlearn: 0.2521998\ttotal: 1.35s\tremaining: 13.3s\n",
      "92:\tlearn: 0.2518636\ttotal: 1.36s\tremaining: 13.3s\n",
      "93:\tlearn: 0.2513920\ttotal: 1.38s\tremaining: 13.3s\n",
      "94:\tlearn: 0.2502678\ttotal: 1.39s\tremaining: 13.2s\n",
      "95:\tlearn: 0.2498281\ttotal: 1.4s\tremaining: 13.2s\n",
      "96:\tlearn: 0.2492624\ttotal: 1.42s\tremaining: 13.2s\n",
      "97:\tlearn: 0.2477132\ttotal: 1.43s\tremaining: 13.2s\n",
      "98:\tlearn: 0.2474668\ttotal: 1.44s\tremaining: 13.1s\n",
      "99:\tlearn: 0.2468985\ttotal: 1.46s\tremaining: 13.1s\n",
      "100:\tlearn: 0.2463820\ttotal: 1.47s\tremaining: 13.1s\n",
      "101:\tlearn: 0.2452982\ttotal: 1.48s\tremaining: 13.1s\n",
      "102:\tlearn: 0.2450852\ttotal: 1.5s\tremaining: 13s\n",
      "103:\tlearn: 0.2449099\ttotal: 1.51s\tremaining: 13s\n",
      "104:\tlearn: 0.2445451\ttotal: 1.52s\tremaining: 13s\n",
      "105:\tlearn: 0.2440517\ttotal: 1.54s\tremaining: 13s\n",
      "106:\tlearn: 0.2435864\ttotal: 1.55s\tremaining: 12.9s\n",
      "107:\tlearn: 0.2429227\ttotal: 1.56s\tremaining: 12.9s\n",
      "108:\tlearn: 0.2426164\ttotal: 1.57s\tremaining: 12.9s\n",
      "109:\tlearn: 0.2424337\ttotal: 1.59s\tremaining: 12.8s\n",
      "110:\tlearn: 0.2422112\ttotal: 1.6s\tremaining: 12.8s\n",
      "111:\tlearn: 0.2417801\ttotal: 1.61s\tremaining: 12.8s\n",
      "112:\tlearn: 0.2409065\ttotal: 1.63s\tremaining: 12.8s\n",
      "113:\tlearn: 0.2407316\ttotal: 1.64s\tremaining: 12.7s\n",
      "114:\tlearn: 0.2404158\ttotal: 1.65s\tremaining: 12.7s\n",
      "115:\tlearn: 0.2399462\ttotal: 1.67s\tremaining: 12.7s\n",
      "116:\tlearn: 0.2398183\ttotal: 1.69s\tremaining: 12.7s\n",
      "117:\tlearn: 0.2393241\ttotal: 1.7s\tremaining: 12.7s\n",
      "118:\tlearn: 0.2385457\ttotal: 1.71s\tremaining: 12.7s\n",
      "119:\tlearn: 0.2378757\ttotal: 1.73s\tremaining: 12.7s\n",
      "120:\tlearn: 0.2375616\ttotal: 1.74s\tremaining: 12.6s\n",
      "121:\tlearn: 0.2372045\ttotal: 1.75s\tremaining: 12.6s\n",
      "122:\tlearn: 0.2363396\ttotal: 1.77s\tremaining: 12.6s\n",
      "123:\tlearn: 0.2361060\ttotal: 1.78s\tremaining: 12.6s\n",
      "124:\tlearn: 0.2349290\ttotal: 1.79s\tremaining: 12.6s\n",
      "125:\tlearn: 0.2345702\ttotal: 1.81s\tremaining: 12.5s\n",
      "126:\tlearn: 0.2343703\ttotal: 1.82s\tremaining: 12.5s\n",
      "127:\tlearn: 0.2339061\ttotal: 1.83s\tremaining: 12.5s\n",
      "128:\tlearn: 0.2337721\ttotal: 1.85s\tremaining: 12.5s\n",
      "129:\tlearn: 0.2334771\ttotal: 1.86s\tremaining: 12.5s\n",
      "130:\tlearn: 0.2333247\ttotal: 1.88s\tremaining: 12.5s\n",
      "131:\tlearn: 0.2332062\ttotal: 1.89s\tremaining: 12.4s\n",
      "132:\tlearn: 0.2327181\ttotal: 1.9s\tremaining: 12.4s\n",
      "133:\tlearn: 0.2324983\ttotal: 1.92s\tremaining: 12.4s\n",
      "134:\tlearn: 0.2318392\ttotal: 1.93s\tremaining: 12.4s\n",
      "135:\tlearn: 0.2314343\ttotal: 1.94s\tremaining: 12.3s\n",
      "136:\tlearn: 0.2310553\ttotal: 1.96s\tremaining: 12.3s\n",
      "137:\tlearn: 0.2305260\ttotal: 1.97s\tremaining: 12.3s\n",
      "138:\tlearn: 0.2300716\ttotal: 1.98s\tremaining: 12.3s\n",
      "139:\tlearn: 0.2297250\ttotal: 1.99s\tremaining: 12.3s\n",
      "140:\tlearn: 0.2296092\ttotal: 2.01s\tremaining: 12.2s\n",
      "141:\tlearn: 0.2289168\ttotal: 2.02s\tremaining: 12.2s\n",
      "142:\tlearn: 0.2287439\ttotal: 2.03s\tremaining: 12.2s\n",
      "143:\tlearn: 0.2284291\ttotal: 2.05s\tremaining: 12.2s\n",
      "144:\tlearn: 0.2283339\ttotal: 2.06s\tremaining: 12.2s\n",
      "145:\tlearn: 0.2276844\ttotal: 2.07s\tremaining: 12.1s\n",
      "146:\tlearn: 0.2273089\ttotal: 2.09s\tremaining: 12.1s\n",
      "147:\tlearn: 0.2269494\ttotal: 2.1s\tremaining: 12.1s\n",
      "148:\tlearn: 0.2268254\ttotal: 2.12s\tremaining: 12.1s\n",
      "149:\tlearn: 0.2265756\ttotal: 2.13s\tremaining: 12.1s\n",
      "150:\tlearn: 0.2263645\ttotal: 2.14s\tremaining: 12s\n",
      "151:\tlearn: 0.2260747\ttotal: 2.15s\tremaining: 12s\n",
      "152:\tlearn: 0.2258168\ttotal: 2.17s\tremaining: 12s\n",
      "153:\tlearn: 0.2256982\ttotal: 2.18s\tremaining: 12s\n",
      "154:\tlearn: 0.2252146\ttotal: 2.19s\tremaining: 12s\n",
      "155:\tlearn: 0.2248414\ttotal: 2.21s\tremaining: 11.9s\n",
      "156:\tlearn: 0.2246458\ttotal: 2.22s\tremaining: 11.9s\n",
      "157:\tlearn: 0.2245145\ttotal: 2.23s\tremaining: 11.9s\n",
      "158:\tlearn: 0.2243774\ttotal: 2.25s\tremaining: 11.9s\n",
      "159:\tlearn: 0.2242065\ttotal: 2.27s\tremaining: 11.9s\n",
      "160:\tlearn: 0.2239959\ttotal: 2.28s\tremaining: 11.9s\n",
      "161:\tlearn: 0.2235297\ttotal: 2.29s\tremaining: 11.9s\n",
      "162:\tlearn: 0.2231925\ttotal: 2.31s\tremaining: 11.9s\n",
      "163:\tlearn: 0.2227427\ttotal: 2.32s\tremaining: 11.8s\n",
      "164:\tlearn: 0.2226176\ttotal: 2.33s\tremaining: 11.8s\n",
      "165:\tlearn: 0.2223060\ttotal: 2.35s\tremaining: 11.8s\n",
      "166:\tlearn: 0.2221016\ttotal: 2.36s\tremaining: 11.8s\n",
      "167:\tlearn: 0.2216879\ttotal: 2.38s\tremaining: 11.8s\n",
      "168:\tlearn: 0.2210291\ttotal: 2.39s\tremaining: 11.8s\n",
      "169:\tlearn: 0.2205569\ttotal: 2.4s\tremaining: 11.7s\n",
      "170:\tlearn: 0.2202838\ttotal: 2.42s\tremaining: 11.7s\n",
      "171:\tlearn: 0.2196233\ttotal: 2.43s\tremaining: 11.7s\n",
      "172:\tlearn: 0.2194321\ttotal: 2.44s\tremaining: 11.7s\n",
      "173:\tlearn: 0.2186328\ttotal: 2.46s\tremaining: 11.7s\n",
      "174:\tlearn: 0.2184815\ttotal: 2.47s\tremaining: 11.6s\n",
      "175:\tlearn: 0.2176464\ttotal: 2.48s\tremaining: 11.6s\n",
      "176:\tlearn: 0.2172405\ttotal: 2.49s\tremaining: 11.6s\n",
      "177:\tlearn: 0.2167201\ttotal: 2.51s\tremaining: 11.6s\n",
      "178:\tlearn: 0.2162971\ttotal: 2.52s\tremaining: 11.6s\n",
      "179:\tlearn: 0.2160791\ttotal: 2.53s\tremaining: 11.5s\n",
      "180:\tlearn: 0.2159182\ttotal: 2.55s\tremaining: 11.5s\n",
      "181:\tlearn: 0.2157272\ttotal: 2.56s\tremaining: 11.5s\n",
      "182:\tlearn: 0.2147561\ttotal: 2.57s\tremaining: 11.5s\n",
      "183:\tlearn: 0.2143728\ttotal: 2.59s\tremaining: 11.5s\n",
      "184:\tlearn: 0.2139514\ttotal: 2.6s\tremaining: 11.5s\n",
      "185:\tlearn: 0.2135569\ttotal: 2.61s\tremaining: 11.4s\n",
      "186:\tlearn: 0.2133087\ttotal: 2.63s\tremaining: 11.4s\n",
      "187:\tlearn: 0.2127829\ttotal: 2.64s\tremaining: 11.4s\n",
      "188:\tlearn: 0.2124840\ttotal: 2.66s\tremaining: 11.4s\n",
      "189:\tlearn: 0.2123947\ttotal: 2.67s\tremaining: 11.4s\n",
      "190:\tlearn: 0.2122928\ttotal: 2.69s\tremaining: 11.4s\n",
      "191:\tlearn: 0.2119895\ttotal: 2.7s\tremaining: 11.4s\n",
      "192:\tlearn: 0.2116098\ttotal: 2.71s\tremaining: 11.3s\n",
      "193:\tlearn: 0.2115138\ttotal: 2.72s\tremaining: 11.3s\n",
      "194:\tlearn: 0.2111469\ttotal: 2.74s\tremaining: 11.3s\n",
      "195:\tlearn: 0.2107712\ttotal: 2.75s\tremaining: 11.3s\n",
      "196:\tlearn: 0.2105590\ttotal: 2.76s\tremaining: 11.3s\n",
      "197:\tlearn: 0.2100593\ttotal: 2.78s\tremaining: 11.3s\n",
      "198:\tlearn: 0.2098710\ttotal: 2.79s\tremaining: 11.2s\n",
      "199:\tlearn: 0.2095113\ttotal: 2.81s\tremaining: 11.2s\n",
      "200:\tlearn: 0.2093301\ttotal: 2.82s\tremaining: 11.2s\n",
      "201:\tlearn: 0.2091615\ttotal: 2.83s\tremaining: 11.2s\n",
      "202:\tlearn: 0.2090010\ttotal: 2.85s\tremaining: 11.2s\n",
      "203:\tlearn: 0.2085153\ttotal: 2.86s\tremaining: 11.2s\n",
      "204:\tlearn: 0.2084107\ttotal: 2.88s\tremaining: 11.2s\n",
      "205:\tlearn: 0.2080402\ttotal: 2.89s\tremaining: 11.1s\n",
      "206:\tlearn: 0.2078907\ttotal: 2.9s\tremaining: 11.1s\n",
      "207:\tlearn: 0.2076049\ttotal: 2.92s\tremaining: 11.1s\n",
      "208:\tlearn: 0.2074718\ttotal: 2.93s\tremaining: 11.1s\n",
      "209:\tlearn: 0.2072662\ttotal: 2.94s\tremaining: 11.1s\n",
      "210:\tlearn: 0.2069593\ttotal: 2.96s\tremaining: 11.1s\n",
      "211:\tlearn: 0.2068825\ttotal: 2.97s\tremaining: 11s\n",
      "212:\tlearn: 0.2066663\ttotal: 2.98s\tremaining: 11s\n",
      "213:\tlearn: 0.2065792\ttotal: 2.99s\tremaining: 11s\n",
      "214:\tlearn: 0.2065172\ttotal: 3.01s\tremaining: 11s\n",
      "215:\tlearn: 0.2062507\ttotal: 3.02s\tremaining: 11s\n",
      "216:\tlearn: 0.2059160\ttotal: 3.03s\tremaining: 10.9s\n",
      "217:\tlearn: 0.2056074\ttotal: 3.05s\tremaining: 10.9s\n",
      "218:\tlearn: 0.2055162\ttotal: 3.06s\tremaining: 10.9s\n",
      "219:\tlearn: 0.2052041\ttotal: 3.08s\tremaining: 10.9s\n",
      "220:\tlearn: 0.2049276\ttotal: 3.09s\tremaining: 10.9s\n",
      "221:\tlearn: 0.2047893\ttotal: 3.1s\tremaining: 10.9s\n",
      "222:\tlearn: 0.2046809\ttotal: 3.11s\tremaining: 10.9s\n",
      "223:\tlearn: 0.2040689\ttotal: 3.13s\tremaining: 10.8s\n",
      "224:\tlearn: 0.2039507\ttotal: 3.14s\tremaining: 10.8s\n",
      "225:\tlearn: 0.2037631\ttotal: 3.15s\tremaining: 10.8s\n",
      "226:\tlearn: 0.2035460\ttotal: 3.17s\tremaining: 10.8s\n",
      "227:\tlearn: 0.2029418\ttotal: 3.18s\tremaining: 10.8s\n",
      "228:\tlearn: 0.2028252\ttotal: 3.19s\tremaining: 10.7s\n",
      "229:\tlearn: 0.2026637\ttotal: 3.2s\tremaining: 10.7s\n",
      "230:\tlearn: 0.2024996\ttotal: 3.21s\tremaining: 10.7s\n",
      "231:\tlearn: 0.2021530\ttotal: 3.23s\tremaining: 10.7s\n",
      "232:\tlearn: 0.2018395\ttotal: 3.25s\tremaining: 10.7s\n",
      "233:\tlearn: 0.2017673\ttotal: 3.26s\tremaining: 10.7s\n",
      "234:\tlearn: 0.2015961\ttotal: 3.27s\tremaining: 10.7s\n",
      "235:\tlearn: 0.2014730\ttotal: 3.29s\tremaining: 10.6s\n",
      "236:\tlearn: 0.2012980\ttotal: 3.3s\tremaining: 10.6s\n",
      "237:\tlearn: 0.2011784\ttotal: 3.32s\tremaining: 10.6s\n",
      "238:\tlearn: 0.2010274\ttotal: 3.33s\tremaining: 10.6s\n",
      "239:\tlearn: 0.2009323\ttotal: 3.34s\tremaining: 10.6s\n",
      "240:\tlearn: 0.2007834\ttotal: 3.35s\tremaining: 10.6s\n",
      "241:\tlearn: 0.2006332\ttotal: 3.37s\tremaining: 10.6s\n",
      "242:\tlearn: 0.2003163\ttotal: 3.38s\tremaining: 10.5s\n",
      "243:\tlearn: 0.2000728\ttotal: 3.4s\tremaining: 10.5s\n",
      "244:\tlearn: 0.1996873\ttotal: 3.41s\tremaining: 10.5s\n",
      "245:\tlearn: 0.1994100\ttotal: 3.43s\tremaining: 10.5s\n",
      "246:\tlearn: 0.1993325\ttotal: 3.44s\tremaining: 10.5s\n",
      "247:\tlearn: 0.1991983\ttotal: 3.46s\tremaining: 10.5s\n",
      "248:\tlearn: 0.1990691\ttotal: 3.47s\tremaining: 10.5s\n",
      "249:\tlearn: 0.1989007\ttotal: 3.48s\tremaining: 10.4s\n",
      "250:\tlearn: 0.1987789\ttotal: 3.5s\tremaining: 10.4s\n",
      "251:\tlearn: 0.1985414\ttotal: 3.51s\tremaining: 10.4s\n",
      "252:\tlearn: 0.1984176\ttotal: 3.52s\tremaining: 10.4s\n",
      "253:\tlearn: 0.1982879\ttotal: 3.53s\tremaining: 10.4s\n",
      "254:\tlearn: 0.1980189\ttotal: 3.55s\tremaining: 10.4s\n",
      "255:\tlearn: 0.1979333\ttotal: 3.56s\tremaining: 10.3s\n",
      "256:\tlearn: 0.1976819\ttotal: 3.57s\tremaining: 10.3s\n",
      "257:\tlearn: 0.1974265\ttotal: 3.59s\tremaining: 10.3s\n",
      "258:\tlearn: 0.1973372\ttotal: 3.6s\tremaining: 10.3s\n",
      "259:\tlearn: 0.1971051\ttotal: 3.61s\tremaining: 10.3s\n",
      "260:\tlearn: 0.1969359\ttotal: 3.63s\tremaining: 10.3s\n",
      "261:\tlearn: 0.1967587\ttotal: 3.64s\tremaining: 10.3s\n",
      "262:\tlearn: 0.1966801\ttotal: 3.66s\tremaining: 10.2s\n",
      "263:\tlearn: 0.1965661\ttotal: 3.67s\tremaining: 10.2s\n",
      "264:\tlearn: 0.1962169\ttotal: 3.69s\tremaining: 10.2s\n",
      "265:\tlearn: 0.1961060\ttotal: 3.7s\tremaining: 10.2s\n",
      "266:\tlearn: 0.1960025\ttotal: 3.71s\tremaining: 10.2s\n",
      "267:\tlearn: 0.1958948\ttotal: 3.73s\tremaining: 10.2s\n",
      "268:\tlearn: 0.1957983\ttotal: 3.74s\tremaining: 10.2s\n",
      "269:\tlearn: 0.1957183\ttotal: 3.75s\tremaining: 10.1s\n",
      "270:\tlearn: 0.1955226\ttotal: 3.77s\tremaining: 10.1s\n",
      "271:\tlearn: 0.1954312\ttotal: 3.78s\tremaining: 10.1s\n",
      "272:\tlearn: 0.1953338\ttotal: 3.79s\tremaining: 10.1s\n",
      "273:\tlearn: 0.1952371\ttotal: 3.81s\tremaining: 10.1s\n",
      "274:\tlearn: 0.1951754\ttotal: 3.82s\tremaining: 10.1s\n",
      "275:\tlearn: 0.1950939\ttotal: 3.83s\tremaining: 10.1s\n",
      "276:\tlearn: 0.1950218\ttotal: 3.85s\tremaining: 10s\n",
      "277:\tlearn: 0.1948895\ttotal: 3.86s\tremaining: 10s\n",
      "278:\tlearn: 0.1948323\ttotal: 3.88s\tremaining: 10s\n",
      "279:\tlearn: 0.1947531\ttotal: 3.89s\tremaining: 10s\n",
      "280:\tlearn: 0.1946824\ttotal: 3.9s\tremaining: 9.99s\n",
      "281:\tlearn: 0.1945907\ttotal: 3.92s\tremaining: 9.98s\n",
      "282:\tlearn: 0.1943944\ttotal: 3.93s\tremaining: 9.96s\n",
      "283:\tlearn: 0.1943053\ttotal: 3.94s\tremaining: 9.95s\n",
      "284:\tlearn: 0.1942520\ttotal: 3.96s\tremaining: 9.93s\n",
      "285:\tlearn: 0.1941669\ttotal: 3.97s\tremaining: 9.91s\n",
      "286:\tlearn: 0.1939491\ttotal: 4.01s\tremaining: 9.96s\n",
      "287:\tlearn: 0.1935835\ttotal: 4.03s\tremaining: 9.96s\n",
      "288:\tlearn: 0.1934532\ttotal: 4.04s\tremaining: 9.94s\n",
      "289:\tlearn: 0.1933514\ttotal: 4.05s\tremaining: 9.93s\n",
      "290:\tlearn: 0.1932101\ttotal: 4.07s\tremaining: 9.91s\n",
      "291:\tlearn: 0.1930603\ttotal: 4.08s\tremaining: 9.9s\n",
      "292:\tlearn: 0.1929745\ttotal: 4.1s\tremaining: 9.89s\n",
      "293:\tlearn: 0.1927921\ttotal: 4.11s\tremaining: 9.87s\n",
      "294:\tlearn: 0.1926884\ttotal: 4.12s\tremaining: 9.85s\n",
      "295:\tlearn: 0.1924827\ttotal: 4.14s\tremaining: 9.84s\n",
      "296:\tlearn: 0.1924288\ttotal: 4.15s\tremaining: 9.82s\n",
      "297:\tlearn: 0.1923450\ttotal: 4.16s\tremaining: 9.8s\n",
      "298:\tlearn: 0.1922258\ttotal: 4.17s\tremaining: 9.79s\n",
      "299:\tlearn: 0.1921604\ttotal: 4.19s\tremaining: 9.77s\n",
      "300:\tlearn: 0.1920970\ttotal: 4.2s\tremaining: 9.76s\n",
      "301:\tlearn: 0.1917690\ttotal: 4.22s\tremaining: 9.75s\n",
      "302:\tlearn: 0.1917057\ttotal: 4.23s\tremaining: 9.74s\n",
      "303:\tlearn: 0.1916203\ttotal: 4.25s\tremaining: 9.73s\n",
      "304:\tlearn: 0.1915392\ttotal: 4.26s\tremaining: 9.71s\n",
      "305:\tlearn: 0.1913779\ttotal: 4.28s\tremaining: 9.7s\n",
      "306:\tlearn: 0.1910963\ttotal: 4.29s\tremaining: 9.69s\n",
      "307:\tlearn: 0.1910242\ttotal: 4.3s\tremaining: 9.67s\n",
      "308:\tlearn: 0.1909335\ttotal: 4.32s\tremaining: 9.66s\n",
      "309:\tlearn: 0.1908700\ttotal: 4.33s\tremaining: 9.64s\n",
      "310:\tlearn: 0.1908210\ttotal: 4.35s\tremaining: 9.63s\n",
      "311:\tlearn: 0.1906898\ttotal: 4.36s\tremaining: 9.61s\n",
      "312:\tlearn: 0.1906348\ttotal: 4.37s\tremaining: 9.6s\n",
      "313:\tlearn: 0.1905685\ttotal: 4.38s\tremaining: 9.58s\n",
      "314:\tlearn: 0.1904787\ttotal: 4.4s\tremaining: 9.56s\n",
      "315:\tlearn: 0.1903591\ttotal: 4.41s\tremaining: 9.55s\n",
      "316:\tlearn: 0.1902759\ttotal: 4.43s\tremaining: 9.54s\n",
      "317:\tlearn: 0.1901770\ttotal: 4.44s\tremaining: 9.52s\n",
      "318:\tlearn: 0.1900259\ttotal: 4.45s\tremaining: 9.51s\n",
      "319:\tlearn: 0.1898989\ttotal: 4.47s\tremaining: 9.49s\n",
      "320:\tlearn: 0.1898168\ttotal: 4.48s\tremaining: 9.48s\n",
      "321:\tlearn: 0.1897379\ttotal: 4.5s\tremaining: 9.47s\n",
      "322:\tlearn: 0.1896937\ttotal: 4.51s\tremaining: 9.45s\n",
      "323:\tlearn: 0.1896150\ttotal: 4.52s\tremaining: 9.44s\n",
      "324:\tlearn: 0.1895358\ttotal: 4.54s\tremaining: 9.42s\n",
      "325:\tlearn: 0.1894764\ttotal: 4.55s\tremaining: 9.4s\n",
      "326:\tlearn: 0.1892779\ttotal: 4.56s\tremaining: 9.39s\n",
      "327:\tlearn: 0.1891703\ttotal: 4.57s\tremaining: 9.37s\n",
      "328:\tlearn: 0.1890882\ttotal: 4.59s\tremaining: 9.36s\n",
      "329:\tlearn: 0.1890126\ttotal: 4.6s\tremaining: 9.34s\n",
      "330:\tlearn: 0.1888625\ttotal: 4.61s\tremaining: 9.32s\n",
      "331:\tlearn: 0.1887288\ttotal: 4.63s\tremaining: 9.31s\n",
      "332:\tlearn: 0.1887013\ttotal: 4.64s\tremaining: 9.3s\n",
      "333:\tlearn: 0.1886476\ttotal: 4.66s\tremaining: 9.28s\n",
      "334:\tlearn: 0.1885594\ttotal: 4.67s\tremaining: 9.27s\n",
      "335:\tlearn: 0.1884611\ttotal: 4.68s\tremaining: 9.26s\n",
      "336:\tlearn: 0.1884016\ttotal: 4.7s\tremaining: 9.24s\n",
      "337:\tlearn: 0.1883313\ttotal: 4.71s\tremaining: 9.23s\n",
      "338:\tlearn: 0.1882383\ttotal: 4.73s\tremaining: 9.22s\n",
      "339:\tlearn: 0.1879509\ttotal: 4.74s\tremaining: 9.2s\n",
      "340:\tlearn: 0.1877575\ttotal: 4.75s\tremaining: 9.19s\n",
      "341:\tlearn: 0.1874891\ttotal: 4.77s\tremaining: 9.18s\n",
      "342:\tlearn: 0.1873367\ttotal: 4.78s\tremaining: 9.16s\n",
      "343:\tlearn: 0.1872742\ttotal: 4.8s\tremaining: 9.15s\n",
      "344:\tlearn: 0.1872131\ttotal: 4.81s\tremaining: 9.13s\n",
      "345:\tlearn: 0.1870776\ttotal: 4.82s\tremaining: 9.12s\n",
      "346:\tlearn: 0.1869907\ttotal: 4.84s\tremaining: 9.1s\n",
      "347:\tlearn: 0.1869167\ttotal: 4.85s\tremaining: 9.09s\n",
      "348:\tlearn: 0.1867822\ttotal: 4.87s\tremaining: 9.08s\n",
      "349:\tlearn: 0.1867130\ttotal: 4.88s\tremaining: 9.06s\n",
      "350:\tlearn: 0.1866716\ttotal: 4.89s\tremaining: 9.05s\n",
      "351:\tlearn: 0.1864639\ttotal: 4.91s\tremaining: 9.03s\n",
      "352:\tlearn: 0.1863707\ttotal: 4.92s\tremaining: 9.02s\n",
      "353:\tlearn: 0.1862776\ttotal: 4.93s\tremaining: 9s\n",
      "354:\tlearn: 0.1860786\ttotal: 4.95s\tremaining: 8.98s\n",
      "355:\tlearn: 0.1860385\ttotal: 4.96s\tremaining: 8.97s\n",
      "356:\tlearn: 0.1859558\ttotal: 4.97s\tremaining: 8.95s\n",
      "357:\tlearn: 0.1857496\ttotal: 4.98s\tremaining: 8.94s\n",
      "358:\tlearn: 0.1856257\ttotal: 5s\tremaining: 8.92s\n",
      "359:\tlearn: 0.1855224\ttotal: 5.01s\tremaining: 8.91s\n",
      "360:\tlearn: 0.1854600\ttotal: 5.02s\tremaining: 8.89s\n",
      "361:\tlearn: 0.1853941\ttotal: 5.03s\tremaining: 8.87s\n",
      "362:\tlearn: 0.1853081\ttotal: 5.05s\tremaining: 8.86s\n",
      "363:\tlearn: 0.1852462\ttotal: 5.06s\tremaining: 8.85s\n",
      "364:\tlearn: 0.1851623\ttotal: 5.08s\tremaining: 8.83s\n",
      "365:\tlearn: 0.1850772\ttotal: 5.09s\tremaining: 8.82s\n",
      "366:\tlearn: 0.1849807\ttotal: 5.1s\tremaining: 8.8s\n",
      "367:\tlearn: 0.1849049\ttotal: 5.12s\tremaining: 8.79s\n",
      "368:\tlearn: 0.1848388\ttotal: 5.13s\tremaining: 8.77s\n",
      "369:\tlearn: 0.1847439\ttotal: 5.14s\tremaining: 8.76s\n",
      "370:\tlearn: 0.1846921\ttotal: 5.15s\tremaining: 8.74s\n",
      "371:\tlearn: 0.1846398\ttotal: 5.17s\tremaining: 8.72s\n",
      "372:\tlearn: 0.1845834\ttotal: 5.18s\tremaining: 8.71s\n",
      "373:\tlearn: 0.1845361\ttotal: 5.19s\tremaining: 8.69s\n",
      "374:\tlearn: 0.1844893\ttotal: 5.21s\tremaining: 8.68s\n",
      "375:\tlearn: 0.1844305\ttotal: 5.22s\tremaining: 8.67s\n",
      "376:\tlearn: 0.1843607\ttotal: 5.24s\tremaining: 8.65s\n",
      "377:\tlearn: 0.1841713\ttotal: 5.25s\tremaining: 8.64s\n",
      "378:\tlearn: 0.1840051\ttotal: 5.27s\tremaining: 8.63s\n",
      "379:\tlearn: 0.1838216\ttotal: 5.29s\tremaining: 8.63s\n",
      "380:\tlearn: 0.1837581\ttotal: 5.3s\tremaining: 8.61s\n",
      "381:\tlearn: 0.1836889\ttotal: 5.32s\tremaining: 8.6s\n",
      "382:\tlearn: 0.1835339\ttotal: 5.33s\tremaining: 8.58s\n",
      "383:\tlearn: 0.1834784\ttotal: 5.34s\tremaining: 8.57s\n",
      "384:\tlearn: 0.1834160\ttotal: 5.36s\tremaining: 8.55s\n",
      "385:\tlearn: 0.1833593\ttotal: 5.37s\tremaining: 8.54s\n",
      "386:\tlearn: 0.1832753\ttotal: 5.38s\tremaining: 8.52s\n",
      "387:\tlearn: 0.1832083\ttotal: 5.39s\tremaining: 8.51s\n",
      "388:\tlearn: 0.1831580\ttotal: 5.41s\tremaining: 8.49s\n",
      "389:\tlearn: 0.1830835\ttotal: 5.42s\tremaining: 8.48s\n",
      "390:\tlearn: 0.1829257\ttotal: 5.43s\tremaining: 8.46s\n",
      "391:\tlearn: 0.1828491\ttotal: 5.45s\tremaining: 8.45s\n",
      "392:\tlearn: 0.1827286\ttotal: 5.46s\tremaining: 8.43s\n",
      "393:\tlearn: 0.1826687\ttotal: 5.47s\tremaining: 8.42s\n",
      "394:\tlearn: 0.1825838\ttotal: 5.49s\tremaining: 8.4s\n",
      "395:\tlearn: 0.1825035\ttotal: 5.5s\tremaining: 8.39s\n",
      "396:\tlearn: 0.1824016\ttotal: 5.51s\tremaining: 8.38s\n",
      "397:\tlearn: 0.1823436\ttotal: 5.53s\tremaining: 8.36s\n",
      "398:\tlearn: 0.1822669\ttotal: 5.54s\tremaining: 8.35s\n",
      "399:\tlearn: 0.1821904\ttotal: 5.55s\tremaining: 8.33s\n",
      "400:\tlearn: 0.1821160\ttotal: 5.57s\tremaining: 8.32s\n",
      "401:\tlearn: 0.1820241\ttotal: 5.58s\tremaining: 8.3s\n",
      "402:\tlearn: 0.1819864\ttotal: 5.6s\tremaining: 8.29s\n",
      "403:\tlearn: 0.1819271\ttotal: 5.61s\tremaining: 8.28s\n",
      "404:\tlearn: 0.1818937\ttotal: 5.63s\tremaining: 8.27s\n",
      "405:\tlearn: 0.1817982\ttotal: 5.64s\tremaining: 8.25s\n",
      "406:\tlearn: 0.1817105\ttotal: 5.66s\tremaining: 8.24s\n",
      "407:\tlearn: 0.1815111\ttotal: 5.67s\tremaining: 8.23s\n",
      "408:\tlearn: 0.1814700\ttotal: 5.68s\tremaining: 8.21s\n",
      "409:\tlearn: 0.1814258\ttotal: 5.7s\tremaining: 8.2s\n",
      "410:\tlearn: 0.1812377\ttotal: 5.71s\tremaining: 8.18s\n",
      "411:\tlearn: 0.1811611\ttotal: 5.72s\tremaining: 8.16s\n",
      "412:\tlearn: 0.1811205\ttotal: 5.73s\tremaining: 8.15s\n",
      "413:\tlearn: 0.1810335\ttotal: 5.75s\tremaining: 8.13s\n",
      "414:\tlearn: 0.1809640\ttotal: 5.76s\tremaining: 8.12s\n",
      "415:\tlearn: 0.1808986\ttotal: 5.77s\tremaining: 8.1s\n",
      "416:\tlearn: 0.1808563\ttotal: 5.78s\tremaining: 8.09s\n",
      "417:\tlearn: 0.1807603\ttotal: 5.8s\tremaining: 8.07s\n",
      "418:\tlearn: 0.1807029\ttotal: 5.81s\tremaining: 8.06s\n",
      "419:\tlearn: 0.1806218\ttotal: 5.83s\tremaining: 8.05s\n",
      "420:\tlearn: 0.1805505\ttotal: 5.84s\tremaining: 8.03s\n",
      "421:\tlearn: 0.1804871\ttotal: 5.86s\tremaining: 8.02s\n",
      "422:\tlearn: 0.1804283\ttotal: 5.87s\tremaining: 8.01s\n",
      "423:\tlearn: 0.1803721\ttotal: 5.88s\tremaining: 7.99s\n",
      "424:\tlearn: 0.1802489\ttotal: 5.9s\tremaining: 7.98s\n",
      "425:\tlearn: 0.1801805\ttotal: 5.91s\tremaining: 7.96s\n",
      "426:\tlearn: 0.1801212\ttotal: 5.92s\tremaining: 7.95s\n",
      "427:\tlearn: 0.1800613\ttotal: 5.93s\tremaining: 7.93s\n",
      "428:\tlearn: 0.1800091\ttotal: 5.95s\tremaining: 7.92s\n",
      "429:\tlearn: 0.1799532\ttotal: 5.96s\tremaining: 7.91s\n",
      "430:\tlearn: 0.1799260\ttotal: 5.98s\tremaining: 7.89s\n",
      "431:\tlearn: 0.1798476\ttotal: 5.99s\tremaining: 7.88s\n",
      "432:\tlearn: 0.1797525\ttotal: 6s\tremaining: 7.86s\n",
      "433:\tlearn: 0.1796895\ttotal: 6.02s\tremaining: 7.85s\n",
      "434:\tlearn: 0.1796399\ttotal: 6.03s\tremaining: 7.83s\n",
      "435:\tlearn: 0.1795879\ttotal: 6.04s\tremaining: 7.82s\n",
      "436:\tlearn: 0.1795178\ttotal: 6.06s\tremaining: 7.81s\n",
      "437:\tlearn: 0.1794680\ttotal: 6.07s\tremaining: 7.79s\n",
      "438:\tlearn: 0.1792767\ttotal: 6.09s\tremaining: 7.78s\n",
      "439:\tlearn: 0.1792317\ttotal: 6.1s\tremaining: 7.77s\n",
      "440:\tlearn: 0.1791827\ttotal: 6.12s\tremaining: 7.75s\n",
      "441:\tlearn: 0.1791207\ttotal: 6.13s\tremaining: 7.74s\n",
      "442:\tlearn: 0.1790627\ttotal: 6.14s\tremaining: 7.72s\n",
      "443:\tlearn: 0.1789860\ttotal: 6.16s\tremaining: 7.71s\n",
      "444:\tlearn: 0.1789527\ttotal: 6.17s\tremaining: 7.69s\n",
      "445:\tlearn: 0.1788942\ttotal: 6.18s\tremaining: 7.68s\n",
      "446:\tlearn: 0.1788207\ttotal: 6.2s\tremaining: 7.67s\n",
      "447:\tlearn: 0.1787650\ttotal: 6.21s\tremaining: 7.65s\n",
      "448:\tlearn: 0.1786114\ttotal: 6.22s\tremaining: 7.64s\n",
      "449:\tlearn: 0.1785787\ttotal: 6.24s\tremaining: 7.62s\n",
      "450:\tlearn: 0.1785216\ttotal: 6.25s\tremaining: 7.61s\n",
      "451:\tlearn: 0.1784661\ttotal: 6.26s\tremaining: 7.59s\n",
      "452:\tlearn: 0.1783995\ttotal: 6.28s\tremaining: 7.58s\n",
      "453:\tlearn: 0.1783302\ttotal: 6.29s\tremaining: 7.57s\n",
      "454:\tlearn: 0.1781583\ttotal: 6.31s\tremaining: 7.55s\n",
      "455:\tlearn: 0.1781111\ttotal: 6.32s\tremaining: 7.54s\n",
      "456:\tlearn: 0.1780133\ttotal: 6.33s\tremaining: 7.53s\n",
      "457:\tlearn: 0.1779154\ttotal: 6.35s\tremaining: 7.51s\n",
      "458:\tlearn: 0.1778648\ttotal: 6.36s\tremaining: 7.5s\n",
      "459:\tlearn: 0.1777898\ttotal: 6.38s\tremaining: 7.49s\n",
      "460:\tlearn: 0.1777473\ttotal: 6.39s\tremaining: 7.47s\n",
      "461:\tlearn: 0.1776931\ttotal: 6.4s\tremaining: 7.46s\n",
      "462:\tlearn: 0.1776267\ttotal: 6.42s\tremaining: 7.44s\n",
      "463:\tlearn: 0.1775737\ttotal: 6.43s\tremaining: 7.43s\n",
      "464:\tlearn: 0.1774896\ttotal: 6.44s\tremaining: 7.41s\n",
      "465:\tlearn: 0.1774377\ttotal: 6.46s\tremaining: 7.4s\n",
      "466:\tlearn: 0.1773026\ttotal: 6.47s\tremaining: 7.38s\n",
      "467:\tlearn: 0.1772031\ttotal: 6.49s\tremaining: 7.37s\n",
      "468:\tlearn: 0.1771236\ttotal: 6.5s\tremaining: 7.36s\n",
      "469:\tlearn: 0.1769866\ttotal: 6.51s\tremaining: 7.34s\n",
      "470:\tlearn: 0.1768895\ttotal: 6.52s\tremaining: 7.33s\n",
      "471:\tlearn: 0.1768201\ttotal: 6.54s\tremaining: 7.31s\n",
      "472:\tlearn: 0.1767976\ttotal: 6.55s\tremaining: 7.3s\n",
      "473:\tlearn: 0.1767462\ttotal: 6.56s\tremaining: 7.28s\n",
      "474:\tlearn: 0.1766912\ttotal: 6.58s\tremaining: 7.27s\n",
      "475:\tlearn: 0.1765897\ttotal: 6.59s\tremaining: 7.25s\n",
      "476:\tlearn: 0.1765169\ttotal: 6.6s\tremaining: 7.24s\n",
      "477:\tlearn: 0.1764679\ttotal: 6.62s\tremaining: 7.23s\n",
      "478:\tlearn: 0.1764129\ttotal: 6.63s\tremaining: 7.21s\n",
      "479:\tlearn: 0.1762853\ttotal: 6.64s\tremaining: 7.2s\n",
      "480:\tlearn: 0.1762261\ttotal: 6.66s\tremaining: 7.18s\n",
      "481:\tlearn: 0.1760770\ttotal: 6.67s\tremaining: 7.17s\n",
      "482:\tlearn: 0.1759608\ttotal: 6.68s\tremaining: 7.16s\n",
      "483:\tlearn: 0.1759259\ttotal: 6.7s\tremaining: 7.14s\n",
      "484:\tlearn: 0.1758545\ttotal: 6.71s\tremaining: 7.13s\n",
      "485:\tlearn: 0.1757901\ttotal: 6.73s\tremaining: 7.12s\n",
      "486:\tlearn: 0.1757578\ttotal: 6.74s\tremaining: 7.1s\n",
      "487:\tlearn: 0.1757107\ttotal: 6.75s\tremaining: 7.08s\n",
      "488:\tlearn: 0.1756584\ttotal: 6.77s\tremaining: 7.07s\n",
      "489:\tlearn: 0.1756252\ttotal: 6.78s\tremaining: 7.05s\n",
      "490:\tlearn: 0.1755556\ttotal: 6.79s\tremaining: 7.04s\n",
      "491:\tlearn: 0.1755278\ttotal: 6.81s\tremaining: 7.03s\n",
      "492:\tlearn: 0.1754637\ttotal: 6.82s\tremaining: 7.01s\n",
      "493:\tlearn: 0.1753561\ttotal: 6.83s\tremaining: 7s\n",
      "494:\tlearn: 0.1752995\ttotal: 6.85s\tremaining: 6.98s\n",
      "495:\tlearn: 0.1752641\ttotal: 6.86s\tremaining: 6.97s\n",
      "496:\tlearn: 0.1752308\ttotal: 6.87s\tremaining: 6.96s\n",
      "497:\tlearn: 0.1751211\ttotal: 6.89s\tremaining: 6.94s\n",
      "498:\tlearn: 0.1750479\ttotal: 6.9s\tremaining: 6.93s\n",
      "499:\tlearn: 0.1749840\ttotal: 6.92s\tremaining: 6.92s\n",
      "500:\tlearn: 0.1749154\ttotal: 6.93s\tremaining: 6.9s\n",
      "501:\tlearn: 0.1748802\ttotal: 6.94s\tremaining: 6.89s\n",
      "502:\tlearn: 0.1748299\ttotal: 6.96s\tremaining: 6.87s\n",
      "503:\tlearn: 0.1746983\ttotal: 6.97s\tremaining: 6.86s\n",
      "504:\tlearn: 0.1746686\ttotal: 6.98s\tremaining: 6.84s\n",
      "505:\tlearn: 0.1746133\ttotal: 7s\tremaining: 6.83s\n",
      "506:\tlearn: 0.1745860\ttotal: 7.01s\tremaining: 6.81s\n",
      "507:\tlearn: 0.1745298\ttotal: 7.02s\tremaining: 6.8s\n",
      "508:\tlearn: 0.1744596\ttotal: 7.04s\tremaining: 6.79s\n",
      "509:\tlearn: 0.1743909\ttotal: 7.05s\tremaining: 6.77s\n",
      "510:\tlearn: 0.1743347\ttotal: 7.06s\tremaining: 6.76s\n",
      "511:\tlearn: 0.1742825\ttotal: 7.08s\tremaining: 6.74s\n",
      "512:\tlearn: 0.1741905\ttotal: 7.09s\tremaining: 6.73s\n",
      "513:\tlearn: 0.1740811\ttotal: 7.11s\tremaining: 6.72s\n",
      "514:\tlearn: 0.1739134\ttotal: 7.12s\tremaining: 6.7s\n",
      "515:\tlearn: 0.1738720\ttotal: 7.13s\tremaining: 6.69s\n",
      "516:\tlearn: 0.1737593\ttotal: 7.14s\tremaining: 6.67s\n",
      "517:\tlearn: 0.1737016\ttotal: 7.16s\tremaining: 6.66s\n",
      "518:\tlearn: 0.1736452\ttotal: 7.17s\tremaining: 6.65s\n",
      "519:\tlearn: 0.1734761\ttotal: 7.19s\tremaining: 6.63s\n",
      "520:\tlearn: 0.1733809\ttotal: 7.2s\tremaining: 6.62s\n",
      "521:\tlearn: 0.1733136\ttotal: 7.21s\tremaining: 6.61s\n",
      "522:\tlearn: 0.1732703\ttotal: 7.23s\tremaining: 6.59s\n",
      "523:\tlearn: 0.1732143\ttotal: 7.24s\tremaining: 6.58s\n",
      "524:\tlearn: 0.1731545\ttotal: 7.25s\tremaining: 6.56s\n",
      "525:\tlearn: 0.1731146\ttotal: 7.27s\tremaining: 6.55s\n",
      "526:\tlearn: 0.1730555\ttotal: 7.28s\tremaining: 6.53s\n",
      "527:\tlearn: 0.1729945\ttotal: 7.29s\tremaining: 6.52s\n",
      "528:\tlearn: 0.1729564\ttotal: 7.31s\tremaining: 6.51s\n",
      "529:\tlearn: 0.1728884\ttotal: 7.32s\tremaining: 6.49s\n",
      "530:\tlearn: 0.1728496\ttotal: 7.33s\tremaining: 6.48s\n",
      "531:\tlearn: 0.1727492\ttotal: 7.35s\tremaining: 6.47s\n",
      "532:\tlearn: 0.1727000\ttotal: 7.36s\tremaining: 6.45s\n",
      "533:\tlearn: 0.1726654\ttotal: 7.38s\tremaining: 6.44s\n",
      "534:\tlearn: 0.1726000\ttotal: 7.39s\tremaining: 6.42s\n",
      "535:\tlearn: 0.1725057\ttotal: 7.4s\tremaining: 6.41s\n",
      "536:\tlearn: 0.1724463\ttotal: 7.42s\tremaining: 6.39s\n",
      "537:\tlearn: 0.1724118\ttotal: 7.43s\tremaining: 6.38s\n",
      "538:\tlearn: 0.1723803\ttotal: 7.45s\tremaining: 6.37s\n",
      "539:\tlearn: 0.1722447\ttotal: 7.46s\tremaining: 6.36s\n",
      "540:\tlearn: 0.1721904\ttotal: 7.47s\tremaining: 6.34s\n",
      "541:\tlearn: 0.1721329\ttotal: 7.49s\tremaining: 6.33s\n",
      "542:\tlearn: 0.1720715\ttotal: 7.5s\tremaining: 6.31s\n",
      "543:\tlearn: 0.1720032\ttotal: 7.51s\tremaining: 6.3s\n",
      "544:\tlearn: 0.1718813\ttotal: 7.53s\tremaining: 6.28s\n",
      "545:\tlearn: 0.1718154\ttotal: 7.54s\tremaining: 6.27s\n",
      "546:\tlearn: 0.1717551\ttotal: 7.55s\tremaining: 6.25s\n",
      "547:\tlearn: 0.1716616\ttotal: 7.57s\tremaining: 6.24s\n",
      "548:\tlearn: 0.1716037\ttotal: 7.58s\tremaining: 6.23s\n",
      "549:\tlearn: 0.1715382\ttotal: 7.6s\tremaining: 6.21s\n",
      "550:\tlearn: 0.1714839\ttotal: 7.61s\tremaining: 6.2s\n",
      "551:\tlearn: 0.1713515\ttotal: 7.63s\tremaining: 6.19s\n",
      "552:\tlearn: 0.1713137\ttotal: 7.64s\tremaining: 6.18s\n",
      "553:\tlearn: 0.1712353\ttotal: 7.66s\tremaining: 6.16s\n",
      "554:\tlearn: 0.1711902\ttotal: 7.67s\tremaining: 6.15s\n",
      "555:\tlearn: 0.1711584\ttotal: 7.68s\tremaining: 6.14s\n",
      "556:\tlearn: 0.1711162\ttotal: 7.7s\tremaining: 6.12s\n",
      "557:\tlearn: 0.1710497\ttotal: 7.71s\tremaining: 6.11s\n",
      "558:\tlearn: 0.1709558\ttotal: 7.72s\tremaining: 6.09s\n",
      "559:\tlearn: 0.1707901\ttotal: 7.74s\tremaining: 6.08s\n",
      "560:\tlearn: 0.1707407\ttotal: 7.75s\tremaining: 6.07s\n",
      "561:\tlearn: 0.1706881\ttotal: 7.76s\tremaining: 6.05s\n",
      "562:\tlearn: 0.1705798\ttotal: 7.78s\tremaining: 6.04s\n",
      "563:\tlearn: 0.1705325\ttotal: 7.79s\tremaining: 6.02s\n",
      "564:\tlearn: 0.1704432\ttotal: 7.8s\tremaining: 6.01s\n",
      "565:\tlearn: 0.1703984\ttotal: 7.82s\tremaining: 5.99s\n",
      "566:\tlearn: 0.1703439\ttotal: 7.83s\tremaining: 5.98s\n",
      "567:\tlearn: 0.1702605\ttotal: 7.84s\tremaining: 5.97s\n",
      "568:\tlearn: 0.1701824\ttotal: 7.86s\tremaining: 5.95s\n",
      "569:\tlearn: 0.1701407\ttotal: 7.87s\tremaining: 5.94s\n",
      "570:\tlearn: 0.1700633\ttotal: 7.89s\tremaining: 5.93s\n",
      "571:\tlearn: 0.1700103\ttotal: 7.9s\tremaining: 5.91s\n",
      "572:\tlearn: 0.1699500\ttotal: 7.92s\tremaining: 5.9s\n",
      "573:\tlearn: 0.1698875\ttotal: 7.93s\tremaining: 5.88s\n",
      "574:\tlearn: 0.1698218\ttotal: 7.94s\tremaining: 5.87s\n",
      "575:\tlearn: 0.1697475\ttotal: 7.96s\tremaining: 5.86s\n",
      "576:\tlearn: 0.1696969\ttotal: 7.97s\tremaining: 5.84s\n",
      "577:\tlearn: 0.1696681\ttotal: 7.98s\tremaining: 5.83s\n",
      "578:\tlearn: 0.1695340\ttotal: 8s\tremaining: 5.81s\n",
      "579:\tlearn: 0.1694875\ttotal: 8.01s\tremaining: 5.8s\n",
      "580:\tlearn: 0.1693989\ttotal: 8.02s\tremaining: 5.79s\n",
      "581:\tlearn: 0.1693794\ttotal: 8.04s\tremaining: 5.77s\n",
      "582:\tlearn: 0.1693341\ttotal: 8.05s\tremaining: 5.76s\n",
      "583:\tlearn: 0.1692975\ttotal: 8.06s\tremaining: 5.74s\n",
      "584:\tlearn: 0.1692069\ttotal: 8.08s\tremaining: 5.73s\n",
      "585:\tlearn: 0.1691374\ttotal: 8.09s\tremaining: 5.72s\n",
      "586:\tlearn: 0.1690960\ttotal: 8.11s\tremaining: 5.7s\n",
      "587:\tlearn: 0.1690470\ttotal: 8.12s\tremaining: 5.69s\n",
      "588:\tlearn: 0.1690093\ttotal: 8.13s\tremaining: 5.67s\n",
      "589:\tlearn: 0.1689294\ttotal: 8.15s\tremaining: 5.66s\n",
      "590:\tlearn: 0.1688724\ttotal: 8.16s\tremaining: 5.65s\n",
      "591:\tlearn: 0.1688028\ttotal: 8.18s\tremaining: 5.63s\n",
      "592:\tlearn: 0.1687704\ttotal: 8.19s\tremaining: 5.62s\n",
      "593:\tlearn: 0.1687266\ttotal: 8.2s\tremaining: 5.61s\n",
      "594:\tlearn: 0.1686735\ttotal: 8.22s\tremaining: 5.59s\n",
      "595:\tlearn: 0.1686340\ttotal: 8.23s\tremaining: 5.58s\n",
      "596:\tlearn: 0.1685587\ttotal: 8.24s\tremaining: 5.57s\n",
      "597:\tlearn: 0.1684823\ttotal: 8.26s\tremaining: 5.55s\n",
      "598:\tlearn: 0.1684093\ttotal: 8.27s\tremaining: 5.54s\n",
      "599:\tlearn: 0.1683681\ttotal: 8.29s\tremaining: 5.52s\n",
      "600:\tlearn: 0.1683221\ttotal: 8.3s\tremaining: 5.51s\n",
      "601:\tlearn: 0.1682664\ttotal: 8.31s\tremaining: 5.5s\n",
      "602:\tlearn: 0.1682095\ttotal: 8.33s\tremaining: 5.48s\n",
      "603:\tlearn: 0.1681664\ttotal: 8.34s\tremaining: 5.47s\n",
      "604:\tlearn: 0.1681032\ttotal: 8.35s\tremaining: 5.45s\n",
      "605:\tlearn: 0.1680516\ttotal: 8.37s\tremaining: 5.44s\n",
      "606:\tlearn: 0.1680066\ttotal: 8.38s\tremaining: 5.42s\n",
      "607:\tlearn: 0.1679630\ttotal: 8.39s\tremaining: 5.41s\n",
      "608:\tlearn: 0.1679356\ttotal: 8.41s\tremaining: 5.4s\n",
      "609:\tlearn: 0.1678821\ttotal: 8.42s\tremaining: 5.38s\n",
      "610:\tlearn: 0.1678331\ttotal: 8.43s\tremaining: 5.37s\n",
      "611:\tlearn: 0.1677984\ttotal: 8.45s\tremaining: 5.36s\n",
      "612:\tlearn: 0.1676768\ttotal: 8.46s\tremaining: 5.34s\n",
      "613:\tlearn: 0.1676321\ttotal: 8.47s\tremaining: 5.33s\n",
      "614:\tlearn: 0.1675818\ttotal: 8.49s\tremaining: 5.31s\n",
      "615:\tlearn: 0.1675494\ttotal: 8.5s\tremaining: 5.3s\n",
      "616:\tlearn: 0.1675168\ttotal: 8.51s\tremaining: 5.29s\n",
      "617:\tlearn: 0.1674801\ttotal: 8.53s\tremaining: 5.27s\n",
      "618:\tlearn: 0.1674271\ttotal: 8.54s\tremaining: 5.26s\n",
      "619:\tlearn: 0.1673454\ttotal: 8.55s\tremaining: 5.24s\n",
      "620:\tlearn: 0.1671756\ttotal: 8.57s\tremaining: 5.23s\n",
      "621:\tlearn: 0.1671505\ttotal: 8.58s\tremaining: 5.21s\n",
      "622:\tlearn: 0.1671059\ttotal: 8.6s\tremaining: 5.2s\n",
      "623:\tlearn: 0.1670674\ttotal: 8.61s\tremaining: 5.19s\n",
      "624:\tlearn: 0.1670213\ttotal: 8.62s\tremaining: 5.17s\n",
      "625:\tlearn: 0.1669633\ttotal: 8.63s\tremaining: 5.16s\n",
      "626:\tlearn: 0.1669313\ttotal: 8.65s\tremaining: 5.14s\n",
      "627:\tlearn: 0.1667590\ttotal: 8.66s\tremaining: 5.13s\n",
      "628:\tlearn: 0.1666893\ttotal: 8.68s\tremaining: 5.12s\n",
      "629:\tlearn: 0.1666336\ttotal: 8.69s\tremaining: 5.1s\n",
      "630:\tlearn: 0.1666006\ttotal: 8.7s\tremaining: 5.09s\n",
      "631:\tlearn: 0.1665510\ttotal: 8.72s\tremaining: 5.08s\n",
      "632:\tlearn: 0.1665296\ttotal: 8.73s\tremaining: 5.06s\n",
      "633:\tlearn: 0.1664652\ttotal: 8.75s\tremaining: 5.05s\n",
      "634:\tlearn: 0.1663495\ttotal: 8.76s\tremaining: 5.03s\n",
      "635:\tlearn: 0.1662679\ttotal: 8.77s\tremaining: 5.02s\n",
      "636:\tlearn: 0.1662138\ttotal: 8.79s\tremaining: 5.01s\n",
      "637:\tlearn: 0.1661404\ttotal: 8.8s\tremaining: 4.99s\n",
      "638:\tlearn: 0.1661002\ttotal: 8.81s\tremaining: 4.98s\n",
      "639:\tlearn: 0.1660380\ttotal: 8.83s\tremaining: 4.96s\n",
      "640:\tlearn: 0.1659051\ttotal: 8.84s\tremaining: 4.95s\n",
      "641:\tlearn: 0.1658592\ttotal: 8.86s\tremaining: 4.94s\n",
      "642:\tlearn: 0.1657994\ttotal: 8.87s\tremaining: 4.92s\n",
      "643:\tlearn: 0.1657325\ttotal: 8.88s\tremaining: 4.91s\n",
      "644:\tlearn: 0.1655776\ttotal: 8.9s\tremaining: 4.9s\n",
      "645:\tlearn: 0.1655400\ttotal: 8.91s\tremaining: 4.88s\n",
      "646:\tlearn: 0.1655088\ttotal: 8.92s\tremaining: 4.87s\n",
      "647:\tlearn: 0.1654702\ttotal: 8.93s\tremaining: 4.85s\n",
      "648:\tlearn: 0.1654237\ttotal: 8.95s\tremaining: 4.84s\n",
      "649:\tlearn: 0.1653740\ttotal: 8.96s\tremaining: 4.82s\n",
      "650:\tlearn: 0.1653048\ttotal: 8.97s\tremaining: 4.81s\n",
      "651:\tlearn: 0.1652520\ttotal: 8.98s\tremaining: 4.8s\n",
      "652:\tlearn: 0.1651768\ttotal: 9s\tremaining: 4.78s\n",
      "653:\tlearn: 0.1651404\ttotal: 9.01s\tremaining: 4.77s\n",
      "654:\tlearn: 0.1650769\ttotal: 9.03s\tremaining: 4.75s\n",
      "655:\tlearn: 0.1650390\ttotal: 9.04s\tremaining: 4.74s\n",
      "656:\tlearn: 0.1650020\ttotal: 9.05s\tremaining: 4.73s\n",
      "657:\tlearn: 0.1648887\ttotal: 9.07s\tremaining: 4.71s\n",
      "658:\tlearn: 0.1648273\ttotal: 9.08s\tremaining: 4.7s\n",
      "659:\tlearn: 0.1647825\ttotal: 9.1s\tremaining: 4.68s\n",
      "660:\tlearn: 0.1647152\ttotal: 9.11s\tremaining: 4.67s\n",
      "661:\tlearn: 0.1646361\ttotal: 9.12s\tremaining: 4.66s\n",
      "662:\tlearn: 0.1645903\ttotal: 9.13s\tremaining: 4.64s\n",
      "663:\tlearn: 0.1645252\ttotal: 9.15s\tremaining: 4.63s\n",
      "664:\tlearn: 0.1644998\ttotal: 9.16s\tremaining: 4.62s\n",
      "665:\tlearn: 0.1644124\ttotal: 9.18s\tremaining: 4.6s\n",
      "666:\tlearn: 0.1643407\ttotal: 9.19s\tremaining: 4.59s\n",
      "667:\tlearn: 0.1642994\ttotal: 9.2s\tremaining: 4.57s\n",
      "668:\tlearn: 0.1641941\ttotal: 9.22s\tremaining: 4.56s\n",
      "669:\tlearn: 0.1641741\ttotal: 9.23s\tremaining: 4.55s\n",
      "670:\tlearn: 0.1640839\ttotal: 9.25s\tremaining: 4.53s\n",
      "671:\tlearn: 0.1640535\ttotal: 9.26s\tremaining: 4.52s\n",
      "672:\tlearn: 0.1640150\ttotal: 9.28s\tremaining: 4.51s\n",
      "673:\tlearn: 0.1639526\ttotal: 9.29s\tremaining: 4.49s\n",
      "674:\tlearn: 0.1639113\ttotal: 9.3s\tremaining: 4.48s\n",
      "675:\tlearn: 0.1638784\ttotal: 9.31s\tremaining: 4.46s\n",
      "676:\tlearn: 0.1638187\ttotal: 9.33s\tremaining: 4.45s\n",
      "677:\tlearn: 0.1637702\ttotal: 9.34s\tremaining: 4.44s\n",
      "678:\tlearn: 0.1636453\ttotal: 9.36s\tremaining: 4.42s\n",
      "679:\tlearn: 0.1635787\ttotal: 9.37s\tremaining: 4.41s\n",
      "680:\tlearn: 0.1635354\ttotal: 9.38s\tremaining: 4.39s\n",
      "681:\tlearn: 0.1634904\ttotal: 9.4s\tremaining: 4.38s\n",
      "682:\tlearn: 0.1634409\ttotal: 9.41s\tremaining: 4.37s\n",
      "683:\tlearn: 0.1633783\ttotal: 9.43s\tremaining: 4.35s\n",
      "684:\tlearn: 0.1633173\ttotal: 9.44s\tremaining: 4.34s\n",
      "685:\tlearn: 0.1632665\ttotal: 9.46s\tremaining: 4.33s\n",
      "686:\tlearn: 0.1632306\ttotal: 9.47s\tremaining: 4.32s\n",
      "687:\tlearn: 0.1631897\ttotal: 9.49s\tremaining: 4.3s\n",
      "688:\tlearn: 0.1631676\ttotal: 9.5s\tremaining: 4.29s\n",
      "689:\tlearn: 0.1631205\ttotal: 9.51s\tremaining: 4.27s\n",
      "690:\tlearn: 0.1630824\ttotal: 9.53s\tremaining: 4.26s\n",
      "691:\tlearn: 0.1630329\ttotal: 9.54s\tremaining: 4.25s\n",
      "692:\tlearn: 0.1629802\ttotal: 9.55s\tremaining: 4.23s\n",
      "693:\tlearn: 0.1629388\ttotal: 9.57s\tremaining: 4.22s\n",
      "694:\tlearn: 0.1628902\ttotal: 9.59s\tremaining: 4.21s\n",
      "695:\tlearn: 0.1627996\ttotal: 9.6s\tremaining: 4.19s\n",
      "696:\tlearn: 0.1627805\ttotal: 9.61s\tremaining: 4.18s\n",
      "697:\tlearn: 0.1627498\ttotal: 9.62s\tremaining: 4.16s\n",
      "698:\tlearn: 0.1627005\ttotal: 9.64s\tremaining: 4.15s\n",
      "699:\tlearn: 0.1626524\ttotal: 9.65s\tremaining: 4.14s\n",
      "700:\tlearn: 0.1626220\ttotal: 9.67s\tremaining: 4.12s\n",
      "701:\tlearn: 0.1625961\ttotal: 9.68s\tremaining: 4.11s\n",
      "702:\tlearn: 0.1625338\ttotal: 9.69s\tremaining: 4.09s\n",
      "703:\tlearn: 0.1624810\ttotal: 9.71s\tremaining: 4.08s\n",
      "704:\tlearn: 0.1624183\ttotal: 9.72s\tremaining: 4.07s\n",
      "705:\tlearn: 0.1622882\ttotal: 9.73s\tremaining: 4.05s\n",
      "706:\tlearn: 0.1622460\ttotal: 9.74s\tremaining: 4.04s\n",
      "707:\tlearn: 0.1621851\ttotal: 9.76s\tremaining: 4.02s\n",
      "708:\tlearn: 0.1621182\ttotal: 9.77s\tremaining: 4.01s\n",
      "709:\tlearn: 0.1620888\ttotal: 9.78s\tremaining: 4s\n",
      "710:\tlearn: 0.1620378\ttotal: 9.8s\tremaining: 3.98s\n",
      "711:\tlearn: 0.1619975\ttotal: 9.81s\tremaining: 3.97s\n",
      "712:\tlearn: 0.1619329\ttotal: 9.82s\tremaining: 3.95s\n",
      "713:\tlearn: 0.1618847\ttotal: 9.84s\tremaining: 3.94s\n",
      "714:\tlearn: 0.1617683\ttotal: 9.85s\tremaining: 3.93s\n",
      "715:\tlearn: 0.1616900\ttotal: 9.87s\tremaining: 3.91s\n",
      "716:\tlearn: 0.1616356\ttotal: 9.88s\tremaining: 3.9s\n",
      "717:\tlearn: 0.1616050\ttotal: 9.89s\tremaining: 3.88s\n",
      "718:\tlearn: 0.1615542\ttotal: 9.91s\tremaining: 3.87s\n",
      "719:\tlearn: 0.1614798\ttotal: 9.92s\tremaining: 3.86s\n",
      "720:\tlearn: 0.1613983\ttotal: 9.93s\tremaining: 3.84s\n",
      "721:\tlearn: 0.1613427\ttotal: 9.95s\tremaining: 3.83s\n",
      "722:\tlearn: 0.1612882\ttotal: 9.96s\tremaining: 3.81s\n",
      "723:\tlearn: 0.1612307\ttotal: 9.97s\tremaining: 3.8s\n",
      "724:\tlearn: 0.1611866\ttotal: 9.99s\tremaining: 3.79s\n",
      "725:\tlearn: 0.1611495\ttotal: 10s\tremaining: 3.77s\n",
      "726:\tlearn: 0.1611202\ttotal: 10s\tremaining: 3.76s\n",
      "727:\tlearn: 0.1610910\ttotal: 10s\tremaining: 3.75s\n",
      "728:\tlearn: 0.1610645\ttotal: 10s\tremaining: 3.73s\n",
      "729:\tlearn: 0.1609912\ttotal: 10.1s\tremaining: 3.72s\n",
      "730:\tlearn: 0.1609558\ttotal: 10.1s\tremaining: 3.71s\n",
      "731:\tlearn: 0.1609027\ttotal: 10.1s\tremaining: 3.69s\n",
      "732:\tlearn: 0.1608525\ttotal: 10.1s\tremaining: 3.68s\n",
      "733:\tlearn: 0.1608032\ttotal: 10.1s\tremaining: 3.66s\n",
      "734:\tlearn: 0.1607315\ttotal: 10.1s\tremaining: 3.65s\n",
      "735:\tlearn: 0.1606859\ttotal: 10.1s\tremaining: 3.64s\n",
      "736:\tlearn: 0.1606253\ttotal: 10.2s\tremaining: 3.62s\n",
      "737:\tlearn: 0.1605783\ttotal: 10.2s\tremaining: 3.61s\n",
      "738:\tlearn: 0.1604809\ttotal: 10.2s\tremaining: 3.6s\n",
      "739:\tlearn: 0.1603924\ttotal: 10.2s\tremaining: 3.58s\n",
      "740:\tlearn: 0.1603399\ttotal: 10.2s\tremaining: 3.57s\n",
      "741:\tlearn: 0.1602706\ttotal: 10.2s\tremaining: 3.55s\n",
      "742:\tlearn: 0.1602296\ttotal: 10.2s\tremaining: 3.54s\n",
      "743:\tlearn: 0.1601966\ttotal: 10.3s\tremaining: 3.53s\n",
      "744:\tlearn: 0.1601775\ttotal: 10.3s\tremaining: 3.51s\n",
      "745:\tlearn: 0.1600901\ttotal: 10.3s\tremaining: 3.5s\n",
      "746:\tlearn: 0.1600327\ttotal: 10.3s\tremaining: 3.48s\n",
      "747:\tlearn: 0.1599247\ttotal: 10.3s\tremaining: 3.47s\n",
      "748:\tlearn: 0.1598904\ttotal: 10.3s\tremaining: 3.46s\n",
      "749:\tlearn: 0.1598389\ttotal: 10.3s\tremaining: 3.44s\n",
      "750:\tlearn: 0.1597874\ttotal: 10.3s\tremaining: 3.43s\n",
      "751:\tlearn: 0.1597532\ttotal: 10.4s\tremaining: 3.42s\n",
      "752:\tlearn: 0.1596713\ttotal: 10.4s\tremaining: 3.4s\n",
      "753:\tlearn: 0.1596025\ttotal: 10.4s\tremaining: 3.39s\n",
      "754:\tlearn: 0.1595532\ttotal: 10.4s\tremaining: 3.37s\n",
      "755:\tlearn: 0.1595255\ttotal: 10.4s\tremaining: 3.36s\n",
      "756:\tlearn: 0.1594771\ttotal: 10.4s\tremaining: 3.35s\n",
      "757:\tlearn: 0.1594390\ttotal: 10.4s\tremaining: 3.33s\n",
      "758:\tlearn: 0.1593586\ttotal: 10.5s\tremaining: 3.32s\n",
      "759:\tlearn: 0.1592740\ttotal: 10.5s\tremaining: 3.31s\n",
      "760:\tlearn: 0.1592420\ttotal: 10.5s\tremaining: 3.29s\n",
      "761:\tlearn: 0.1591967\ttotal: 10.5s\tremaining: 3.28s\n",
      "762:\tlearn: 0.1591578\ttotal: 10.5s\tremaining: 3.27s\n",
      "763:\tlearn: 0.1591062\ttotal: 10.5s\tremaining: 3.25s\n",
      "764:\tlearn: 0.1590153\ttotal: 10.5s\tremaining: 3.24s\n",
      "765:\tlearn: 0.1589429\ttotal: 10.6s\tremaining: 3.23s\n",
      "766:\tlearn: 0.1589128\ttotal: 10.6s\tremaining: 3.21s\n",
      "767:\tlearn: 0.1588633\ttotal: 10.6s\tremaining: 3.2s\n",
      "768:\tlearn: 0.1587856\ttotal: 10.6s\tremaining: 3.18s\n",
      "769:\tlearn: 0.1587457\ttotal: 10.6s\tremaining: 3.17s\n",
      "770:\tlearn: 0.1587128\ttotal: 10.6s\tremaining: 3.16s\n",
      "771:\tlearn: 0.1586430\ttotal: 10.6s\tremaining: 3.14s\n",
      "772:\tlearn: 0.1585352\ttotal: 10.7s\tremaining: 3.13s\n",
      "773:\tlearn: 0.1585073\ttotal: 10.7s\tremaining: 3.11s\n",
      "774:\tlearn: 0.1584494\ttotal: 10.7s\tremaining: 3.1s\n",
      "775:\tlearn: 0.1583824\ttotal: 10.7s\tremaining: 3.09s\n",
      "776:\tlearn: 0.1583235\ttotal: 10.7s\tremaining: 3.07s\n",
      "777:\tlearn: 0.1582598\ttotal: 10.7s\tremaining: 3.06s\n",
      "778:\tlearn: 0.1582161\ttotal: 10.7s\tremaining: 3.04s\n",
      "779:\tlearn: 0.1581260\ttotal: 10.7s\tremaining: 3.03s\n",
      "780:\tlearn: 0.1580997\ttotal: 10.8s\tremaining: 3.02s\n",
      "781:\tlearn: 0.1580594\ttotal: 10.8s\tremaining: 3s\n",
      "782:\tlearn: 0.1580150\ttotal: 10.8s\tremaining: 2.99s\n",
      "783:\tlearn: 0.1579849\ttotal: 10.8s\tremaining: 2.98s\n",
      "784:\tlearn: 0.1579435\ttotal: 10.8s\tremaining: 2.96s\n",
      "785:\tlearn: 0.1579037\ttotal: 10.8s\tremaining: 2.95s\n",
      "786:\tlearn: 0.1578547\ttotal: 10.8s\tremaining: 2.93s\n",
      "787:\tlearn: 0.1578273\ttotal: 10.9s\tremaining: 2.92s\n",
      "788:\tlearn: 0.1577699\ttotal: 10.9s\tremaining: 2.91s\n",
      "789:\tlearn: 0.1577141\ttotal: 10.9s\tremaining: 2.89s\n",
      "790:\tlearn: 0.1576702\ttotal: 10.9s\tremaining: 2.88s\n",
      "791:\tlearn: 0.1576304\ttotal: 10.9s\tremaining: 2.87s\n",
      "792:\tlearn: 0.1575874\ttotal: 10.9s\tremaining: 2.85s\n",
      "793:\tlearn: 0.1575234\ttotal: 10.9s\tremaining: 2.84s\n",
      "794:\tlearn: 0.1574734\ttotal: 10.9s\tremaining: 2.82s\n",
      "795:\tlearn: 0.1574340\ttotal: 11s\tremaining: 2.81s\n",
      "796:\tlearn: 0.1574062\ttotal: 11s\tremaining: 2.79s\n",
      "797:\tlearn: 0.1573682\ttotal: 11s\tremaining: 2.78s\n",
      "798:\tlearn: 0.1573088\ttotal: 11s\tremaining: 2.77s\n",
      "799:\tlearn: 0.1572618\ttotal: 11s\tremaining: 2.75s\n",
      "800:\tlearn: 0.1572119\ttotal: 11s\tremaining: 2.74s\n",
      "801:\tlearn: 0.1571697\ttotal: 11s\tremaining: 2.73s\n",
      "802:\tlearn: 0.1571237\ttotal: 11.1s\tremaining: 2.71s\n",
      "803:\tlearn: 0.1570955\ttotal: 11.1s\tremaining: 2.7s\n",
      "804:\tlearn: 0.1570762\ttotal: 11.1s\tremaining: 2.69s\n",
      "805:\tlearn: 0.1570594\ttotal: 11.1s\tremaining: 2.67s\n",
      "806:\tlearn: 0.1570160\ttotal: 11.1s\tremaining: 2.66s\n",
      "807:\tlearn: 0.1569712\ttotal: 11.1s\tremaining: 2.64s\n",
      "808:\tlearn: 0.1569286\ttotal: 11.1s\tremaining: 2.63s\n",
      "809:\tlearn: 0.1568620\ttotal: 11.2s\tremaining: 2.62s\n",
      "810:\tlearn: 0.1568293\ttotal: 11.2s\tremaining: 2.6s\n",
      "811:\tlearn: 0.1567761\ttotal: 11.2s\tremaining: 2.59s\n",
      "812:\tlearn: 0.1567397\ttotal: 11.2s\tremaining: 2.57s\n",
      "813:\tlearn: 0.1566800\ttotal: 11.2s\tremaining: 2.56s\n",
      "814:\tlearn: 0.1566053\ttotal: 11.2s\tremaining: 2.55s\n",
      "815:\tlearn: 0.1565403\ttotal: 11.2s\tremaining: 2.53s\n",
      "816:\tlearn: 0.1564803\ttotal: 11.2s\tremaining: 2.52s\n",
      "817:\tlearn: 0.1564581\ttotal: 11.3s\tremaining: 2.5s\n",
      "818:\tlearn: 0.1564126\ttotal: 11.3s\tremaining: 2.49s\n",
      "819:\tlearn: 0.1563608\ttotal: 11.3s\tremaining: 2.48s\n",
      "820:\tlearn: 0.1563195\ttotal: 11.3s\tremaining: 2.46s\n",
      "821:\tlearn: 0.1562727\ttotal: 11.3s\tremaining: 2.45s\n",
      "822:\tlearn: 0.1562021\ttotal: 11.3s\tremaining: 2.44s\n",
      "823:\tlearn: 0.1561610\ttotal: 11.3s\tremaining: 2.42s\n",
      "824:\tlearn: 0.1560910\ttotal: 11.4s\tremaining: 2.41s\n",
      "825:\tlearn: 0.1560546\ttotal: 11.4s\tremaining: 2.4s\n",
      "826:\tlearn: 0.1560310\ttotal: 11.4s\tremaining: 2.38s\n",
      "827:\tlearn: 0.1559960\ttotal: 11.4s\tremaining: 2.37s\n",
      "828:\tlearn: 0.1559629\ttotal: 11.4s\tremaining: 2.35s\n",
      "829:\tlearn: 0.1559161\ttotal: 11.4s\tremaining: 2.34s\n",
      "830:\tlearn: 0.1558952\ttotal: 11.4s\tremaining: 2.33s\n",
      "831:\tlearn: 0.1558635\ttotal: 11.5s\tremaining: 2.31s\n",
      "832:\tlearn: 0.1557904\ttotal: 11.5s\tremaining: 2.3s\n",
      "833:\tlearn: 0.1557586\ttotal: 11.5s\tremaining: 2.29s\n",
      "834:\tlearn: 0.1557283\ttotal: 11.5s\tremaining: 2.27s\n",
      "835:\tlearn: 0.1556908\ttotal: 11.5s\tremaining: 2.26s\n",
      "836:\tlearn: 0.1556732\ttotal: 11.5s\tremaining: 2.24s\n",
      "837:\tlearn: 0.1556369\ttotal: 11.5s\tremaining: 2.23s\n",
      "838:\tlearn: 0.1555601\ttotal: 11.6s\tremaining: 2.22s\n",
      "839:\tlearn: 0.1555175\ttotal: 11.6s\tremaining: 2.2s\n",
      "840:\tlearn: 0.1554715\ttotal: 11.6s\tremaining: 2.19s\n",
      "841:\tlearn: 0.1554417\ttotal: 11.6s\tremaining: 2.17s\n",
      "842:\tlearn: 0.1554152\ttotal: 11.6s\tremaining: 2.16s\n",
      "843:\tlearn: 0.1553601\ttotal: 11.6s\tremaining: 2.15s\n",
      "844:\tlearn: 0.1553330\ttotal: 11.6s\tremaining: 2.13s\n",
      "845:\tlearn: 0.1552653\ttotal: 11.6s\tremaining: 2.12s\n",
      "846:\tlearn: 0.1552286\ttotal: 11.7s\tremaining: 2.11s\n",
      "847:\tlearn: 0.1552065\ttotal: 11.7s\tremaining: 2.09s\n",
      "848:\tlearn: 0.1551628\ttotal: 11.7s\tremaining: 2.08s\n",
      "849:\tlearn: 0.1550904\ttotal: 11.7s\tremaining: 2.07s\n",
      "850:\tlearn: 0.1550277\ttotal: 11.7s\tremaining: 2.05s\n",
      "851:\tlearn: 0.1549917\ttotal: 11.7s\tremaining: 2.04s\n",
      "852:\tlearn: 0.1549516\ttotal: 11.8s\tremaining: 2.02s\n",
      "853:\tlearn: 0.1548821\ttotal: 11.8s\tremaining: 2.01s\n",
      "854:\tlearn: 0.1548269\ttotal: 11.8s\tremaining: 2s\n",
      "855:\tlearn: 0.1548059\ttotal: 11.8s\tremaining: 1.98s\n",
      "856:\tlearn: 0.1547588\ttotal: 11.8s\tremaining: 1.97s\n",
      "857:\tlearn: 0.1547225\ttotal: 11.8s\tremaining: 1.96s\n",
      "858:\tlearn: 0.1546826\ttotal: 11.8s\tremaining: 1.94s\n",
      "859:\tlearn: 0.1546437\ttotal: 11.8s\tremaining: 1.93s\n",
      "860:\tlearn: 0.1545903\ttotal: 11.9s\tremaining: 1.92s\n",
      "861:\tlearn: 0.1545431\ttotal: 11.9s\tremaining: 1.9s\n",
      "862:\tlearn: 0.1545185\ttotal: 11.9s\tremaining: 1.89s\n",
      "863:\tlearn: 0.1544535\ttotal: 11.9s\tremaining: 1.87s\n",
      "864:\tlearn: 0.1544407\ttotal: 11.9s\tremaining: 1.86s\n",
      "865:\tlearn: 0.1544130\ttotal: 11.9s\tremaining: 1.84s\n",
      "866:\tlearn: 0.1543862\ttotal: 11.9s\tremaining: 1.83s\n",
      "867:\tlearn: 0.1543427\ttotal: 12s\tremaining: 1.82s\n",
      "868:\tlearn: 0.1542757\ttotal: 12s\tremaining: 1.8s\n",
      "869:\tlearn: 0.1542253\ttotal: 12s\tremaining: 1.79s\n",
      "870:\tlearn: 0.1541846\ttotal: 12s\tremaining: 1.78s\n",
      "871:\tlearn: 0.1541522\ttotal: 12s\tremaining: 1.76s\n",
      "872:\tlearn: 0.1541053\ttotal: 12s\tremaining: 1.75s\n",
      "873:\tlearn: 0.1540060\ttotal: 12s\tremaining: 1.74s\n",
      "874:\tlearn: 0.1539661\ttotal: 12.1s\tremaining: 1.72s\n",
      "875:\tlearn: 0.1539057\ttotal: 12.1s\tremaining: 1.71s\n",
      "876:\tlearn: 0.1538659\ttotal: 12.1s\tremaining: 1.69s\n",
      "877:\tlearn: 0.1537758\ttotal: 12.1s\tremaining: 1.68s\n",
      "878:\tlearn: 0.1537445\ttotal: 12.1s\tremaining: 1.67s\n",
      "879:\tlearn: 0.1536689\ttotal: 12.1s\tremaining: 1.65s\n",
      "880:\tlearn: 0.1536296\ttotal: 12.1s\tremaining: 1.64s\n",
      "881:\tlearn: 0.1536015\ttotal: 12.1s\tremaining: 1.62s\n",
      "882:\tlearn: 0.1535450\ttotal: 12.2s\tremaining: 1.61s\n",
      "883:\tlearn: 0.1535292\ttotal: 12.2s\tremaining: 1.6s\n",
      "884:\tlearn: 0.1535036\ttotal: 12.2s\tremaining: 1.58s\n",
      "885:\tlearn: 0.1534702\ttotal: 12.2s\tremaining: 1.57s\n",
      "886:\tlearn: 0.1533934\ttotal: 12.2s\tremaining: 1.55s\n",
      "887:\tlearn: 0.1533613\ttotal: 12.2s\tremaining: 1.54s\n",
      "888:\tlearn: 0.1533335\ttotal: 12.2s\tremaining: 1.53s\n",
      "889:\tlearn: 0.1532864\ttotal: 12.2s\tremaining: 1.51s\n",
      "890:\tlearn: 0.1532536\ttotal: 12.3s\tremaining: 1.5s\n",
      "891:\tlearn: 0.1532336\ttotal: 12.3s\tremaining: 1.49s\n",
      "892:\tlearn: 0.1531923\ttotal: 12.3s\tremaining: 1.47s\n",
      "893:\tlearn: 0.1531659\ttotal: 12.3s\tremaining: 1.46s\n",
      "894:\tlearn: 0.1531014\ttotal: 12.3s\tremaining: 1.44s\n",
      "895:\tlearn: 0.1530564\ttotal: 12.3s\tremaining: 1.43s\n",
      "896:\tlearn: 0.1530016\ttotal: 12.3s\tremaining: 1.42s\n",
      "897:\tlearn: 0.1529523\ttotal: 12.3s\tremaining: 1.4s\n",
      "898:\tlearn: 0.1529163\ttotal: 12.4s\tremaining: 1.39s\n",
      "899:\tlearn: 0.1528560\ttotal: 12.4s\tremaining: 1.38s\n",
      "900:\tlearn: 0.1528269\ttotal: 12.4s\tremaining: 1.36s\n",
      "901:\tlearn: 0.1527707\ttotal: 12.4s\tremaining: 1.35s\n",
      "902:\tlearn: 0.1527137\ttotal: 12.4s\tremaining: 1.33s\n",
      "903:\tlearn: 0.1526830\ttotal: 12.4s\tremaining: 1.32s\n",
      "904:\tlearn: 0.1525983\ttotal: 12.4s\tremaining: 1.31s\n",
      "905:\tlearn: 0.1525588\ttotal: 12.5s\tremaining: 1.29s\n",
      "906:\tlearn: 0.1525307\ttotal: 12.5s\tremaining: 1.28s\n",
      "907:\tlearn: 0.1524988\ttotal: 12.5s\tremaining: 1.26s\n",
      "908:\tlearn: 0.1524118\ttotal: 12.5s\tremaining: 1.25s\n",
      "909:\tlearn: 0.1523768\ttotal: 12.5s\tremaining: 1.24s\n",
      "910:\tlearn: 0.1523432\ttotal: 12.5s\tremaining: 1.22s\n",
      "911:\tlearn: 0.1523209\ttotal: 12.5s\tremaining: 1.21s\n",
      "912:\tlearn: 0.1522684\ttotal: 12.6s\tremaining: 1.2s\n",
      "913:\tlearn: 0.1522264\ttotal: 12.6s\tremaining: 1.18s\n",
      "914:\tlearn: 0.1521834\ttotal: 12.6s\tremaining: 1.17s\n",
      "915:\tlearn: 0.1521455\ttotal: 12.6s\tremaining: 1.16s\n",
      "916:\tlearn: 0.1521006\ttotal: 12.6s\tremaining: 1.14s\n",
      "917:\tlearn: 0.1520825\ttotal: 12.6s\tremaining: 1.13s\n",
      "918:\tlearn: 0.1520336\ttotal: 12.6s\tremaining: 1.11s\n",
      "919:\tlearn: 0.1519978\ttotal: 12.7s\tremaining: 1.1s\n",
      "920:\tlearn: 0.1519648\ttotal: 12.7s\tremaining: 1.09s\n",
      "921:\tlearn: 0.1519263\ttotal: 12.7s\tremaining: 1.07s\n",
      "922:\tlearn: 0.1518622\ttotal: 12.7s\tremaining: 1.06s\n",
      "923:\tlearn: 0.1518234\ttotal: 12.7s\tremaining: 1.04s\n",
      "924:\tlearn: 0.1517890\ttotal: 12.7s\tremaining: 1.03s\n",
      "925:\tlearn: 0.1517567\ttotal: 12.7s\tremaining: 1.02s\n",
      "926:\tlearn: 0.1517151\ttotal: 12.7s\tremaining: 1s\n",
      "927:\tlearn: 0.1516675\ttotal: 12.8s\tremaining: 990ms\n",
      "928:\tlearn: 0.1516176\ttotal: 12.8s\tremaining: 976ms\n",
      "929:\tlearn: 0.1515586\ttotal: 12.8s\tremaining: 962ms\n",
      "930:\tlearn: 0.1515376\ttotal: 12.8s\tremaining: 949ms\n",
      "931:\tlearn: 0.1514994\ttotal: 12.8s\tremaining: 935ms\n",
      "932:\tlearn: 0.1514587\ttotal: 12.8s\tremaining: 921ms\n",
      "933:\tlearn: 0.1514076\ttotal: 12.8s\tremaining: 907ms\n",
      "934:\tlearn: 0.1513472\ttotal: 12.9s\tremaining: 894ms\n",
      "935:\tlearn: 0.1513157\ttotal: 12.9s\tremaining: 880ms\n",
      "936:\tlearn: 0.1512637\ttotal: 12.9s\tremaining: 866ms\n",
      "937:\tlearn: 0.1512398\ttotal: 12.9s\tremaining: 852ms\n",
      "938:\tlearn: 0.1511975\ttotal: 12.9s\tremaining: 838ms\n",
      "939:\tlearn: 0.1511520\ttotal: 12.9s\tremaining: 825ms\n",
      "940:\tlearn: 0.1511068\ttotal: 12.9s\tremaining: 811ms\n",
      "941:\tlearn: 0.1510813\ttotal: 12.9s\tremaining: 797ms\n",
      "942:\tlearn: 0.1510527\ttotal: 13s\tremaining: 783ms\n",
      "943:\tlearn: 0.1510272\ttotal: 13s\tremaining: 770ms\n",
      "944:\tlearn: 0.1509940\ttotal: 13s\tremaining: 756ms\n",
      "945:\tlearn: 0.1509508\ttotal: 13s\tremaining: 742ms\n",
      "946:\tlearn: 0.1509093\ttotal: 13s\tremaining: 728ms\n",
      "947:\tlearn: 0.1508644\ttotal: 13s\tremaining: 715ms\n",
      "948:\tlearn: 0.1508251\ttotal: 13s\tremaining: 701ms\n",
      "949:\tlearn: 0.1507927\ttotal: 13.1s\tremaining: 687ms\n",
      "950:\tlearn: 0.1507390\ttotal: 13.1s\tremaining: 673ms\n",
      "951:\tlearn: 0.1506880\ttotal: 13.1s\tremaining: 660ms\n",
      "952:\tlearn: 0.1506310\ttotal: 13.1s\tremaining: 646ms\n",
      "953:\tlearn: 0.1506033\ttotal: 13.1s\tremaining: 632ms\n",
      "954:\tlearn: 0.1505697\ttotal: 13.1s\tremaining: 619ms\n",
      "955:\tlearn: 0.1505344\ttotal: 13.1s\tremaining: 605ms\n",
      "956:\tlearn: 0.1504895\ttotal: 13.2s\tremaining: 591ms\n",
      "957:\tlearn: 0.1504493\ttotal: 13.2s\tremaining: 577ms\n",
      "958:\tlearn: 0.1504317\ttotal: 13.2s\tremaining: 563ms\n",
      "959:\tlearn: 0.1503996\ttotal: 13.2s\tremaining: 550ms\n",
      "960:\tlearn: 0.1503714\ttotal: 13.2s\tremaining: 536ms\n",
      "961:\tlearn: 0.1503149\ttotal: 13.2s\tremaining: 523ms\n",
      "962:\tlearn: 0.1502636\ttotal: 13.2s\tremaining: 509ms\n",
      "963:\tlearn: 0.1502418\ttotal: 13.3s\tremaining: 495ms\n",
      "964:\tlearn: 0.1501841\ttotal: 13.3s\tremaining: 481ms\n",
      "965:\tlearn: 0.1501511\ttotal: 13.3s\tremaining: 468ms\n",
      "966:\tlearn: 0.1501283\ttotal: 13.3s\tremaining: 454ms\n",
      "967:\tlearn: 0.1500929\ttotal: 13.3s\tremaining: 440ms\n",
      "968:\tlearn: 0.1500598\ttotal: 13.3s\tremaining: 426ms\n",
      "969:\tlearn: 0.1500229\ttotal: 13.3s\tremaining: 412ms\n",
      "970:\tlearn: 0.1499759\ttotal: 13.3s\tremaining: 399ms\n",
      "971:\tlearn: 0.1499251\ttotal: 13.4s\tremaining: 385ms\n",
      "972:\tlearn: 0.1498959\ttotal: 13.4s\tremaining: 371ms\n",
      "973:\tlearn: 0.1498841\ttotal: 13.4s\tremaining: 357ms\n",
      "974:\tlearn: 0.1498097\ttotal: 13.4s\tremaining: 344ms\n",
      "975:\tlearn: 0.1497623\ttotal: 13.4s\tremaining: 330ms\n",
      "976:\tlearn: 0.1497219\ttotal: 13.4s\tremaining: 316ms\n",
      "977:\tlearn: 0.1496616\ttotal: 13.4s\tremaining: 302ms\n",
      "978:\tlearn: 0.1496116\ttotal: 13.5s\tremaining: 289ms\n",
      "979:\tlearn: 0.1495868\ttotal: 13.5s\tremaining: 275ms\n",
      "980:\tlearn: 0.1495667\ttotal: 13.5s\tremaining: 261ms\n",
      "981:\tlearn: 0.1495258\ttotal: 13.5s\tremaining: 247ms\n",
      "982:\tlearn: 0.1494946\ttotal: 13.5s\tremaining: 234ms\n",
      "983:\tlearn: 0.1494349\ttotal: 13.5s\tremaining: 220ms\n",
      "984:\tlearn: 0.1494034\ttotal: 13.5s\tremaining: 206ms\n",
      "985:\tlearn: 0.1493541\ttotal: 13.6s\tremaining: 192ms\n",
      "986:\tlearn: 0.1493191\ttotal: 13.6s\tremaining: 179ms\n",
      "987:\tlearn: 0.1492824\ttotal: 13.6s\tremaining: 165ms\n",
      "988:\tlearn: 0.1492422\ttotal: 13.6s\tremaining: 151ms\n",
      "989:\tlearn: 0.1492157\ttotal: 13.6s\tremaining: 137ms\n",
      "990:\tlearn: 0.1491618\ttotal: 13.6s\tremaining: 124ms\n",
      "991:\tlearn: 0.1491158\ttotal: 13.7s\tremaining: 110ms\n",
      "992:\tlearn: 0.1489915\ttotal: 13.7s\tremaining: 96.5ms\n",
      "993:\tlearn: 0.1489883\ttotal: 13.7s\tremaining: 82.7ms\n",
      "994:\tlearn: 0.1489408\ttotal: 13.7s\tremaining: 69ms\n",
      "995:\tlearn: 0.1488900\ttotal: 13.7s\tremaining: 55.2ms\n",
      "996:\tlearn: 0.1488132\ttotal: 13.8s\tremaining: 41.4ms\n",
      "997:\tlearn: 0.1487917\ttotal: 13.8s\tremaining: 27.6ms\n",
      "998:\tlearn: 0.1487596\ttotal: 13.8s\tremaining: 13.8ms\n",
      "999:\tlearn: 0.1487199\ttotal: 13.8s\tremaining: 0us\n",
      "Learning rate set to 0.070523\n",
      "0:\tlearn: 0.6536654\ttotal: 24.7ms\tremaining: 24.7s\n",
      "1:\tlearn: 0.6335791\ttotal: 38.3ms\tremaining: 19.1s\n",
      "2:\tlearn: 0.6041262\ttotal: 52.9ms\tremaining: 17.6s\n",
      "3:\tlearn: 0.5631476\ttotal: 67.5ms\tremaining: 16.8s\n",
      "4:\tlearn: 0.5454604\ttotal: 82.2ms\tremaining: 16.4s\n",
      "5:\tlearn: 0.5297447\ttotal: 96ms\tremaining: 15.9s\n",
      "6:\tlearn: 0.5102351\ttotal: 110ms\tremaining: 15.6s\n",
      "7:\tlearn: 0.4992789\ttotal: 123ms\tremaining: 15.3s\n",
      "8:\tlearn: 0.4900841\ttotal: 137ms\tremaining: 15.1s\n",
      "9:\tlearn: 0.4829790\ttotal: 151ms\tremaining: 14.9s\n",
      "10:\tlearn: 0.4778079\ttotal: 163ms\tremaining: 14.7s\n",
      "11:\tlearn: 0.4731641\ttotal: 177ms\tremaining: 14.6s\n",
      "12:\tlearn: 0.4679910\ttotal: 191ms\tremaining: 14.5s\n",
      "13:\tlearn: 0.4475022\ttotal: 204ms\tremaining: 14.4s\n",
      "14:\tlearn: 0.4391895\ttotal: 219ms\tremaining: 14.4s\n",
      "15:\tlearn: 0.4343361\ttotal: 233ms\tremaining: 14.3s\n",
      "16:\tlearn: 0.4310595\ttotal: 249ms\tremaining: 14.4s\n",
      "17:\tlearn: 0.4271202\ttotal: 284ms\tremaining: 15.5s\n",
      "18:\tlearn: 0.4234238\ttotal: 304ms\tremaining: 15.7s\n",
      "19:\tlearn: 0.4160777\ttotal: 319ms\tremaining: 15.6s\n",
      "20:\tlearn: 0.4125739\ttotal: 333ms\tremaining: 15.5s\n",
      "21:\tlearn: 0.4104695\ttotal: 346ms\tremaining: 15.4s\n",
      "22:\tlearn: 0.4062023\ttotal: 359ms\tremaining: 15.3s\n",
      "23:\tlearn: 0.3991117\ttotal: 373ms\tremaining: 15.2s\n",
      "24:\tlearn: 0.3966888\ttotal: 398ms\tremaining: 15.5s\n",
      "25:\tlearn: 0.3943333\ttotal: 412ms\tremaining: 15.4s\n",
      "26:\tlearn: 0.3892561\ttotal: 427ms\tremaining: 15.4s\n",
      "27:\tlearn: 0.3834875\ttotal: 442ms\tremaining: 15.3s\n",
      "28:\tlearn: 0.3775255\ttotal: 457ms\tremaining: 15.3s\n",
      "29:\tlearn: 0.3735124\ttotal: 473ms\tremaining: 15.3s\n",
      "30:\tlearn: 0.3674538\ttotal: 485ms\tremaining: 15.2s\n",
      "31:\tlearn: 0.3630371\ttotal: 497ms\tremaining: 15s\n",
      "32:\tlearn: 0.3612184\ttotal: 511ms\tremaining: 15s\n",
      "33:\tlearn: 0.3592206\ttotal: 523ms\tremaining: 14.9s\n",
      "34:\tlearn: 0.3552535\ttotal: 535ms\tremaining: 14.7s\n",
      "35:\tlearn: 0.3455154\ttotal: 549ms\tremaining: 14.7s\n",
      "36:\tlearn: 0.3412198\ttotal: 562ms\tremaining: 14.6s\n",
      "37:\tlearn: 0.3351928\ttotal: 576ms\tremaining: 14.6s\n",
      "38:\tlearn: 0.3332415\ttotal: 589ms\tremaining: 14.5s\n",
      "39:\tlearn: 0.3271910\ttotal: 603ms\tremaining: 14.5s\n",
      "40:\tlearn: 0.3229597\ttotal: 615ms\tremaining: 14.4s\n",
      "41:\tlearn: 0.3216216\ttotal: 630ms\tremaining: 14.4s\n",
      "42:\tlearn: 0.3191214\ttotal: 644ms\tremaining: 14.3s\n",
      "43:\tlearn: 0.3176166\ttotal: 657ms\tremaining: 14.3s\n",
      "44:\tlearn: 0.3158657\ttotal: 669ms\tremaining: 14.2s\n",
      "45:\tlearn: 0.3146760\ttotal: 695ms\tremaining: 14.4s\n",
      "46:\tlearn: 0.3098887\ttotal: 726ms\tremaining: 14.7s\n",
      "47:\tlearn: 0.3081774\ttotal: 741ms\tremaining: 14.7s\n",
      "48:\tlearn: 0.3065135\ttotal: 760ms\tremaining: 14.7s\n",
      "49:\tlearn: 0.3053901\ttotal: 771ms\tremaining: 14.6s\n",
      "50:\tlearn: 0.3047186\ttotal: 785ms\tremaining: 14.6s\n",
      "51:\tlearn: 0.3019775\ttotal: 800ms\tremaining: 14.6s\n",
      "52:\tlearn: 0.2974658\ttotal: 813ms\tremaining: 14.5s\n",
      "53:\tlearn: 0.2954658\ttotal: 826ms\tremaining: 14.5s\n",
      "54:\tlearn: 0.2935624\ttotal: 843ms\tremaining: 14.5s\n",
      "55:\tlearn: 0.2918382\ttotal: 857ms\tremaining: 14.4s\n",
      "56:\tlearn: 0.2910642\ttotal: 869ms\tremaining: 14.4s\n",
      "57:\tlearn: 0.2889759\ttotal: 882ms\tremaining: 14.3s\n",
      "58:\tlearn: 0.2852298\ttotal: 896ms\tremaining: 14.3s\n",
      "59:\tlearn: 0.2843917\ttotal: 909ms\tremaining: 14.2s\n",
      "60:\tlearn: 0.2825954\ttotal: 922ms\tremaining: 14.2s\n",
      "61:\tlearn: 0.2811535\ttotal: 935ms\tremaining: 14.1s\n",
      "62:\tlearn: 0.2797126\ttotal: 948ms\tremaining: 14.1s\n",
      "63:\tlearn: 0.2791708\ttotal: 961ms\tremaining: 14.1s\n",
      "64:\tlearn: 0.2775361\ttotal: 974ms\tremaining: 14s\n",
      "65:\tlearn: 0.2765103\ttotal: 988ms\tremaining: 14s\n",
      "66:\tlearn: 0.2757932\ttotal: 1s\tremaining: 14s\n",
      "67:\tlearn: 0.2735134\ttotal: 1.01s\tremaining: 13.9s\n",
      "68:\tlearn: 0.2714215\ttotal: 1.03s\tremaining: 13.9s\n",
      "69:\tlearn: 0.2708789\ttotal: 1.04s\tremaining: 13.9s\n",
      "70:\tlearn: 0.2686726\ttotal: 1.06s\tremaining: 13.8s\n",
      "71:\tlearn: 0.2673658\ttotal: 1.07s\tremaining: 13.8s\n",
      "72:\tlearn: 0.2657976\ttotal: 1.08s\tremaining: 13.8s\n",
      "73:\tlearn: 0.2646955\ttotal: 1.1s\tremaining: 13.7s\n",
      "74:\tlearn: 0.2621020\ttotal: 1.11s\tremaining: 13.7s\n",
      "75:\tlearn: 0.2612138\ttotal: 1.12s\tremaining: 13.7s\n",
      "76:\tlearn: 0.2609128\ttotal: 1.14s\tremaining: 13.6s\n",
      "77:\tlearn: 0.2587290\ttotal: 1.15s\tremaining: 13.6s\n",
      "78:\tlearn: 0.2584706\ttotal: 1.16s\tremaining: 13.6s\n",
      "79:\tlearn: 0.2581054\ttotal: 1.18s\tremaining: 13.5s\n",
      "80:\tlearn: 0.2573235\ttotal: 1.19s\tremaining: 13.5s\n",
      "81:\tlearn: 0.2556211\ttotal: 1.2s\tremaining: 13.5s\n",
      "82:\tlearn: 0.2543325\ttotal: 1.22s\tremaining: 13.5s\n",
      "83:\tlearn: 0.2533160\ttotal: 1.23s\tremaining: 13.5s\n",
      "84:\tlearn: 0.2529203\ttotal: 1.25s\tremaining: 13.5s\n",
      "85:\tlearn: 0.2522923\ttotal: 1.27s\tremaining: 13.5s\n",
      "86:\tlearn: 0.2516178\ttotal: 1.28s\tremaining: 13.4s\n",
      "87:\tlearn: 0.2507043\ttotal: 1.29s\tremaining: 13.4s\n",
      "88:\tlearn: 0.2501441\ttotal: 1.31s\tremaining: 13.4s\n",
      "89:\tlearn: 0.2499407\ttotal: 1.32s\tremaining: 13.4s\n",
      "90:\tlearn: 0.2491754\ttotal: 1.33s\tremaining: 13.3s\n",
      "91:\tlearn: 0.2487151\ttotal: 1.35s\tremaining: 13.3s\n",
      "92:\tlearn: 0.2480735\ttotal: 1.36s\tremaining: 13.3s\n",
      "93:\tlearn: 0.2478862\ttotal: 1.38s\tremaining: 13.3s\n",
      "94:\tlearn: 0.2472329\ttotal: 1.39s\tremaining: 13.2s\n",
      "95:\tlearn: 0.2466692\ttotal: 1.4s\tremaining: 13.2s\n",
      "96:\tlearn: 0.2464287\ttotal: 1.42s\tremaining: 13.2s\n",
      "97:\tlearn: 0.2460500\ttotal: 1.43s\tremaining: 13.2s\n",
      "98:\tlearn: 0.2453957\ttotal: 1.45s\tremaining: 13.2s\n",
      "99:\tlearn: 0.2451519\ttotal: 1.46s\tremaining: 13.1s\n",
      "100:\tlearn: 0.2445092\ttotal: 1.47s\tremaining: 13.1s\n",
      "101:\tlearn: 0.2439245\ttotal: 1.48s\tremaining: 13.1s\n",
      "102:\tlearn: 0.2436390\ttotal: 1.5s\tremaining: 13s\n",
      "103:\tlearn: 0.2431516\ttotal: 1.51s\tremaining: 13s\n",
      "104:\tlearn: 0.2428041\ttotal: 1.52s\tremaining: 13s\n",
      "105:\tlearn: 0.2426173\ttotal: 1.53s\tremaining: 12.9s\n",
      "106:\tlearn: 0.2423694\ttotal: 1.55s\tremaining: 12.9s\n",
      "107:\tlearn: 0.2419894\ttotal: 1.56s\tremaining: 12.9s\n",
      "108:\tlearn: 0.2405238\ttotal: 1.57s\tremaining: 12.9s\n",
      "109:\tlearn: 0.2396964\ttotal: 1.59s\tremaining: 12.8s\n",
      "110:\tlearn: 0.2393625\ttotal: 1.6s\tremaining: 12.8s\n",
      "111:\tlearn: 0.2390971\ttotal: 1.61s\tremaining: 12.8s\n",
      "112:\tlearn: 0.2386594\ttotal: 1.63s\tremaining: 12.8s\n",
      "113:\tlearn: 0.2376764\ttotal: 1.64s\tremaining: 12.8s\n",
      "114:\tlearn: 0.2368128\ttotal: 1.66s\tremaining: 12.7s\n",
      "115:\tlearn: 0.2358193\ttotal: 1.67s\tremaining: 12.7s\n",
      "116:\tlearn: 0.2354672\ttotal: 1.68s\tremaining: 12.7s\n",
      "117:\tlearn: 0.2349065\ttotal: 1.7s\tremaining: 12.7s\n",
      "118:\tlearn: 0.2345146\ttotal: 1.71s\tremaining: 12.7s\n",
      "119:\tlearn: 0.2343909\ttotal: 1.72s\tremaining: 12.6s\n",
      "120:\tlearn: 0.2339092\ttotal: 1.73s\tremaining: 12.6s\n",
      "121:\tlearn: 0.2333583\ttotal: 1.75s\tremaining: 12.6s\n",
      "122:\tlearn: 0.2328731\ttotal: 1.76s\tremaining: 12.6s\n",
      "123:\tlearn: 0.2326753\ttotal: 1.77s\tremaining: 12.5s\n",
      "124:\tlearn: 0.2324872\ttotal: 1.79s\tremaining: 12.5s\n",
      "125:\tlearn: 0.2318578\ttotal: 1.8s\tremaining: 12.5s\n",
      "126:\tlearn: 0.2314453\ttotal: 1.81s\tremaining: 12.5s\n",
      "127:\tlearn: 0.2304142\ttotal: 1.83s\tremaining: 12.5s\n",
      "128:\tlearn: 0.2302727\ttotal: 1.84s\tremaining: 12.5s\n",
      "129:\tlearn: 0.2298667\ttotal: 1.86s\tremaining: 12.5s\n",
      "130:\tlearn: 0.2294158\ttotal: 1.87s\tremaining: 12.4s\n",
      "131:\tlearn: 0.2288512\ttotal: 1.89s\tremaining: 12.4s\n",
      "132:\tlearn: 0.2284122\ttotal: 1.9s\tremaining: 12.4s\n",
      "133:\tlearn: 0.2280525\ttotal: 1.92s\tremaining: 12.4s\n",
      "134:\tlearn: 0.2277483\ttotal: 1.93s\tremaining: 12.4s\n",
      "135:\tlearn: 0.2275084\ttotal: 1.94s\tremaining: 12.3s\n",
      "136:\tlearn: 0.2273470\ttotal: 1.95s\tremaining: 12.3s\n",
      "137:\tlearn: 0.2270413\ttotal: 1.97s\tremaining: 12.3s\n",
      "138:\tlearn: 0.2268831\ttotal: 1.98s\tremaining: 12.3s\n",
      "139:\tlearn: 0.2264520\ttotal: 2s\tremaining: 12.3s\n",
      "140:\tlearn: 0.2263142\ttotal: 2.01s\tremaining: 12.2s\n",
      "141:\tlearn: 0.2259582\ttotal: 2.02s\tremaining: 12.2s\n",
      "142:\tlearn: 0.2255016\ttotal: 2.04s\tremaining: 12.2s\n",
      "143:\tlearn: 0.2253025\ttotal: 2.05s\tremaining: 12.2s\n",
      "144:\tlearn: 0.2248578\ttotal: 2.06s\tremaining: 12.2s\n",
      "145:\tlearn: 0.2247037\ttotal: 2.08s\tremaining: 12.2s\n",
      "146:\tlearn: 0.2241456\ttotal: 2.09s\tremaining: 12.1s\n",
      "147:\tlearn: 0.2237425\ttotal: 2.1s\tremaining: 12.1s\n",
      "148:\tlearn: 0.2234557\ttotal: 2.11s\tremaining: 12.1s\n",
      "149:\tlearn: 0.2231279\ttotal: 2.13s\tremaining: 12.1s\n",
      "150:\tlearn: 0.2230355\ttotal: 2.14s\tremaining: 12s\n",
      "151:\tlearn: 0.2229223\ttotal: 2.15s\tremaining: 12s\n",
      "152:\tlearn: 0.2226322\ttotal: 2.17s\tremaining: 12s\n",
      "153:\tlearn: 0.2224001\ttotal: 2.18s\tremaining: 12s\n",
      "154:\tlearn: 0.2217688\ttotal: 2.19s\tremaining: 12s\n",
      "155:\tlearn: 0.2212451\ttotal: 2.21s\tremaining: 11.9s\n",
      "156:\tlearn: 0.2201706\ttotal: 2.22s\tremaining: 11.9s\n",
      "157:\tlearn: 0.2198444\ttotal: 2.24s\tremaining: 11.9s\n",
      "158:\tlearn: 0.2194753\ttotal: 2.25s\tremaining: 11.9s\n",
      "159:\tlearn: 0.2189357\ttotal: 2.27s\tremaining: 11.9s\n",
      "160:\tlearn: 0.2184901\ttotal: 2.28s\tremaining: 11.9s\n",
      "161:\tlearn: 0.2180556\ttotal: 2.3s\tremaining: 11.9s\n",
      "162:\tlearn: 0.2179278\ttotal: 2.31s\tremaining: 11.9s\n",
      "163:\tlearn: 0.2177676\ttotal: 2.32s\tremaining: 11.8s\n",
      "164:\tlearn: 0.2173008\ttotal: 2.34s\tremaining: 11.8s\n",
      "165:\tlearn: 0.2169464\ttotal: 2.35s\tremaining: 11.8s\n",
      "166:\tlearn: 0.2166151\ttotal: 2.36s\tremaining: 11.8s\n",
      "167:\tlearn: 0.2159858\ttotal: 2.38s\tremaining: 11.8s\n",
      "168:\tlearn: 0.2155282\ttotal: 2.39s\tremaining: 11.8s\n",
      "169:\tlearn: 0.2153747\ttotal: 2.4s\tremaining: 11.7s\n",
      "170:\tlearn: 0.2151600\ttotal: 2.42s\tremaining: 11.7s\n",
      "171:\tlearn: 0.2149249\ttotal: 2.43s\tremaining: 11.7s\n",
      "172:\tlearn: 0.2145707\ttotal: 2.44s\tremaining: 11.7s\n",
      "173:\tlearn: 0.2144702\ttotal: 2.46s\tremaining: 11.7s\n",
      "174:\tlearn: 0.2143269\ttotal: 2.48s\tremaining: 11.7s\n",
      "175:\tlearn: 0.2140338\ttotal: 2.49s\tremaining: 11.7s\n",
      "176:\tlearn: 0.2137204\ttotal: 2.51s\tremaining: 11.7s\n",
      "177:\tlearn: 0.2132899\ttotal: 2.52s\tremaining: 11.7s\n",
      "178:\tlearn: 0.2127995\ttotal: 2.54s\tremaining: 11.6s\n",
      "179:\tlearn: 0.2124974\ttotal: 2.55s\tremaining: 11.6s\n",
      "180:\tlearn: 0.2121248\ttotal: 2.56s\tremaining: 11.6s\n",
      "181:\tlearn: 0.2120129\ttotal: 2.58s\tremaining: 11.6s\n",
      "182:\tlearn: 0.2117451\ttotal: 2.59s\tremaining: 11.6s\n",
      "183:\tlearn: 0.2115190\ttotal: 2.6s\tremaining: 11.6s\n",
      "184:\tlearn: 0.2113082\ttotal: 2.62s\tremaining: 11.5s\n",
      "185:\tlearn: 0.2111235\ttotal: 2.63s\tremaining: 11.5s\n",
      "186:\tlearn: 0.2110170\ttotal: 2.64s\tremaining: 11.5s\n",
      "187:\tlearn: 0.2108577\ttotal: 2.66s\tremaining: 11.5s\n",
      "188:\tlearn: 0.2107482\ttotal: 2.67s\tremaining: 11.5s\n",
      "189:\tlearn: 0.2105790\ttotal: 2.69s\tremaining: 11.4s\n",
      "190:\tlearn: 0.2100462\ttotal: 2.7s\tremaining: 11.4s\n",
      "191:\tlearn: 0.2098000\ttotal: 2.71s\tremaining: 11.4s\n",
      "192:\tlearn: 0.2095031\ttotal: 2.72s\tremaining: 11.4s\n",
      "193:\tlearn: 0.2092613\ttotal: 2.74s\tremaining: 11.4s\n",
      "194:\tlearn: 0.2089509\ttotal: 2.75s\tremaining: 11.3s\n",
      "195:\tlearn: 0.2086555\ttotal: 2.76s\tremaining: 11.3s\n",
      "196:\tlearn: 0.2084897\ttotal: 2.78s\tremaining: 11.3s\n",
      "197:\tlearn: 0.2083043\ttotal: 2.79s\tremaining: 11.3s\n",
      "198:\tlearn: 0.2081856\ttotal: 2.8s\tremaining: 11.3s\n",
      "199:\tlearn: 0.2080705\ttotal: 2.82s\tremaining: 11.3s\n",
      "200:\tlearn: 0.2077764\ttotal: 2.83s\tremaining: 11.3s\n",
      "201:\tlearn: 0.2076116\ttotal: 2.85s\tremaining: 11.3s\n",
      "202:\tlearn: 0.2073957\ttotal: 2.86s\tremaining: 11.2s\n",
      "203:\tlearn: 0.2072255\ttotal: 2.88s\tremaining: 11.2s\n",
      "204:\tlearn: 0.2069508\ttotal: 2.89s\tremaining: 11.2s\n",
      "205:\tlearn: 0.2068347\ttotal: 2.9s\tremaining: 11.2s\n",
      "206:\tlearn: 0.2066645\ttotal: 2.92s\tremaining: 11.2s\n",
      "207:\tlearn: 0.2065919\ttotal: 2.93s\tremaining: 11.2s\n",
      "208:\tlearn: 0.2065252\ttotal: 2.94s\tremaining: 11.1s\n",
      "209:\tlearn: 0.2063588\ttotal: 2.96s\tremaining: 11.1s\n",
      "210:\tlearn: 0.2062755\ttotal: 2.97s\tremaining: 11.1s\n",
      "211:\tlearn: 0.2061013\ttotal: 2.98s\tremaining: 11.1s\n",
      "212:\tlearn: 0.2059799\ttotal: 3s\tremaining: 11.1s\n",
      "213:\tlearn: 0.2057714\ttotal: 3.01s\tremaining: 11.1s\n",
      "214:\tlearn: 0.2056448\ttotal: 3.02s\tremaining: 11s\n",
      "215:\tlearn: 0.2052187\ttotal: 3.04s\tremaining: 11s\n",
      "216:\tlearn: 0.2050414\ttotal: 3.05s\tremaining: 11s\n",
      "217:\tlearn: 0.2049004\ttotal: 3.07s\tremaining: 11s\n",
      "218:\tlearn: 0.2048173\ttotal: 3.08s\tremaining: 11s\n",
      "219:\tlearn: 0.2046981\ttotal: 3.09s\tremaining: 11s\n",
      "220:\tlearn: 0.2043335\ttotal: 3.11s\tremaining: 11s\n",
      "221:\tlearn: 0.2040456\ttotal: 3.12s\tremaining: 10.9s\n",
      "222:\tlearn: 0.2039350\ttotal: 3.13s\tremaining: 10.9s\n",
      "223:\tlearn: 0.2038589\ttotal: 3.15s\tremaining: 10.9s\n",
      "224:\tlearn: 0.2034372\ttotal: 3.16s\tremaining: 10.9s\n",
      "225:\tlearn: 0.2033152\ttotal: 3.17s\tremaining: 10.9s\n",
      "226:\tlearn: 0.2032318\ttotal: 3.19s\tremaining: 10.8s\n",
      "227:\tlearn: 0.2031029\ttotal: 3.2s\tremaining: 10.8s\n",
      "228:\tlearn: 0.2030277\ttotal: 3.21s\tremaining: 10.8s\n",
      "229:\tlearn: 0.2025940\ttotal: 3.23s\tremaining: 10.8s\n",
      "230:\tlearn: 0.2025143\ttotal: 3.24s\tremaining: 10.8s\n",
      "231:\tlearn: 0.2022023\ttotal: 3.26s\tremaining: 10.8s\n",
      "232:\tlearn: 0.2021176\ttotal: 3.27s\tremaining: 10.8s\n",
      "233:\tlearn: 0.2019456\ttotal: 3.28s\tremaining: 10.8s\n",
      "234:\tlearn: 0.2018007\ttotal: 3.3s\tremaining: 10.7s\n",
      "235:\tlearn: 0.2016740\ttotal: 3.31s\tremaining: 10.7s\n",
      "236:\tlearn: 0.2012849\ttotal: 3.33s\tremaining: 10.7s\n",
      "237:\tlearn: 0.2011438\ttotal: 3.34s\tremaining: 10.7s\n",
      "238:\tlearn: 0.2008416\ttotal: 3.35s\tremaining: 10.7s\n",
      "239:\tlearn: 0.2006187\ttotal: 3.36s\tremaining: 10.7s\n",
      "240:\tlearn: 0.2005191\ttotal: 3.38s\tremaining: 10.6s\n",
      "241:\tlearn: 0.2003251\ttotal: 3.39s\tremaining: 10.6s\n",
      "242:\tlearn: 0.2001879\ttotal: 3.4s\tremaining: 10.6s\n",
      "243:\tlearn: 0.2001029\ttotal: 3.42s\tremaining: 10.6s\n",
      "244:\tlearn: 0.1999709\ttotal: 3.43s\tremaining: 10.6s\n",
      "245:\tlearn: 0.1997919\ttotal: 3.45s\tremaining: 10.6s\n",
      "246:\tlearn: 0.1997187\ttotal: 3.46s\tremaining: 10.5s\n",
      "247:\tlearn: 0.1995577\ttotal: 3.48s\tremaining: 10.5s\n",
      "248:\tlearn: 0.1990586\ttotal: 3.49s\tremaining: 10.5s\n",
      "249:\tlearn: 0.1989972\ttotal: 3.5s\tremaining: 10.5s\n",
      "250:\tlearn: 0.1988941\ttotal: 3.52s\tremaining: 10.5s\n",
      "251:\tlearn: 0.1985746\ttotal: 3.53s\tremaining: 10.5s\n",
      "252:\tlearn: 0.1982011\ttotal: 3.54s\tremaining: 10.5s\n",
      "253:\tlearn: 0.1981350\ttotal: 3.56s\tremaining: 10.4s\n",
      "254:\tlearn: 0.1979733\ttotal: 3.57s\tremaining: 10.4s\n",
      "255:\tlearn: 0.1978625\ttotal: 3.58s\tremaining: 10.4s\n",
      "256:\tlearn: 0.1975410\ttotal: 3.59s\tremaining: 10.4s\n",
      "257:\tlearn: 0.1974167\ttotal: 3.61s\tremaining: 10.4s\n",
      "258:\tlearn: 0.1973093\ttotal: 3.62s\tremaining: 10.4s\n",
      "259:\tlearn: 0.1971856\ttotal: 3.63s\tremaining: 10.3s\n",
      "260:\tlearn: 0.1971317\ttotal: 3.65s\tremaining: 10.3s\n",
      "261:\tlearn: 0.1969798\ttotal: 3.66s\tremaining: 10.3s\n",
      "262:\tlearn: 0.1968837\ttotal: 3.68s\tremaining: 10.3s\n",
      "263:\tlearn: 0.1967508\ttotal: 3.69s\tremaining: 10.3s\n",
      "264:\tlearn: 0.1966623\ttotal: 3.71s\tremaining: 10.3s\n",
      "265:\tlearn: 0.1965737\ttotal: 3.72s\tremaining: 10.3s\n",
      "266:\tlearn: 0.1962123\ttotal: 3.73s\tremaining: 10.2s\n",
      "267:\tlearn: 0.1958219\ttotal: 3.75s\tremaining: 10.2s\n",
      "268:\tlearn: 0.1955680\ttotal: 3.76s\tremaining: 10.2s\n",
      "269:\tlearn: 0.1954875\ttotal: 3.77s\tremaining: 10.2s\n",
      "270:\tlearn: 0.1954240\ttotal: 3.79s\tremaining: 10.2s\n",
      "271:\tlearn: 0.1952616\ttotal: 3.8s\tremaining: 10.2s\n",
      "272:\tlearn: 0.1948982\ttotal: 3.82s\tremaining: 10.2s\n",
      "273:\tlearn: 0.1948335\ttotal: 3.83s\tremaining: 10.2s\n",
      "274:\tlearn: 0.1946486\ttotal: 3.85s\tremaining: 10.2s\n",
      "275:\tlearn: 0.1945713\ttotal: 3.86s\tremaining: 10.1s\n",
      "276:\tlearn: 0.1943506\ttotal: 3.88s\tremaining: 10.1s\n",
      "277:\tlearn: 0.1942317\ttotal: 3.89s\tremaining: 10.1s\n",
      "278:\tlearn: 0.1940789\ttotal: 3.9s\tremaining: 10.1s\n",
      "279:\tlearn: 0.1940162\ttotal: 3.92s\tremaining: 10.1s\n",
      "280:\tlearn: 0.1939948\ttotal: 3.93s\tremaining: 10.1s\n",
      "281:\tlearn: 0.1937441\ttotal: 3.94s\tremaining: 10s\n",
      "282:\tlearn: 0.1936348\ttotal: 3.96s\tremaining: 10s\n",
      "283:\tlearn: 0.1935662\ttotal: 3.97s\tremaining: 10s\n",
      "284:\tlearn: 0.1935101\ttotal: 3.98s\tremaining: 9.99s\n",
      "285:\tlearn: 0.1934254\ttotal: 4s\tremaining: 9.98s\n",
      "286:\tlearn: 0.1933713\ttotal: 4.01s\tremaining: 9.96s\n",
      "287:\tlearn: 0.1932833\ttotal: 4.02s\tremaining: 9.95s\n",
      "288:\tlearn: 0.1932002\ttotal: 4.04s\tremaining: 9.93s\n",
      "289:\tlearn: 0.1931173\ttotal: 4.05s\tremaining: 9.92s\n",
      "290:\tlearn: 0.1928393\ttotal: 4.07s\tremaining: 9.91s\n",
      "291:\tlearn: 0.1925055\ttotal: 4.08s\tremaining: 9.89s\n",
      "292:\tlearn: 0.1923300\ttotal: 4.09s\tremaining: 9.88s\n",
      "293:\tlearn: 0.1922281\ttotal: 4.11s\tremaining: 9.87s\n",
      "294:\tlearn: 0.1921498\ttotal: 4.12s\tremaining: 9.85s\n",
      "295:\tlearn: 0.1920386\ttotal: 4.13s\tremaining: 9.83s\n",
      "296:\tlearn: 0.1919364\ttotal: 4.14s\tremaining: 9.81s\n",
      "297:\tlearn: 0.1916816\ttotal: 4.16s\tremaining: 9.8s\n",
      "298:\tlearn: 0.1914914\ttotal: 4.17s\tremaining: 9.78s\n",
      "299:\tlearn: 0.1914218\ttotal: 4.18s\tremaining: 9.76s\n",
      "300:\tlearn: 0.1913433\ttotal: 4.2s\tremaining: 9.75s\n",
      "301:\tlearn: 0.1912426\ttotal: 4.22s\tremaining: 9.75s\n",
      "302:\tlearn: 0.1911609\ttotal: 4.23s\tremaining: 9.74s\n",
      "303:\tlearn: 0.1910773\ttotal: 4.25s\tremaining: 9.72s\n",
      "304:\tlearn: 0.1909590\ttotal: 4.26s\tremaining: 9.71s\n",
      "305:\tlearn: 0.1908127\ttotal: 4.28s\tremaining: 9.7s\n",
      "306:\tlearn: 0.1907180\ttotal: 4.29s\tremaining: 9.68s\n",
      "307:\tlearn: 0.1906548\ttotal: 4.3s\tremaining: 9.67s\n",
      "308:\tlearn: 0.1905730\ttotal: 4.32s\tremaining: 9.65s\n",
      "309:\tlearn: 0.1904302\ttotal: 4.33s\tremaining: 9.63s\n",
      "310:\tlearn: 0.1901743\ttotal: 4.34s\tremaining: 9.62s\n",
      "311:\tlearn: 0.1900525\ttotal: 4.36s\tremaining: 9.61s\n",
      "312:\tlearn: 0.1899156\ttotal: 4.37s\tremaining: 9.59s\n",
      "313:\tlearn: 0.1898614\ttotal: 4.38s\tremaining: 9.58s\n",
      "314:\tlearn: 0.1898027\ttotal: 4.4s\tremaining: 9.56s\n",
      "315:\tlearn: 0.1897447\ttotal: 4.41s\tremaining: 9.54s\n",
      "316:\tlearn: 0.1896080\ttotal: 4.42s\tremaining: 9.53s\n",
      "317:\tlearn: 0.1895457\ttotal: 4.44s\tremaining: 9.51s\n",
      "318:\tlearn: 0.1894547\ttotal: 4.45s\tremaining: 9.5s\n",
      "319:\tlearn: 0.1893984\ttotal: 4.46s\tremaining: 9.49s\n",
      "320:\tlearn: 0.1893671\ttotal: 4.48s\tremaining: 9.47s\n",
      "321:\tlearn: 0.1893358\ttotal: 4.49s\tremaining: 9.46s\n",
      "322:\tlearn: 0.1892889\ttotal: 4.5s\tremaining: 9.44s\n",
      "323:\tlearn: 0.1892510\ttotal: 4.51s\tremaining: 9.42s\n",
      "324:\tlearn: 0.1891455\ttotal: 4.53s\tremaining: 9.4s\n",
      "325:\tlearn: 0.1890624\ttotal: 4.54s\tremaining: 9.38s\n",
      "326:\tlearn: 0.1888925\ttotal: 4.55s\tremaining: 9.37s\n",
      "327:\tlearn: 0.1888151\ttotal: 4.56s\tremaining: 9.35s\n",
      "328:\tlearn: 0.1885399\ttotal: 4.58s\tremaining: 9.34s\n",
      "329:\tlearn: 0.1883691\ttotal: 4.59s\tremaining: 9.32s\n",
      "330:\tlearn: 0.1882684\ttotal: 4.6s\tremaining: 9.31s\n",
      "331:\tlearn: 0.1882679\ttotal: 4.62s\tremaining: 9.29s\n",
      "332:\tlearn: 0.1881992\ttotal: 4.63s\tremaining: 9.28s\n",
      "333:\tlearn: 0.1879224\ttotal: 4.65s\tremaining: 9.27s\n",
      "334:\tlearn: 0.1876441\ttotal: 4.66s\tremaining: 9.26s\n",
      "335:\tlearn: 0.1874992\ttotal: 4.68s\tremaining: 9.25s\n",
      "336:\tlearn: 0.1873706\ttotal: 4.7s\tremaining: 9.24s\n",
      "337:\tlearn: 0.1872268\ttotal: 4.71s\tremaining: 9.23s\n",
      "338:\tlearn: 0.1869429\ttotal: 4.73s\tremaining: 9.23s\n",
      "339:\tlearn: 0.1867544\ttotal: 4.75s\tremaining: 9.21s\n",
      "340:\tlearn: 0.1866003\ttotal: 4.76s\tremaining: 9.2s\n",
      "341:\tlearn: 0.1864572\ttotal: 4.77s\tremaining: 9.18s\n",
      "342:\tlearn: 0.1863936\ttotal: 4.79s\tremaining: 9.17s\n",
      "343:\tlearn: 0.1862506\ttotal: 4.8s\tremaining: 9.16s\n",
      "344:\tlearn: 0.1860617\ttotal: 4.82s\tremaining: 9.15s\n",
      "345:\tlearn: 0.1860296\ttotal: 4.83s\tremaining: 9.13s\n",
      "346:\tlearn: 0.1858729\ttotal: 4.84s\tremaining: 9.11s\n",
      "347:\tlearn: 0.1858040\ttotal: 4.86s\tremaining: 9.1s\n",
      "348:\tlearn: 0.1857422\ttotal: 4.87s\tremaining: 9.09s\n",
      "349:\tlearn: 0.1856860\ttotal: 4.88s\tremaining: 9.07s\n",
      "350:\tlearn: 0.1856860\ttotal: 4.9s\tremaining: 9.05s\n",
      "351:\tlearn: 0.1855390\ttotal: 4.91s\tremaining: 9.04s\n",
      "352:\tlearn: 0.1852699\ttotal: 4.92s\tremaining: 9.02s\n",
      "353:\tlearn: 0.1851812\ttotal: 4.93s\tremaining: 9.01s\n",
      "354:\tlearn: 0.1851379\ttotal: 4.95s\tremaining: 8.99s\n",
      "355:\tlearn: 0.1850745\ttotal: 4.96s\tremaining: 8.97s\n",
      "356:\tlearn: 0.1848333\ttotal: 4.97s\tremaining: 8.96s\n",
      "357:\tlearn: 0.1847770\ttotal: 4.99s\tremaining: 8.95s\n",
      "358:\tlearn: 0.1847070\ttotal: 5s\tremaining: 8.93s\n",
      "359:\tlearn: 0.1846181\ttotal: 5.01s\tremaining: 8.91s\n",
      "360:\tlearn: 0.1845434\ttotal: 5.03s\tremaining: 8.9s\n",
      "361:\tlearn: 0.1844989\ttotal: 5.04s\tremaining: 8.88s\n",
      "362:\tlearn: 0.1842981\ttotal: 5.06s\tremaining: 8.87s\n",
      "363:\tlearn: 0.1842233\ttotal: 5.07s\tremaining: 8.86s\n",
      "364:\tlearn: 0.1841265\ttotal: 5.08s\tremaining: 8.84s\n",
      "365:\tlearn: 0.1839387\ttotal: 5.1s\tremaining: 8.83s\n",
      "366:\tlearn: 0.1838324\ttotal: 5.11s\tremaining: 8.81s\n",
      "367:\tlearn: 0.1837762\ttotal: 5.12s\tremaining: 8.8s\n",
      "368:\tlearn: 0.1836708\ttotal: 5.13s\tremaining: 8.78s\n",
      "369:\tlearn: 0.1836026\ttotal: 5.15s\tremaining: 8.77s\n",
      "370:\tlearn: 0.1835748\ttotal: 5.16s\tremaining: 8.75s\n",
      "371:\tlearn: 0.1834493\ttotal: 5.17s\tremaining: 8.73s\n",
      "372:\tlearn: 0.1833190\ttotal: 5.19s\tremaining: 8.72s\n",
      "373:\tlearn: 0.1831671\ttotal: 5.2s\tremaining: 8.71s\n",
      "374:\tlearn: 0.1830632\ttotal: 5.22s\tremaining: 8.7s\n",
      "375:\tlearn: 0.1829322\ttotal: 5.23s\tremaining: 8.68s\n",
      "376:\tlearn: 0.1828673\ttotal: 5.24s\tremaining: 8.66s\n",
      "377:\tlearn: 0.1828236\ttotal: 5.25s\tremaining: 8.65s\n",
      "378:\tlearn: 0.1827207\ttotal: 5.27s\tremaining: 8.63s\n",
      "379:\tlearn: 0.1824732\ttotal: 5.28s\tremaining: 8.62s\n",
      "380:\tlearn: 0.1824385\ttotal: 5.29s\tremaining: 8.6s\n",
      "381:\tlearn: 0.1823921\ttotal: 5.31s\tremaining: 8.59s\n",
      "382:\tlearn: 0.1823103\ttotal: 5.32s\tremaining: 8.57s\n",
      "383:\tlearn: 0.1821411\ttotal: 5.33s\tremaining: 8.56s\n",
      "384:\tlearn: 0.1820646\ttotal: 5.35s\tremaining: 8.54s\n",
      "385:\tlearn: 0.1820294\ttotal: 5.36s\tremaining: 8.53s\n",
      "386:\tlearn: 0.1819825\ttotal: 5.37s\tremaining: 8.51s\n",
      "387:\tlearn: 0.1819058\ttotal: 5.38s\tremaining: 8.49s\n",
      "388:\tlearn: 0.1818543\ttotal: 5.4s\tremaining: 8.48s\n",
      "389:\tlearn: 0.1817738\ttotal: 5.41s\tremaining: 8.46s\n",
      "390:\tlearn: 0.1817559\ttotal: 5.42s\tremaining: 8.45s\n",
      "391:\tlearn: 0.1815202\ttotal: 5.44s\tremaining: 8.43s\n",
      "392:\tlearn: 0.1814614\ttotal: 5.45s\tremaining: 8.42s\n",
      "393:\tlearn: 0.1813973\ttotal: 5.47s\tremaining: 8.41s\n",
      "394:\tlearn: 0.1812569\ttotal: 5.48s\tremaining: 8.4s\n",
      "395:\tlearn: 0.1811370\ttotal: 5.5s\tremaining: 8.38s\n",
      "396:\tlearn: 0.1810724\ttotal: 5.51s\tremaining: 8.37s\n",
      "397:\tlearn: 0.1810369\ttotal: 5.52s\tremaining: 8.35s\n",
      "398:\tlearn: 0.1809365\ttotal: 5.54s\tremaining: 8.34s\n",
      "399:\tlearn: 0.1807688\ttotal: 5.55s\tremaining: 8.33s\n",
      "400:\tlearn: 0.1806718\ttotal: 5.56s\tremaining: 8.31s\n",
      "401:\tlearn: 0.1806446\ttotal: 5.58s\tremaining: 8.3s\n",
      "402:\tlearn: 0.1805947\ttotal: 5.59s\tremaining: 8.28s\n",
      "403:\tlearn: 0.1803878\ttotal: 5.6s\tremaining: 8.27s\n",
      "404:\tlearn: 0.1803249\ttotal: 5.62s\tremaining: 8.25s\n",
      "405:\tlearn: 0.1802449\ttotal: 5.63s\tremaining: 8.24s\n",
      "406:\tlearn: 0.1800170\ttotal: 5.64s\tremaining: 8.23s\n",
      "407:\tlearn: 0.1799564\ttotal: 5.66s\tremaining: 8.21s\n",
      "408:\tlearn: 0.1798915\ttotal: 5.67s\tremaining: 8.2s\n",
      "409:\tlearn: 0.1797873\ttotal: 5.69s\tremaining: 8.18s\n",
      "410:\tlearn: 0.1797226\ttotal: 5.7s\tremaining: 8.17s\n",
      "411:\tlearn: 0.1796498\ttotal: 5.71s\tremaining: 8.15s\n",
      "412:\tlearn: 0.1796191\ttotal: 5.72s\tremaining: 8.14s\n",
      "413:\tlearn: 0.1795763\ttotal: 5.74s\tremaining: 8.12s\n",
      "414:\tlearn: 0.1793879\ttotal: 5.75s\tremaining: 8.11s\n",
      "415:\tlearn: 0.1792676\ttotal: 5.76s\tremaining: 8.09s\n",
      "416:\tlearn: 0.1791621\ttotal: 5.78s\tremaining: 8.08s\n",
      "417:\tlearn: 0.1791060\ttotal: 5.79s\tremaining: 8.07s\n",
      "418:\tlearn: 0.1790113\ttotal: 5.81s\tremaining: 8.05s\n",
      "419:\tlearn: 0.1789234\ttotal: 5.82s\tremaining: 8.04s\n",
      "420:\tlearn: 0.1788818\ttotal: 5.83s\tremaining: 8.02s\n",
      "421:\tlearn: 0.1788245\ttotal: 5.85s\tremaining: 8.01s\n",
      "422:\tlearn: 0.1787630\ttotal: 5.86s\tremaining: 8s\n",
      "423:\tlearn: 0.1786500\ttotal: 5.88s\tremaining: 7.98s\n",
      "424:\tlearn: 0.1785560\ttotal: 5.89s\tremaining: 7.97s\n",
      "425:\tlearn: 0.1784817\ttotal: 5.9s\tremaining: 7.95s\n",
      "426:\tlearn: 0.1784039\ttotal: 5.92s\tremaining: 7.94s\n",
      "427:\tlearn: 0.1783543\ttotal: 5.93s\tremaining: 7.92s\n",
      "428:\tlearn: 0.1781826\ttotal: 5.94s\tremaining: 7.91s\n",
      "429:\tlearn: 0.1780961\ttotal: 5.96s\tremaining: 7.89s\n",
      "430:\tlearn: 0.1780394\ttotal: 5.97s\tremaining: 7.88s\n",
      "431:\tlearn: 0.1779447\ttotal: 5.98s\tremaining: 7.86s\n",
      "432:\tlearn: 0.1778826\ttotal: 5.99s\tremaining: 7.85s\n",
      "433:\tlearn: 0.1778433\ttotal: 6s\tremaining: 7.83s\n",
      "434:\tlearn: 0.1777784\ttotal: 6.02s\tremaining: 7.82s\n",
      "435:\tlearn: 0.1777225\ttotal: 6.03s\tremaining: 7.8s\n",
      "436:\tlearn: 0.1776614\ttotal: 6.04s\tremaining: 7.79s\n",
      "437:\tlearn: 0.1776025\ttotal: 6.06s\tremaining: 7.77s\n",
      "438:\tlearn: 0.1775540\ttotal: 6.07s\tremaining: 7.76s\n",
      "439:\tlearn: 0.1774399\ttotal: 6.08s\tremaining: 7.75s\n",
      "440:\tlearn: 0.1773577\ttotal: 6.1s\tremaining: 7.73s\n",
      "441:\tlearn: 0.1772776\ttotal: 6.11s\tremaining: 7.71s\n",
      "442:\tlearn: 0.1771969\ttotal: 6.12s\tremaining: 7.7s\n",
      "443:\tlearn: 0.1771145\ttotal: 6.14s\tremaining: 7.68s\n",
      "444:\tlearn: 0.1770443\ttotal: 6.15s\tremaining: 7.67s\n",
      "445:\tlearn: 0.1769337\ttotal: 6.16s\tremaining: 7.66s\n",
      "446:\tlearn: 0.1768958\ttotal: 6.18s\tremaining: 7.64s\n",
      "447:\tlearn: 0.1768196\ttotal: 6.19s\tremaining: 7.63s\n",
      "448:\tlearn: 0.1767637\ttotal: 6.21s\tremaining: 7.62s\n",
      "449:\tlearn: 0.1766634\ttotal: 6.22s\tremaining: 7.6s\n",
      "450:\tlearn: 0.1766136\ttotal: 6.23s\tremaining: 7.59s\n",
      "451:\tlearn: 0.1765464\ttotal: 6.25s\tremaining: 7.57s\n",
      "452:\tlearn: 0.1764984\ttotal: 6.26s\tremaining: 7.56s\n",
      "453:\tlearn: 0.1764722\ttotal: 6.27s\tremaining: 7.54s\n",
      "454:\tlearn: 0.1764247\ttotal: 6.29s\tremaining: 7.53s\n",
      "455:\tlearn: 0.1763763\ttotal: 6.3s\tremaining: 7.51s\n",
      "456:\tlearn: 0.1762985\ttotal: 6.31s\tremaining: 7.5s\n",
      "457:\tlearn: 0.1762469\ttotal: 6.33s\tremaining: 7.49s\n",
      "458:\tlearn: 0.1761503\ttotal: 6.34s\tremaining: 7.47s\n",
      "459:\tlearn: 0.1761178\ttotal: 6.35s\tremaining: 7.46s\n",
      "460:\tlearn: 0.1760359\ttotal: 6.37s\tremaining: 7.44s\n",
      "461:\tlearn: 0.1760033\ttotal: 6.38s\tremaining: 7.43s\n",
      "462:\tlearn: 0.1759743\ttotal: 6.39s\tremaining: 7.41s\n",
      "463:\tlearn: 0.1759240\ttotal: 6.41s\tremaining: 7.4s\n",
      "464:\tlearn: 0.1758637\ttotal: 6.42s\tremaining: 7.39s\n",
      "465:\tlearn: 0.1757808\ttotal: 6.44s\tremaining: 7.38s\n",
      "466:\tlearn: 0.1757063\ttotal: 6.45s\tremaining: 7.37s\n",
      "467:\tlearn: 0.1756670\ttotal: 6.47s\tremaining: 7.35s\n",
      "468:\tlearn: 0.1756090\ttotal: 6.48s\tremaining: 7.34s\n",
      "469:\tlearn: 0.1755803\ttotal: 6.5s\tremaining: 7.33s\n",
      "470:\tlearn: 0.1755174\ttotal: 6.51s\tremaining: 7.31s\n",
      "471:\tlearn: 0.1754764\ttotal: 6.52s\tremaining: 7.3s\n",
      "472:\tlearn: 0.1753989\ttotal: 6.54s\tremaining: 7.28s\n",
      "473:\tlearn: 0.1752999\ttotal: 6.55s\tremaining: 7.27s\n",
      "474:\tlearn: 0.1752236\ttotal: 6.57s\tremaining: 7.26s\n",
      "475:\tlearn: 0.1751547\ttotal: 6.58s\tremaining: 7.24s\n",
      "476:\tlearn: 0.1751130\ttotal: 6.59s\tremaining: 7.23s\n",
      "477:\tlearn: 0.1750637\ttotal: 6.61s\tremaining: 7.22s\n",
      "478:\tlearn: 0.1750013\ttotal: 6.62s\tremaining: 7.2s\n",
      "479:\tlearn: 0.1749522\ttotal: 6.64s\tremaining: 7.19s\n",
      "480:\tlearn: 0.1748420\ttotal: 6.65s\tremaining: 7.18s\n",
      "481:\tlearn: 0.1747426\ttotal: 6.67s\tremaining: 7.17s\n",
      "482:\tlearn: 0.1747010\ttotal: 6.68s\tremaining: 7.15s\n",
      "483:\tlearn: 0.1745256\ttotal: 6.7s\tremaining: 7.14s\n",
      "484:\tlearn: 0.1744742\ttotal: 6.71s\tremaining: 7.13s\n",
      "485:\tlearn: 0.1744270\ttotal: 6.73s\tremaining: 7.11s\n",
      "486:\tlearn: 0.1743876\ttotal: 6.74s\tremaining: 7.1s\n",
      "487:\tlearn: 0.1743102\ttotal: 6.75s\tremaining: 7.09s\n",
      "488:\tlearn: 0.1742008\ttotal: 6.77s\tremaining: 7.08s\n",
      "489:\tlearn: 0.1741523\ttotal: 6.79s\tremaining: 7.07s\n",
      "490:\tlearn: 0.1741144\ttotal: 6.8s\tremaining: 7.05s\n",
      "491:\tlearn: 0.1740574\ttotal: 6.82s\tremaining: 7.04s\n",
      "492:\tlearn: 0.1740031\ttotal: 6.83s\tremaining: 7.03s\n",
      "493:\tlearn: 0.1738840\ttotal: 6.85s\tremaining: 7.01s\n",
      "494:\tlearn: 0.1738245\ttotal: 6.87s\tremaining: 7.01s\n",
      "495:\tlearn: 0.1737352\ttotal: 6.89s\tremaining: 7s\n",
      "496:\tlearn: 0.1736897\ttotal: 6.9s\tremaining: 6.99s\n",
      "497:\tlearn: 0.1736413\ttotal: 6.92s\tremaining: 6.97s\n",
      "498:\tlearn: 0.1736093\ttotal: 6.93s\tremaining: 6.96s\n",
      "499:\tlearn: 0.1735685\ttotal: 6.95s\tremaining: 6.95s\n",
      "500:\tlearn: 0.1734448\ttotal: 6.96s\tremaining: 6.93s\n",
      "501:\tlearn: 0.1734237\ttotal: 6.97s\tremaining: 6.92s\n",
      "502:\tlearn: 0.1733409\ttotal: 6.99s\tremaining: 6.91s\n",
      "503:\tlearn: 0.1732504\ttotal: 7s\tremaining: 6.89s\n",
      "504:\tlearn: 0.1731806\ttotal: 7.02s\tremaining: 6.88s\n",
      "505:\tlearn: 0.1731268\ttotal: 7.03s\tremaining: 6.87s\n",
      "506:\tlearn: 0.1730446\ttotal: 7.05s\tremaining: 6.85s\n",
      "507:\tlearn: 0.1729839\ttotal: 7.06s\tremaining: 6.84s\n",
      "508:\tlearn: 0.1728356\ttotal: 7.08s\tremaining: 6.83s\n",
      "509:\tlearn: 0.1727828\ttotal: 7.09s\tremaining: 6.82s\n",
      "510:\tlearn: 0.1727449\ttotal: 7.11s\tremaining: 6.8s\n",
      "511:\tlearn: 0.1726465\ttotal: 7.12s\tremaining: 6.79s\n",
      "512:\tlearn: 0.1725783\ttotal: 7.14s\tremaining: 6.78s\n",
      "513:\tlearn: 0.1724799\ttotal: 7.15s\tremaining: 6.76s\n",
      "514:\tlearn: 0.1723108\ttotal: 7.17s\tremaining: 6.75s\n",
      "515:\tlearn: 0.1721550\ttotal: 7.19s\tremaining: 6.74s\n",
      "516:\tlearn: 0.1721012\ttotal: 7.2s\tremaining: 6.73s\n",
      "517:\tlearn: 0.1720446\ttotal: 7.22s\tremaining: 6.72s\n",
      "518:\tlearn: 0.1719088\ttotal: 7.24s\tremaining: 6.71s\n",
      "519:\tlearn: 0.1717551\ttotal: 7.25s\tremaining: 6.69s\n",
      "520:\tlearn: 0.1717278\ttotal: 7.26s\tremaining: 6.68s\n",
      "521:\tlearn: 0.1716567\ttotal: 7.28s\tremaining: 6.67s\n",
      "522:\tlearn: 0.1716053\ttotal: 7.29s\tremaining: 6.65s\n",
      "523:\tlearn: 0.1715284\ttotal: 7.31s\tremaining: 6.64s\n",
      "524:\tlearn: 0.1714710\ttotal: 7.32s\tremaining: 6.63s\n",
      "525:\tlearn: 0.1714207\ttotal: 7.34s\tremaining: 6.61s\n",
      "526:\tlearn: 0.1713638\ttotal: 7.35s\tremaining: 6.6s\n",
      "527:\tlearn: 0.1713297\ttotal: 7.37s\tremaining: 6.58s\n",
      "528:\tlearn: 0.1712795\ttotal: 7.38s\tremaining: 6.57s\n",
      "529:\tlearn: 0.1712450\ttotal: 7.4s\tremaining: 6.56s\n",
      "530:\tlearn: 0.1712028\ttotal: 7.41s\tremaining: 6.55s\n",
      "531:\tlearn: 0.1711341\ttotal: 7.43s\tremaining: 6.54s\n",
      "532:\tlearn: 0.1710952\ttotal: 7.45s\tremaining: 6.53s\n",
      "533:\tlearn: 0.1709998\ttotal: 7.47s\tremaining: 6.51s\n",
      "534:\tlearn: 0.1709496\ttotal: 7.48s\tremaining: 6.5s\n",
      "535:\tlearn: 0.1709163\ttotal: 7.5s\tremaining: 6.49s\n",
      "536:\tlearn: 0.1708383\ttotal: 7.51s\tremaining: 6.47s\n",
      "537:\tlearn: 0.1708197\ttotal: 7.52s\tremaining: 6.46s\n",
      "538:\tlearn: 0.1707823\ttotal: 7.54s\tremaining: 6.45s\n",
      "539:\tlearn: 0.1707432\ttotal: 7.55s\tremaining: 6.43s\n",
      "540:\tlearn: 0.1706630\ttotal: 7.56s\tremaining: 6.42s\n",
      "541:\tlearn: 0.1705261\ttotal: 7.58s\tremaining: 6.4s\n",
      "542:\tlearn: 0.1704304\ttotal: 7.59s\tremaining: 6.39s\n",
      "543:\tlearn: 0.1704168\ttotal: 7.6s\tremaining: 6.37s\n",
      "544:\tlearn: 0.1703814\ttotal: 7.62s\tremaining: 6.36s\n",
      "545:\tlearn: 0.1702301\ttotal: 7.63s\tremaining: 6.34s\n",
      "546:\tlearn: 0.1701798\ttotal: 7.64s\tremaining: 6.33s\n",
      "547:\tlearn: 0.1701067\ttotal: 7.66s\tremaining: 6.32s\n",
      "548:\tlearn: 0.1700338\ttotal: 7.67s\tremaining: 6.3s\n",
      "549:\tlearn: 0.1699860\ttotal: 7.69s\tremaining: 6.29s\n",
      "550:\tlearn: 0.1699423\ttotal: 7.7s\tremaining: 6.28s\n",
      "551:\tlearn: 0.1699070\ttotal: 7.72s\tremaining: 6.26s\n",
      "552:\tlearn: 0.1698455\ttotal: 7.73s\tremaining: 6.25s\n",
      "553:\tlearn: 0.1697897\ttotal: 7.75s\tremaining: 6.24s\n",
      "554:\tlearn: 0.1697596\ttotal: 7.76s\tremaining: 6.22s\n",
      "555:\tlearn: 0.1697002\ttotal: 7.78s\tremaining: 6.21s\n",
      "556:\tlearn: 0.1696261\ttotal: 7.79s\tremaining: 6.2s\n",
      "557:\tlearn: 0.1695832\ttotal: 7.81s\tremaining: 6.18s\n",
      "558:\tlearn: 0.1695604\ttotal: 7.82s\tremaining: 6.17s\n",
      "559:\tlearn: 0.1695169\ttotal: 7.84s\tremaining: 6.16s\n",
      "560:\tlearn: 0.1693400\ttotal: 7.85s\tremaining: 6.15s\n",
      "561:\tlearn: 0.1692583\ttotal: 7.87s\tremaining: 6.13s\n",
      "562:\tlearn: 0.1692095\ttotal: 7.88s\tremaining: 6.12s\n",
      "563:\tlearn: 0.1691479\ttotal: 7.9s\tremaining: 6.11s\n",
      "564:\tlearn: 0.1690741\ttotal: 7.91s\tremaining: 6.09s\n",
      "565:\tlearn: 0.1688977\ttotal: 7.93s\tremaining: 6.08s\n",
      "566:\tlearn: 0.1688550\ttotal: 7.94s\tremaining: 6.07s\n",
      "567:\tlearn: 0.1687876\ttotal: 7.96s\tremaining: 6.05s\n",
      "568:\tlearn: 0.1687435\ttotal: 7.97s\tremaining: 6.04s\n",
      "569:\tlearn: 0.1686744\ttotal: 7.99s\tremaining: 6.02s\n",
      "570:\tlearn: 0.1686039\ttotal: 8s\tremaining: 6.01s\n",
      "571:\tlearn: 0.1685268\ttotal: 8.02s\tremaining: 6s\n",
      "572:\tlearn: 0.1684623\ttotal: 8.03s\tremaining: 5.98s\n",
      "573:\tlearn: 0.1684175\ttotal: 8.04s\tremaining: 5.97s\n",
      "574:\tlearn: 0.1683585\ttotal: 8.06s\tremaining: 5.96s\n",
      "575:\tlearn: 0.1683183\ttotal: 8.07s\tremaining: 5.94s\n",
      "576:\tlearn: 0.1682676\ttotal: 8.11s\tremaining: 5.95s\n",
      "577:\tlearn: 0.1682073\ttotal: 8.13s\tremaining: 5.93s\n",
      "578:\tlearn: 0.1681531\ttotal: 8.14s\tremaining: 5.92s\n",
      "579:\tlearn: 0.1681333\ttotal: 8.15s\tremaining: 5.9s\n",
      "580:\tlearn: 0.1680324\ttotal: 8.17s\tremaining: 5.89s\n",
      "581:\tlearn: 0.1679960\ttotal: 8.18s\tremaining: 5.88s\n",
      "582:\tlearn: 0.1679156\ttotal: 8.2s\tremaining: 5.86s\n",
      "583:\tlearn: 0.1678579\ttotal: 8.21s\tremaining: 5.85s\n",
      "584:\tlearn: 0.1678069\ttotal: 8.22s\tremaining: 5.83s\n",
      "585:\tlearn: 0.1677649\ttotal: 8.24s\tremaining: 5.82s\n",
      "586:\tlearn: 0.1677209\ttotal: 8.25s\tremaining: 5.8s\n",
      "587:\tlearn: 0.1676562\ttotal: 8.26s\tremaining: 5.79s\n",
      "588:\tlearn: 0.1676513\ttotal: 8.27s\tremaining: 5.77s\n",
      "589:\tlearn: 0.1675850\ttotal: 8.29s\tremaining: 5.76s\n",
      "590:\tlearn: 0.1675462\ttotal: 8.3s\tremaining: 5.74s\n",
      "591:\tlearn: 0.1675052\ttotal: 8.31s\tremaining: 5.73s\n",
      "592:\tlearn: 0.1674359\ttotal: 8.32s\tremaining: 5.71s\n",
      "593:\tlearn: 0.1673978\ttotal: 8.34s\tremaining: 5.7s\n",
      "594:\tlearn: 0.1672733\ttotal: 8.35s\tremaining: 5.68s\n",
      "595:\tlearn: 0.1672105\ttotal: 8.36s\tremaining: 5.67s\n",
      "596:\tlearn: 0.1671512\ttotal: 8.38s\tremaining: 5.65s\n",
      "597:\tlearn: 0.1671275\ttotal: 8.39s\tremaining: 5.64s\n",
      "598:\tlearn: 0.1670884\ttotal: 8.4s\tremaining: 5.63s\n",
      "599:\tlearn: 0.1670184\ttotal: 8.42s\tremaining: 5.61s\n",
      "600:\tlearn: 0.1669019\ttotal: 8.43s\tremaining: 5.6s\n",
      "601:\tlearn: 0.1668613\ttotal: 8.44s\tremaining: 5.58s\n",
      "602:\tlearn: 0.1668152\ttotal: 8.46s\tremaining: 5.57s\n",
      "603:\tlearn: 0.1667107\ttotal: 8.47s\tremaining: 5.55s\n",
      "604:\tlearn: 0.1666448\ttotal: 8.48s\tremaining: 5.54s\n",
      "605:\tlearn: 0.1665825\ttotal: 8.5s\tremaining: 5.52s\n",
      "606:\tlearn: 0.1665145\ttotal: 8.51s\tremaining: 5.51s\n",
      "607:\tlearn: 0.1664565\ttotal: 8.52s\tremaining: 5.5s\n",
      "608:\tlearn: 0.1664329\ttotal: 8.54s\tremaining: 5.48s\n",
      "609:\tlearn: 0.1663667\ttotal: 8.55s\tremaining: 5.47s\n",
      "610:\tlearn: 0.1662920\ttotal: 8.56s\tremaining: 5.45s\n",
      "611:\tlearn: 0.1662718\ttotal: 8.57s\tremaining: 5.43s\n",
      "612:\tlearn: 0.1662197\ttotal: 8.59s\tremaining: 5.42s\n",
      "613:\tlearn: 0.1661802\ttotal: 8.6s\tremaining: 5.41s\n",
      "614:\tlearn: 0.1661123\ttotal: 8.61s\tremaining: 5.39s\n",
      "615:\tlearn: 0.1660712\ttotal: 8.63s\tremaining: 5.38s\n",
      "616:\tlearn: 0.1660585\ttotal: 8.64s\tremaining: 5.36s\n",
      "617:\tlearn: 0.1660229\ttotal: 8.65s\tremaining: 5.35s\n",
      "618:\tlearn: 0.1659435\ttotal: 8.67s\tremaining: 5.33s\n",
      "619:\tlearn: 0.1658795\ttotal: 8.68s\tremaining: 5.32s\n",
      "620:\tlearn: 0.1658149\ttotal: 8.69s\tremaining: 5.3s\n",
      "621:\tlearn: 0.1657746\ttotal: 8.71s\tremaining: 5.29s\n",
      "622:\tlearn: 0.1657535\ttotal: 8.72s\tremaining: 5.28s\n",
      "623:\tlearn: 0.1657087\ttotal: 8.73s\tremaining: 5.26s\n",
      "624:\tlearn: 0.1656095\ttotal: 8.74s\tremaining: 5.25s\n",
      "625:\tlearn: 0.1655744\ttotal: 8.76s\tremaining: 5.23s\n",
      "626:\tlearn: 0.1655432\ttotal: 8.77s\tremaining: 5.22s\n",
      "627:\tlearn: 0.1655084\ttotal: 8.79s\tremaining: 5.2s\n",
      "628:\tlearn: 0.1654443\ttotal: 8.8s\tremaining: 5.19s\n",
      "629:\tlearn: 0.1653843\ttotal: 8.81s\tremaining: 5.17s\n",
      "630:\tlearn: 0.1653614\ttotal: 8.83s\tremaining: 5.16s\n",
      "631:\tlearn: 0.1653426\ttotal: 8.84s\tremaining: 5.15s\n",
      "632:\tlearn: 0.1652964\ttotal: 8.85s\tremaining: 5.13s\n",
      "633:\tlearn: 0.1652419\ttotal: 8.87s\tremaining: 5.12s\n",
      "634:\tlearn: 0.1651869\ttotal: 8.88s\tremaining: 5.11s\n",
      "635:\tlearn: 0.1651543\ttotal: 8.9s\tremaining: 5.09s\n",
      "636:\tlearn: 0.1650613\ttotal: 8.92s\tremaining: 5.08s\n",
      "637:\tlearn: 0.1650473\ttotal: 8.93s\tremaining: 5.07s\n",
      "638:\tlearn: 0.1650291\ttotal: 8.94s\tremaining: 5.05s\n",
      "639:\tlearn: 0.1649105\ttotal: 8.95s\tremaining: 5.04s\n",
      "640:\tlearn: 0.1648694\ttotal: 8.97s\tremaining: 5.02s\n",
      "641:\tlearn: 0.1648151\ttotal: 8.98s\tremaining: 5.01s\n",
      "642:\tlearn: 0.1647367\ttotal: 8.99s\tremaining: 4.99s\n",
      "643:\tlearn: 0.1647065\ttotal: 9.01s\tremaining: 4.98s\n",
      "644:\tlearn: 0.1646635\ttotal: 9.02s\tremaining: 4.97s\n",
      "645:\tlearn: 0.1645872\ttotal: 9.04s\tremaining: 4.95s\n",
      "646:\tlearn: 0.1645320\ttotal: 9.05s\tremaining: 4.94s\n",
      "647:\tlearn: 0.1644814\ttotal: 9.06s\tremaining: 4.92s\n",
      "648:\tlearn: 0.1642848\ttotal: 9.08s\tremaining: 4.91s\n",
      "649:\tlearn: 0.1642847\ttotal: 9.09s\tremaining: 4.89s\n",
      "650:\tlearn: 0.1642386\ttotal: 9.1s\tremaining: 4.88s\n",
      "651:\tlearn: 0.1641733\ttotal: 9.11s\tremaining: 4.86s\n",
      "652:\tlearn: 0.1641031\ttotal: 9.13s\tremaining: 4.85s\n",
      "653:\tlearn: 0.1640388\ttotal: 9.14s\tremaining: 4.84s\n",
      "654:\tlearn: 0.1639895\ttotal: 9.16s\tremaining: 4.82s\n",
      "655:\tlearn: 0.1639468\ttotal: 9.17s\tremaining: 4.81s\n",
      "656:\tlearn: 0.1638763\ttotal: 9.18s\tremaining: 4.79s\n",
      "657:\tlearn: 0.1638763\ttotal: 9.19s\tremaining: 4.78s\n",
      "658:\tlearn: 0.1638295\ttotal: 9.21s\tremaining: 4.76s\n",
      "659:\tlearn: 0.1637862\ttotal: 9.22s\tremaining: 4.75s\n",
      "660:\tlearn: 0.1637371\ttotal: 9.23s\tremaining: 4.74s\n",
      "661:\tlearn: 0.1635858\ttotal: 9.25s\tremaining: 4.72s\n",
      "662:\tlearn: 0.1635566\ttotal: 9.26s\tremaining: 4.71s\n",
      "663:\tlearn: 0.1635039\ttotal: 9.27s\tremaining: 4.69s\n",
      "664:\tlearn: 0.1634722\ttotal: 9.29s\tremaining: 4.68s\n",
      "665:\tlearn: 0.1633920\ttotal: 9.3s\tremaining: 4.66s\n",
      "666:\tlearn: 0.1633250\ttotal: 9.31s\tremaining: 4.65s\n",
      "667:\tlearn: 0.1632189\ttotal: 9.32s\tremaining: 4.63s\n",
      "668:\tlearn: 0.1631535\ttotal: 9.34s\tremaining: 4.62s\n",
      "669:\tlearn: 0.1630984\ttotal: 9.35s\tremaining: 4.61s\n",
      "670:\tlearn: 0.1630517\ttotal: 9.37s\tremaining: 4.59s\n",
      "671:\tlearn: 0.1630093\ttotal: 9.38s\tremaining: 4.58s\n",
      "672:\tlearn: 0.1629537\ttotal: 9.39s\tremaining: 4.56s\n",
      "673:\tlearn: 0.1629027\ttotal: 9.41s\tremaining: 4.55s\n",
      "674:\tlearn: 0.1628188\ttotal: 9.42s\tremaining: 4.54s\n",
      "675:\tlearn: 0.1627488\ttotal: 9.43s\tremaining: 4.52s\n",
      "676:\tlearn: 0.1627266\ttotal: 9.45s\tremaining: 4.51s\n",
      "677:\tlearn: 0.1626978\ttotal: 9.46s\tremaining: 4.49s\n",
      "678:\tlearn: 0.1626302\ttotal: 9.47s\tremaining: 4.48s\n",
      "679:\tlearn: 0.1625823\ttotal: 9.48s\tremaining: 4.46s\n",
      "680:\tlearn: 0.1625320\ttotal: 9.5s\tremaining: 4.45s\n",
      "681:\tlearn: 0.1624760\ttotal: 9.51s\tremaining: 4.43s\n",
      "682:\tlearn: 0.1624187\ttotal: 9.52s\tremaining: 4.42s\n",
      "683:\tlearn: 0.1623858\ttotal: 9.54s\tremaining: 4.41s\n",
      "684:\tlearn: 0.1623462\ttotal: 9.55s\tremaining: 4.39s\n",
      "685:\tlearn: 0.1623137\ttotal: 9.56s\tremaining: 4.38s\n",
      "686:\tlearn: 0.1622518\ttotal: 9.57s\tremaining: 4.36s\n",
      "687:\tlearn: 0.1622236\ttotal: 9.59s\tremaining: 4.35s\n",
      "688:\tlearn: 0.1621672\ttotal: 9.6s\tremaining: 4.33s\n",
      "689:\tlearn: 0.1621314\ttotal: 9.62s\tremaining: 4.32s\n",
      "690:\tlearn: 0.1620930\ttotal: 9.63s\tremaining: 4.31s\n",
      "691:\tlearn: 0.1620514\ttotal: 9.64s\tremaining: 4.29s\n",
      "692:\tlearn: 0.1619955\ttotal: 9.65s\tremaining: 4.28s\n",
      "693:\tlearn: 0.1618320\ttotal: 9.67s\tremaining: 4.26s\n",
      "694:\tlearn: 0.1617855\ttotal: 9.68s\tremaining: 4.25s\n",
      "695:\tlearn: 0.1617528\ttotal: 9.69s\tremaining: 4.23s\n",
      "696:\tlearn: 0.1616819\ttotal: 9.71s\tremaining: 4.22s\n",
      "697:\tlearn: 0.1616313\ttotal: 9.72s\tremaining: 4.21s\n",
      "698:\tlearn: 0.1615978\ttotal: 9.74s\tremaining: 4.19s\n",
      "699:\tlearn: 0.1615404\ttotal: 9.75s\tremaining: 4.18s\n",
      "700:\tlearn: 0.1615067\ttotal: 9.76s\tremaining: 4.16s\n",
      "701:\tlearn: 0.1614694\ttotal: 9.78s\tremaining: 4.15s\n",
      "702:\tlearn: 0.1614046\ttotal: 9.79s\tremaining: 4.14s\n",
      "703:\tlearn: 0.1613380\ttotal: 9.8s\tremaining: 4.12s\n",
      "704:\tlearn: 0.1612936\ttotal: 9.82s\tremaining: 4.11s\n",
      "705:\tlearn: 0.1612478\ttotal: 9.83s\tremaining: 4.09s\n",
      "706:\tlearn: 0.1612212\ttotal: 9.84s\tremaining: 4.08s\n",
      "707:\tlearn: 0.1611736\ttotal: 9.86s\tremaining: 4.06s\n",
      "708:\tlearn: 0.1611212\ttotal: 9.87s\tremaining: 4.05s\n",
      "709:\tlearn: 0.1610762\ttotal: 9.88s\tremaining: 4.04s\n",
      "710:\tlearn: 0.1609951\ttotal: 9.89s\tremaining: 4.02s\n",
      "711:\tlearn: 0.1609420\ttotal: 9.91s\tremaining: 4.01s\n",
      "712:\tlearn: 0.1608993\ttotal: 9.92s\tremaining: 3.99s\n",
      "713:\tlearn: 0.1608227\ttotal: 9.93s\tremaining: 3.98s\n",
      "714:\tlearn: 0.1608043\ttotal: 9.95s\tremaining: 3.96s\n",
      "715:\tlearn: 0.1607670\ttotal: 9.96s\tremaining: 3.95s\n",
      "716:\tlearn: 0.1607371\ttotal: 9.97s\tremaining: 3.94s\n",
      "717:\tlearn: 0.1606715\ttotal: 9.98s\tremaining: 3.92s\n",
      "718:\tlearn: 0.1606142\ttotal: 10s\tremaining: 3.91s\n",
      "719:\tlearn: 0.1605680\ttotal: 10s\tremaining: 3.89s\n",
      "720:\tlearn: 0.1605294\ttotal: 10s\tremaining: 3.88s\n",
      "721:\tlearn: 0.1604818\ttotal: 10s\tremaining: 3.87s\n",
      "722:\tlearn: 0.1604243\ttotal: 10.1s\tremaining: 3.85s\n",
      "723:\tlearn: 0.1603641\ttotal: 10.1s\tremaining: 3.84s\n",
      "724:\tlearn: 0.1602698\ttotal: 10.1s\tremaining: 3.82s\n",
      "725:\tlearn: 0.1602331\ttotal: 10.1s\tremaining: 3.81s\n",
      "726:\tlearn: 0.1602156\ttotal: 10.1s\tremaining: 3.79s\n",
      "727:\tlearn: 0.1601598\ttotal: 10.1s\tremaining: 3.78s\n",
      "728:\tlearn: 0.1601323\ttotal: 10.1s\tremaining: 3.77s\n",
      "729:\tlearn: 0.1600863\ttotal: 10.1s\tremaining: 3.75s\n",
      "730:\tlearn: 0.1600216\ttotal: 10.2s\tremaining: 3.74s\n",
      "731:\tlearn: 0.1599776\ttotal: 10.2s\tremaining: 3.72s\n",
      "732:\tlearn: 0.1599347\ttotal: 10.2s\tremaining: 3.71s\n",
      "733:\tlearn: 0.1598502\ttotal: 10.2s\tremaining: 3.7s\n",
      "734:\tlearn: 0.1597931\ttotal: 10.2s\tremaining: 3.68s\n",
      "735:\tlearn: 0.1597409\ttotal: 10.2s\tremaining: 3.67s\n",
      "736:\tlearn: 0.1596674\ttotal: 10.3s\tremaining: 3.66s\n",
      "737:\tlearn: 0.1595751\ttotal: 10.3s\tremaining: 3.65s\n",
      "738:\tlearn: 0.1595108\ttotal: 10.3s\tremaining: 3.64s\n",
      "739:\tlearn: 0.1594511\ttotal: 10.3s\tremaining: 3.62s\n",
      "740:\tlearn: 0.1594267\ttotal: 10.3s\tremaining: 3.61s\n",
      "741:\tlearn: 0.1593657\ttotal: 10.3s\tremaining: 3.59s\n",
      "742:\tlearn: 0.1593201\ttotal: 10.4s\tremaining: 3.58s\n",
      "743:\tlearn: 0.1592894\ttotal: 10.4s\tremaining: 3.56s\n",
      "744:\tlearn: 0.1592727\ttotal: 10.4s\tremaining: 3.55s\n",
      "745:\tlearn: 0.1592269\ttotal: 10.4s\tremaining: 3.54s\n",
      "746:\tlearn: 0.1591818\ttotal: 10.4s\tremaining: 3.52s\n",
      "747:\tlearn: 0.1591269\ttotal: 10.4s\tremaining: 3.51s\n",
      "748:\tlearn: 0.1590730\ttotal: 10.4s\tremaining: 3.5s\n",
      "749:\tlearn: 0.1590726\ttotal: 10.4s\tremaining: 3.48s\n",
      "750:\tlearn: 0.1590526\ttotal: 10.5s\tremaining: 3.47s\n",
      "751:\tlearn: 0.1590189\ttotal: 10.5s\tremaining: 3.45s\n",
      "752:\tlearn: 0.1589608\ttotal: 10.5s\tremaining: 3.44s\n",
      "753:\tlearn: 0.1589463\ttotal: 10.5s\tremaining: 3.42s\n",
      "754:\tlearn: 0.1589125\ttotal: 10.5s\tremaining: 3.41s\n",
      "755:\tlearn: 0.1588722\ttotal: 10.5s\tremaining: 3.39s\n",
      "756:\tlearn: 0.1588368\ttotal: 10.5s\tremaining: 3.38s\n",
      "757:\tlearn: 0.1587851\ttotal: 10.5s\tremaining: 3.37s\n",
      "758:\tlearn: 0.1587360\ttotal: 10.6s\tremaining: 3.35s\n",
      "759:\tlearn: 0.1586976\ttotal: 10.6s\tremaining: 3.34s\n",
      "760:\tlearn: 0.1586238\ttotal: 10.6s\tremaining: 3.33s\n",
      "761:\tlearn: 0.1585798\ttotal: 10.6s\tremaining: 3.31s\n",
      "762:\tlearn: 0.1585246\ttotal: 10.6s\tremaining: 3.3s\n",
      "763:\tlearn: 0.1584961\ttotal: 10.6s\tremaining: 3.28s\n",
      "764:\tlearn: 0.1584420\ttotal: 10.6s\tremaining: 3.27s\n",
      "765:\tlearn: 0.1584390\ttotal: 10.7s\tremaining: 3.25s\n",
      "766:\tlearn: 0.1584122\ttotal: 10.7s\tremaining: 3.24s\n",
      "767:\tlearn: 0.1583676\ttotal: 10.7s\tremaining: 3.23s\n",
      "768:\tlearn: 0.1583304\ttotal: 10.7s\tremaining: 3.21s\n",
      "769:\tlearn: 0.1583088\ttotal: 10.7s\tremaining: 3.2s\n",
      "770:\tlearn: 0.1582633\ttotal: 10.7s\tremaining: 3.19s\n",
      "771:\tlearn: 0.1582439\ttotal: 10.7s\tremaining: 3.17s\n",
      "772:\tlearn: 0.1582262\ttotal: 10.8s\tremaining: 3.16s\n",
      "773:\tlearn: 0.1581825\ttotal: 10.8s\tremaining: 3.14s\n",
      "774:\tlearn: 0.1581514\ttotal: 10.8s\tremaining: 3.13s\n",
      "775:\tlearn: 0.1581160\ttotal: 10.8s\tremaining: 3.12s\n",
      "776:\tlearn: 0.1580611\ttotal: 10.8s\tremaining: 3.1s\n",
      "777:\tlearn: 0.1580160\ttotal: 10.8s\tremaining: 3.09s\n",
      "778:\tlearn: 0.1579755\ttotal: 10.8s\tremaining: 3.07s\n",
      "779:\tlearn: 0.1579211\ttotal: 10.8s\tremaining: 3.06s\n",
      "780:\tlearn: 0.1578759\ttotal: 10.9s\tremaining: 3.04s\n",
      "781:\tlearn: 0.1578292\ttotal: 10.9s\tremaining: 3.03s\n",
      "782:\tlearn: 0.1577752\ttotal: 10.9s\tremaining: 3.02s\n",
      "783:\tlearn: 0.1576800\ttotal: 10.9s\tremaining: 3s\n",
      "784:\tlearn: 0.1576519\ttotal: 10.9s\tremaining: 2.99s\n",
      "785:\tlearn: 0.1576415\ttotal: 10.9s\tremaining: 2.98s\n",
      "786:\tlearn: 0.1575958\ttotal: 10.9s\tremaining: 2.96s\n",
      "787:\tlearn: 0.1575449\ttotal: 11s\tremaining: 2.95s\n",
      "788:\tlearn: 0.1574894\ttotal: 11s\tremaining: 2.93s\n",
      "789:\tlearn: 0.1574473\ttotal: 11s\tremaining: 2.92s\n",
      "790:\tlearn: 0.1573900\ttotal: 11s\tremaining: 2.9s\n",
      "791:\tlearn: 0.1573343\ttotal: 11s\tremaining: 2.89s\n",
      "792:\tlearn: 0.1573317\ttotal: 11s\tremaining: 2.88s\n",
      "793:\tlearn: 0.1572719\ttotal: 11s\tremaining: 2.87s\n",
      "794:\tlearn: 0.1572478\ttotal: 11.1s\tremaining: 2.85s\n",
      "795:\tlearn: 0.1571768\ttotal: 11.1s\tremaining: 2.84s\n",
      "796:\tlearn: 0.1571241\ttotal: 11.1s\tremaining: 2.82s\n",
      "797:\tlearn: 0.1570690\ttotal: 11.1s\tremaining: 2.81s\n",
      "798:\tlearn: 0.1570159\ttotal: 11.1s\tremaining: 2.8s\n",
      "799:\tlearn: 0.1569389\ttotal: 11.1s\tremaining: 2.78s\n",
      "800:\tlearn: 0.1568835\ttotal: 11.1s\tremaining: 2.77s\n",
      "801:\tlearn: 0.1568064\ttotal: 11.2s\tremaining: 2.75s\n",
      "802:\tlearn: 0.1567597\ttotal: 11.2s\tremaining: 2.74s\n",
      "803:\tlearn: 0.1566708\ttotal: 11.2s\tremaining: 2.73s\n",
      "804:\tlearn: 0.1566031\ttotal: 11.2s\tremaining: 2.71s\n",
      "805:\tlearn: 0.1565572\ttotal: 11.2s\tremaining: 2.7s\n",
      "806:\tlearn: 0.1565246\ttotal: 11.2s\tremaining: 2.69s\n",
      "807:\tlearn: 0.1564600\ttotal: 11.2s\tremaining: 2.67s\n",
      "808:\tlearn: 0.1564474\ttotal: 11.3s\tremaining: 2.66s\n",
      "809:\tlearn: 0.1564012\ttotal: 11.3s\tremaining: 2.64s\n",
      "810:\tlearn: 0.1563540\ttotal: 11.3s\tremaining: 2.63s\n",
      "811:\tlearn: 0.1563096\ttotal: 11.3s\tremaining: 2.62s\n",
      "812:\tlearn: 0.1562744\ttotal: 11.3s\tremaining: 2.6s\n",
      "813:\tlearn: 0.1562336\ttotal: 11.3s\tremaining: 2.59s\n",
      "814:\tlearn: 0.1562031\ttotal: 11.3s\tremaining: 2.57s\n",
      "815:\tlearn: 0.1561548\ttotal: 11.4s\tremaining: 2.56s\n",
      "816:\tlearn: 0.1560769\ttotal: 11.4s\tremaining: 2.55s\n",
      "817:\tlearn: 0.1560422\ttotal: 11.4s\tremaining: 2.53s\n",
      "818:\tlearn: 0.1560219\ttotal: 11.4s\tremaining: 2.52s\n",
      "819:\tlearn: 0.1559283\ttotal: 11.4s\tremaining: 2.5s\n",
      "820:\tlearn: 0.1558595\ttotal: 11.4s\tremaining: 2.49s\n",
      "821:\tlearn: 0.1557496\ttotal: 11.4s\tremaining: 2.48s\n",
      "822:\tlearn: 0.1556994\ttotal: 11.4s\tremaining: 2.46s\n",
      "823:\tlearn: 0.1556501\ttotal: 11.5s\tremaining: 2.45s\n",
      "824:\tlearn: 0.1556107\ttotal: 11.5s\tremaining: 2.43s\n",
      "825:\tlearn: 0.1555880\ttotal: 11.5s\tremaining: 2.42s\n",
      "826:\tlearn: 0.1554888\ttotal: 11.5s\tremaining: 2.41s\n",
      "827:\tlearn: 0.1554359\ttotal: 11.5s\tremaining: 2.39s\n",
      "828:\tlearn: 0.1553554\ttotal: 11.5s\tremaining: 2.38s\n",
      "829:\tlearn: 0.1553020\ttotal: 11.5s\tremaining: 2.36s\n",
      "830:\tlearn: 0.1552332\ttotal: 11.6s\tremaining: 2.35s\n",
      "831:\tlearn: 0.1551844\ttotal: 11.6s\tremaining: 2.33s\n",
      "832:\tlearn: 0.1551350\ttotal: 11.6s\tremaining: 2.32s\n",
      "833:\tlearn: 0.1550753\ttotal: 11.6s\tremaining: 2.31s\n",
      "834:\tlearn: 0.1550051\ttotal: 11.6s\tremaining: 2.29s\n",
      "835:\tlearn: 0.1549626\ttotal: 11.6s\tremaining: 2.28s\n",
      "836:\tlearn: 0.1549347\ttotal: 11.6s\tremaining: 2.27s\n",
      "837:\tlearn: 0.1548622\ttotal: 11.6s\tremaining: 2.25s\n",
      "838:\tlearn: 0.1548325\ttotal: 11.7s\tremaining: 2.24s\n",
      "839:\tlearn: 0.1547902\ttotal: 11.7s\tremaining: 2.22s\n",
      "840:\tlearn: 0.1547586\ttotal: 11.7s\tremaining: 2.21s\n",
      "841:\tlearn: 0.1547121\ttotal: 11.7s\tremaining: 2.19s\n",
      "842:\tlearn: 0.1546875\ttotal: 11.7s\tremaining: 2.18s\n",
      "843:\tlearn: 0.1546585\ttotal: 11.7s\tremaining: 2.17s\n",
      "844:\tlearn: 0.1546105\ttotal: 11.7s\tremaining: 2.15s\n",
      "845:\tlearn: 0.1545671\ttotal: 11.8s\tremaining: 2.14s\n",
      "846:\tlearn: 0.1545027\ttotal: 11.8s\tremaining: 2.13s\n",
      "847:\tlearn: 0.1544829\ttotal: 11.8s\tremaining: 2.11s\n",
      "848:\tlearn: 0.1544538\ttotal: 11.8s\tremaining: 2.1s\n",
      "849:\tlearn: 0.1544163\ttotal: 11.8s\tremaining: 2.08s\n",
      "850:\tlearn: 0.1543691\ttotal: 11.8s\tremaining: 2.07s\n",
      "851:\tlearn: 0.1543298\ttotal: 11.8s\tremaining: 2.06s\n",
      "852:\tlearn: 0.1543123\ttotal: 11.8s\tremaining: 2.04s\n",
      "853:\tlearn: 0.1542864\ttotal: 11.9s\tremaining: 2.03s\n",
      "854:\tlearn: 0.1542505\ttotal: 11.9s\tremaining: 2.01s\n",
      "855:\tlearn: 0.1542218\ttotal: 11.9s\tremaining: 2s\n",
      "856:\tlearn: 0.1541900\ttotal: 11.9s\tremaining: 1.98s\n",
      "857:\tlearn: 0.1541707\ttotal: 11.9s\tremaining: 1.97s\n",
      "858:\tlearn: 0.1541081\ttotal: 11.9s\tremaining: 1.96s\n",
      "859:\tlearn: 0.1540696\ttotal: 11.9s\tremaining: 1.94s\n",
      "860:\tlearn: 0.1540015\ttotal: 11.9s\tremaining: 1.93s\n",
      "861:\tlearn: 0.1539632\ttotal: 12s\tremaining: 1.92s\n",
      "862:\tlearn: 0.1539083\ttotal: 12s\tremaining: 1.9s\n",
      "863:\tlearn: 0.1538733\ttotal: 12s\tremaining: 1.89s\n",
      "864:\tlearn: 0.1538163\ttotal: 12s\tremaining: 1.87s\n",
      "865:\tlearn: 0.1537750\ttotal: 12s\tremaining: 1.86s\n",
      "866:\tlearn: 0.1537376\ttotal: 12s\tremaining: 1.84s\n",
      "867:\tlearn: 0.1537073\ttotal: 12s\tremaining: 1.83s\n",
      "868:\tlearn: 0.1536593\ttotal: 12.1s\tremaining: 1.82s\n",
      "869:\tlearn: 0.1535861\ttotal: 12.1s\tremaining: 1.8s\n",
      "870:\tlearn: 0.1535610\ttotal: 12.1s\tremaining: 1.79s\n",
      "871:\tlearn: 0.1535056\ttotal: 12.1s\tremaining: 1.77s\n",
      "872:\tlearn: 0.1534855\ttotal: 12.1s\tremaining: 1.76s\n",
      "873:\tlearn: 0.1534167\ttotal: 12.1s\tremaining: 1.75s\n",
      "874:\tlearn: 0.1533993\ttotal: 12.1s\tremaining: 1.73s\n",
      "875:\tlearn: 0.1533564\ttotal: 12.2s\tremaining: 1.72s\n",
      "876:\tlearn: 0.1532808\ttotal: 12.2s\tremaining: 1.71s\n",
      "877:\tlearn: 0.1531514\ttotal: 12.2s\tremaining: 1.69s\n",
      "878:\tlearn: 0.1531287\ttotal: 12.2s\tremaining: 1.68s\n",
      "879:\tlearn: 0.1531139\ttotal: 12.2s\tremaining: 1.66s\n",
      "880:\tlearn: 0.1530948\ttotal: 12.2s\tremaining: 1.65s\n",
      "881:\tlearn: 0.1530767\ttotal: 12.2s\tremaining: 1.64s\n",
      "882:\tlearn: 0.1530241\ttotal: 12.2s\tremaining: 1.62s\n",
      "883:\tlearn: 0.1529674\ttotal: 12.3s\tremaining: 1.61s\n",
      "884:\tlearn: 0.1529306\ttotal: 12.3s\tremaining: 1.59s\n",
      "885:\tlearn: 0.1528806\ttotal: 12.3s\tremaining: 1.58s\n",
      "886:\tlearn: 0.1528498\ttotal: 12.3s\tremaining: 1.57s\n",
      "887:\tlearn: 0.1528278\ttotal: 12.3s\tremaining: 1.55s\n",
      "888:\tlearn: 0.1527967\ttotal: 12.3s\tremaining: 1.54s\n",
      "889:\tlearn: 0.1527326\ttotal: 12.3s\tremaining: 1.52s\n",
      "890:\tlearn: 0.1526985\ttotal: 12.4s\tremaining: 1.51s\n",
      "891:\tlearn: 0.1526723\ttotal: 12.4s\tremaining: 1.5s\n",
      "892:\tlearn: 0.1526196\ttotal: 12.4s\tremaining: 1.48s\n",
      "893:\tlearn: 0.1525933\ttotal: 12.4s\tremaining: 1.47s\n",
      "894:\tlearn: 0.1525485\ttotal: 12.4s\tremaining: 1.46s\n",
      "895:\tlearn: 0.1525063\ttotal: 12.4s\tremaining: 1.44s\n",
      "896:\tlearn: 0.1524577\ttotal: 12.4s\tremaining: 1.43s\n",
      "897:\tlearn: 0.1524011\ttotal: 12.4s\tremaining: 1.41s\n",
      "898:\tlearn: 0.1523512\ttotal: 12.5s\tremaining: 1.4s\n",
      "899:\tlearn: 0.1523429\ttotal: 12.5s\tremaining: 1.39s\n",
      "900:\tlearn: 0.1522988\ttotal: 12.5s\tremaining: 1.37s\n",
      "901:\tlearn: 0.1522455\ttotal: 12.5s\tremaining: 1.36s\n",
      "902:\tlearn: 0.1522105\ttotal: 12.5s\tremaining: 1.34s\n",
      "903:\tlearn: 0.1521569\ttotal: 12.5s\tremaining: 1.33s\n",
      "904:\tlearn: 0.1521155\ttotal: 12.5s\tremaining: 1.32s\n",
      "905:\tlearn: 0.1520831\ttotal: 12.6s\tremaining: 1.3s\n",
      "906:\tlearn: 0.1520489\ttotal: 12.6s\tremaining: 1.29s\n",
      "907:\tlearn: 0.1519672\ttotal: 12.6s\tremaining: 1.27s\n",
      "908:\tlearn: 0.1519344\ttotal: 12.6s\tremaining: 1.26s\n",
      "909:\tlearn: 0.1518934\ttotal: 12.6s\tremaining: 1.25s\n",
      "910:\tlearn: 0.1518551\ttotal: 12.6s\tremaining: 1.23s\n",
      "911:\tlearn: 0.1518213\ttotal: 12.6s\tremaining: 1.22s\n",
      "912:\tlearn: 0.1517776\ttotal: 12.6s\tremaining: 1.21s\n",
      "913:\tlearn: 0.1517334\ttotal: 12.7s\tremaining: 1.19s\n",
      "914:\tlearn: 0.1517108\ttotal: 12.7s\tremaining: 1.18s\n",
      "915:\tlearn: 0.1516236\ttotal: 12.7s\tremaining: 1.16s\n",
      "916:\tlearn: 0.1515482\ttotal: 12.7s\tremaining: 1.15s\n",
      "917:\tlearn: 0.1514963\ttotal: 12.7s\tremaining: 1.14s\n",
      "918:\tlearn: 0.1514718\ttotal: 12.7s\tremaining: 1.12s\n",
      "919:\tlearn: 0.1514605\ttotal: 12.7s\tremaining: 1.11s\n",
      "920:\tlearn: 0.1514191\ttotal: 12.8s\tremaining: 1.09s\n",
      "921:\tlearn: 0.1513453\ttotal: 12.8s\tremaining: 1.08s\n",
      "922:\tlearn: 0.1512667\ttotal: 12.8s\tremaining: 1.07s\n",
      "923:\tlearn: 0.1512163\ttotal: 12.8s\tremaining: 1.05s\n",
      "924:\tlearn: 0.1511897\ttotal: 12.8s\tremaining: 1.04s\n",
      "925:\tlearn: 0.1511494\ttotal: 12.8s\tremaining: 1.03s\n",
      "926:\tlearn: 0.1511122\ttotal: 12.9s\tremaining: 1.01s\n",
      "927:\tlearn: 0.1510756\ttotal: 12.9s\tremaining: 998ms\n",
      "928:\tlearn: 0.1510387\ttotal: 12.9s\tremaining: 984ms\n",
      "929:\tlearn: 0.1510109\ttotal: 12.9s\tremaining: 970ms\n",
      "930:\tlearn: 0.1509546\ttotal: 12.9s\tremaining: 957ms\n",
      "931:\tlearn: 0.1509021\ttotal: 12.9s\tremaining: 943ms\n",
      "932:\tlearn: 0.1508499\ttotal: 12.9s\tremaining: 929ms\n",
      "933:\tlearn: 0.1508322\ttotal: 12.9s\tremaining: 915ms\n",
      "934:\tlearn: 0.1507933\ttotal: 13s\tremaining: 901ms\n",
      "935:\tlearn: 0.1507690\ttotal: 13s\tremaining: 887ms\n",
      "936:\tlearn: 0.1507331\ttotal: 13s\tremaining: 873ms\n",
      "937:\tlearn: 0.1506690\ttotal: 13s\tremaining: 860ms\n",
      "938:\tlearn: 0.1506429\ttotal: 13s\tremaining: 846ms\n",
      "939:\tlearn: 0.1506057\ttotal: 13s\tremaining: 832ms\n",
      "940:\tlearn: 0.1505708\ttotal: 13s\tremaining: 818ms\n",
      "941:\tlearn: 0.1505249\ttotal: 13.1s\tremaining: 804ms\n",
      "942:\tlearn: 0.1504863\ttotal: 13.1s\tremaining: 790ms\n",
      "943:\tlearn: 0.1504589\ttotal: 13.1s\tremaining: 776ms\n",
      "944:\tlearn: 0.1504088\ttotal: 13.1s\tremaining: 762ms\n",
      "945:\tlearn: 0.1503657\ttotal: 13.1s\tremaining: 748ms\n",
      "946:\tlearn: 0.1503003\ttotal: 13.1s\tremaining: 734ms\n",
      "947:\tlearn: 0.1502696\ttotal: 13.1s\tremaining: 720ms\n",
      "948:\tlearn: 0.1502404\ttotal: 13.1s\tremaining: 707ms\n",
      "949:\tlearn: 0.1501820\ttotal: 13.2s\tremaining: 693ms\n",
      "950:\tlearn: 0.1501749\ttotal: 13.2s\tremaining: 679ms\n",
      "951:\tlearn: 0.1501415\ttotal: 13.2s\tremaining: 665ms\n",
      "952:\tlearn: 0.1501162\ttotal: 13.2s\tremaining: 651ms\n",
      "953:\tlearn: 0.1500363\ttotal: 13.2s\tremaining: 637ms\n",
      "954:\tlearn: 0.1499748\ttotal: 13.2s\tremaining: 623ms\n",
      "955:\tlearn: 0.1499479\ttotal: 13.2s\tremaining: 609ms\n",
      "956:\tlearn: 0.1498984\ttotal: 13.3s\tremaining: 595ms\n",
      "957:\tlearn: 0.1498342\ttotal: 13.3s\tremaining: 582ms\n",
      "958:\tlearn: 0.1498073\ttotal: 13.3s\tremaining: 568ms\n",
      "959:\tlearn: 0.1497500\ttotal: 13.3s\tremaining: 554ms\n",
      "960:\tlearn: 0.1497152\ttotal: 13.3s\tremaining: 540ms\n",
      "961:\tlearn: 0.1496685\ttotal: 13.3s\tremaining: 526ms\n",
      "962:\tlearn: 0.1496427\ttotal: 13.3s\tremaining: 512ms\n",
      "963:\tlearn: 0.1495980\ttotal: 13.3s\tremaining: 498ms\n",
      "964:\tlearn: 0.1495661\ttotal: 13.4s\tremaining: 484ms\n",
      "965:\tlearn: 0.1495169\ttotal: 13.4s\tremaining: 471ms\n",
      "966:\tlearn: 0.1494853\ttotal: 13.4s\tremaining: 457ms\n",
      "967:\tlearn: 0.1494434\ttotal: 13.4s\tremaining: 443ms\n",
      "968:\tlearn: 0.1494178\ttotal: 13.4s\tremaining: 429ms\n",
      "969:\tlearn: 0.1493872\ttotal: 13.4s\tremaining: 415ms\n",
      "970:\tlearn: 0.1493410\ttotal: 13.4s\tremaining: 401ms\n",
      "971:\tlearn: 0.1492929\ttotal: 13.4s\tremaining: 387ms\n",
      "972:\tlearn: 0.1492652\ttotal: 13.5s\tremaining: 374ms\n",
      "973:\tlearn: 0.1492182\ttotal: 13.5s\tremaining: 360ms\n",
      "974:\tlearn: 0.1491895\ttotal: 13.5s\tremaining: 346ms\n",
      "975:\tlearn: 0.1491221\ttotal: 13.5s\tremaining: 332ms\n",
      "976:\tlearn: 0.1491065\ttotal: 13.5s\tremaining: 318ms\n",
      "977:\tlearn: 0.1490891\ttotal: 13.5s\tremaining: 304ms\n",
      "978:\tlearn: 0.1490325\ttotal: 13.5s\tremaining: 290ms\n",
      "979:\tlearn: 0.1490008\ttotal: 13.6s\tremaining: 277ms\n",
      "980:\tlearn: 0.1489656\ttotal: 13.6s\tremaining: 263ms\n",
      "981:\tlearn: 0.1488896\ttotal: 13.6s\tremaining: 249ms\n",
      "982:\tlearn: 0.1488720\ttotal: 13.6s\tremaining: 235ms\n",
      "983:\tlearn: 0.1488424\ttotal: 13.6s\tremaining: 222ms\n",
      "984:\tlearn: 0.1488357\ttotal: 13.6s\tremaining: 208ms\n",
      "985:\tlearn: 0.1487913\ttotal: 13.7s\tremaining: 194ms\n",
      "986:\tlearn: 0.1487519\ttotal: 13.7s\tremaining: 180ms\n",
      "987:\tlearn: 0.1487263\ttotal: 13.7s\tremaining: 166ms\n",
      "988:\tlearn: 0.1487066\ttotal: 13.7s\tremaining: 152ms\n",
      "989:\tlearn: 0.1486524\ttotal: 13.7s\tremaining: 139ms\n",
      "990:\tlearn: 0.1486085\ttotal: 13.7s\tremaining: 125ms\n",
      "991:\tlearn: 0.1485790\ttotal: 13.7s\tremaining: 111ms\n",
      "992:\tlearn: 0.1485275\ttotal: 13.8s\tremaining: 97ms\n",
      "993:\tlearn: 0.1484930\ttotal: 13.8s\tremaining: 83.2ms\n",
      "994:\tlearn: 0.1484555\ttotal: 13.8s\tremaining: 69.3ms\n",
      "995:\tlearn: 0.1484287\ttotal: 13.8s\tremaining: 55.4ms\n",
      "996:\tlearn: 0.1483858\ttotal: 13.8s\tremaining: 41.6ms\n",
      "997:\tlearn: 0.1483548\ttotal: 13.8s\tremaining: 27.7ms\n",
      "998:\tlearn: 0.1483165\ttotal: 13.8s\tremaining: 13.9ms\n",
      "999:\tlearn: 0.1482761\ttotal: 13.9s\tremaining: 0us\n",
      "Learning rate set to 0.070523\n",
      "0:\tlearn: 0.6299456\ttotal: 18.1ms\tremaining: 18.1s\n",
      "1:\tlearn: 0.6017164\ttotal: 32.8ms\tremaining: 16.4s\n",
      "2:\tlearn: 0.5864909\ttotal: 46.5ms\tremaining: 15.5s\n",
      "3:\tlearn: 0.5610559\ttotal: 60.4ms\tremaining: 15s\n",
      "4:\tlearn: 0.5363244\ttotal: 75.4ms\tremaining: 15s\n",
      "5:\tlearn: 0.5209246\ttotal: 89.3ms\tremaining: 14.8s\n",
      "6:\tlearn: 0.5064597\ttotal: 102ms\tremaining: 14.5s\n",
      "7:\tlearn: 0.4967958\ttotal: 116ms\tremaining: 14.4s\n",
      "8:\tlearn: 0.4889932\ttotal: 129ms\tremaining: 14.2s\n",
      "9:\tlearn: 0.4803863\ttotal: 142ms\tremaining: 14.1s\n",
      "10:\tlearn: 0.4766792\ttotal: 155ms\tremaining: 13.9s\n",
      "11:\tlearn: 0.4544720\ttotal: 193ms\tremaining: 15.9s\n",
      "12:\tlearn: 0.4463965\ttotal: 209ms\tremaining: 15.9s\n",
      "13:\tlearn: 0.4406605\ttotal: 223ms\tremaining: 15.7s\n",
      "14:\tlearn: 0.4353542\ttotal: 237ms\tremaining: 15.6s\n",
      "15:\tlearn: 0.4303844\ttotal: 251ms\tremaining: 15.4s\n",
      "16:\tlearn: 0.4182335\ttotal: 264ms\tremaining: 15.3s\n",
      "17:\tlearn: 0.4131949\ttotal: 277ms\tremaining: 15.1s\n",
      "18:\tlearn: 0.4044171\ttotal: 291ms\tremaining: 15s\n",
      "19:\tlearn: 0.3998521\ttotal: 303ms\tremaining: 14.9s\n",
      "20:\tlearn: 0.3952735\ttotal: 315ms\tremaining: 14.7s\n",
      "21:\tlearn: 0.3919778\ttotal: 328ms\tremaining: 14.6s\n",
      "22:\tlearn: 0.3880878\ttotal: 343ms\tremaining: 14.6s\n",
      "23:\tlearn: 0.3848557\ttotal: 357ms\tremaining: 14.5s\n",
      "24:\tlearn: 0.3812885\ttotal: 370ms\tremaining: 14.4s\n",
      "25:\tlearn: 0.3787485\ttotal: 383ms\tremaining: 14.3s\n",
      "26:\tlearn: 0.3761348\ttotal: 396ms\tremaining: 14.3s\n",
      "27:\tlearn: 0.3722163\ttotal: 409ms\tremaining: 14.2s\n",
      "28:\tlearn: 0.3694691\ttotal: 421ms\tremaining: 14.1s\n",
      "29:\tlearn: 0.3641942\ttotal: 434ms\tremaining: 14s\n",
      "30:\tlearn: 0.3565108\ttotal: 448ms\tremaining: 14s\n",
      "31:\tlearn: 0.3523537\ttotal: 461ms\tremaining: 14s\n",
      "32:\tlearn: 0.3494635\ttotal: 474ms\tremaining: 13.9s\n",
      "33:\tlearn: 0.3463984\ttotal: 487ms\tremaining: 13.8s\n",
      "34:\tlearn: 0.3440804\ttotal: 501ms\tremaining: 13.8s\n",
      "35:\tlearn: 0.3429848\ttotal: 514ms\tremaining: 13.8s\n",
      "36:\tlearn: 0.3389430\ttotal: 527ms\tremaining: 13.7s\n",
      "37:\tlearn: 0.3373772\ttotal: 540ms\tremaining: 13.7s\n",
      "38:\tlearn: 0.3349875\ttotal: 553ms\tremaining: 13.6s\n",
      "39:\tlearn: 0.3279519\ttotal: 567ms\tremaining: 13.6s\n",
      "40:\tlearn: 0.3254304\ttotal: 580ms\tremaining: 13.6s\n",
      "41:\tlearn: 0.3209010\ttotal: 592ms\tremaining: 13.5s\n",
      "42:\tlearn: 0.3167229\ttotal: 605ms\tremaining: 13.5s\n",
      "43:\tlearn: 0.3132894\ttotal: 618ms\tremaining: 13.4s\n",
      "44:\tlearn: 0.3117875\ttotal: 631ms\tremaining: 13.4s\n",
      "45:\tlearn: 0.3090266\ttotal: 643ms\tremaining: 13.3s\n",
      "46:\tlearn: 0.3070721\ttotal: 658ms\tremaining: 13.3s\n",
      "47:\tlearn: 0.3061905\ttotal: 680ms\tremaining: 13.5s\n",
      "48:\tlearn: 0.3031983\ttotal: 705ms\tremaining: 13.7s\n",
      "49:\tlearn: 0.3013845\ttotal: 730ms\tremaining: 13.9s\n",
      "50:\tlearn: 0.2984383\ttotal: 748ms\tremaining: 13.9s\n",
      "51:\tlearn: 0.2971461\ttotal: 769ms\tremaining: 14s\n",
      "52:\tlearn: 0.2953636\ttotal: 782ms\tremaining: 14s\n",
      "53:\tlearn: 0.2934368\ttotal: 795ms\tremaining: 13.9s\n",
      "54:\tlearn: 0.2923768\ttotal: 809ms\tremaining: 13.9s\n",
      "55:\tlearn: 0.2918493\ttotal: 821ms\tremaining: 13.8s\n",
      "56:\tlearn: 0.2913003\ttotal: 835ms\tremaining: 13.8s\n",
      "57:\tlearn: 0.2875261\ttotal: 849ms\tremaining: 13.8s\n",
      "58:\tlearn: 0.2847335\ttotal: 864ms\tremaining: 13.8s\n",
      "59:\tlearn: 0.2835425\ttotal: 878ms\tremaining: 13.7s\n",
      "60:\tlearn: 0.2830765\ttotal: 892ms\tremaining: 13.7s\n",
      "61:\tlearn: 0.2809373\ttotal: 904ms\tremaining: 13.7s\n",
      "62:\tlearn: 0.2804752\ttotal: 917ms\tremaining: 13.6s\n",
      "63:\tlearn: 0.2794396\ttotal: 930ms\tremaining: 13.6s\n",
      "64:\tlearn: 0.2785686\ttotal: 943ms\tremaining: 13.6s\n",
      "65:\tlearn: 0.2753400\ttotal: 956ms\tremaining: 13.5s\n",
      "66:\tlearn: 0.2742260\ttotal: 970ms\tremaining: 13.5s\n",
      "67:\tlearn: 0.2726077\ttotal: 983ms\tremaining: 13.5s\n",
      "68:\tlearn: 0.2721136\ttotal: 998ms\tremaining: 13.5s\n",
      "69:\tlearn: 0.2691343\ttotal: 1.01s\tremaining: 13.5s\n",
      "70:\tlearn: 0.2683168\ttotal: 1.03s\tremaining: 13.4s\n",
      "71:\tlearn: 0.2676758\ttotal: 1.04s\tremaining: 13.4s\n",
      "72:\tlearn: 0.2650123\ttotal: 1.05s\tremaining: 13.4s\n",
      "73:\tlearn: 0.2647068\ttotal: 1.07s\tremaining: 13.4s\n",
      "74:\tlearn: 0.2638951\ttotal: 1.09s\tremaining: 13.4s\n",
      "75:\tlearn: 0.2632749\ttotal: 1.11s\tremaining: 13.5s\n",
      "76:\tlearn: 0.2605534\ttotal: 1.12s\tremaining: 13.5s\n",
      "77:\tlearn: 0.2601364\ttotal: 1.14s\tremaining: 13.4s\n",
      "78:\tlearn: 0.2597854\ttotal: 1.15s\tremaining: 13.4s\n",
      "79:\tlearn: 0.2588259\ttotal: 1.16s\tremaining: 13.4s\n",
      "80:\tlearn: 0.2583037\ttotal: 1.18s\tremaining: 13.4s\n",
      "81:\tlearn: 0.2577410\ttotal: 1.19s\tremaining: 13.3s\n",
      "82:\tlearn: 0.2553782\ttotal: 1.2s\tremaining: 13.3s\n",
      "83:\tlearn: 0.2544079\ttotal: 1.22s\tremaining: 13.3s\n",
      "84:\tlearn: 0.2537391\ttotal: 1.23s\tremaining: 13.2s\n",
      "85:\tlearn: 0.2531988\ttotal: 1.24s\tremaining: 13.2s\n",
      "86:\tlearn: 0.2528619\ttotal: 1.26s\tremaining: 13.2s\n",
      "87:\tlearn: 0.2517343\ttotal: 1.31s\tremaining: 13.6s\n",
      "88:\tlearn: 0.2512270\ttotal: 1.33s\tremaining: 13.6s\n",
      "89:\tlearn: 0.2506442\ttotal: 1.35s\tremaining: 13.6s\n",
      "90:\tlearn: 0.2497913\ttotal: 1.36s\tremaining: 13.6s\n",
      "91:\tlearn: 0.2495661\ttotal: 1.38s\tremaining: 13.6s\n",
      "92:\tlearn: 0.2491397\ttotal: 1.39s\tremaining: 13.5s\n",
      "93:\tlearn: 0.2485497\ttotal: 1.4s\tremaining: 13.5s\n",
      "94:\tlearn: 0.2481832\ttotal: 1.42s\tremaining: 13.5s\n",
      "95:\tlearn: 0.2479707\ttotal: 1.43s\tremaining: 13.5s\n",
      "96:\tlearn: 0.2477511\ttotal: 1.45s\tremaining: 13.5s\n",
      "97:\tlearn: 0.2475458\ttotal: 1.46s\tremaining: 13.4s\n",
      "98:\tlearn: 0.2472009\ttotal: 1.47s\tremaining: 13.4s\n",
      "99:\tlearn: 0.2464977\ttotal: 1.49s\tremaining: 13.4s\n",
      "100:\tlearn: 0.2461271\ttotal: 1.5s\tremaining: 13.4s\n",
      "101:\tlearn: 0.2456162\ttotal: 1.51s\tremaining: 13.3s\n",
      "102:\tlearn: 0.2451422\ttotal: 1.53s\tremaining: 13.3s\n",
      "103:\tlearn: 0.2446791\ttotal: 1.54s\tremaining: 13.3s\n",
      "104:\tlearn: 0.2442409\ttotal: 1.55s\tremaining: 13.2s\n",
      "105:\tlearn: 0.2430173\ttotal: 1.56s\tremaining: 13.2s\n",
      "106:\tlearn: 0.2428084\ttotal: 1.58s\tremaining: 13.2s\n",
      "107:\tlearn: 0.2426512\ttotal: 1.59s\tremaining: 13.1s\n",
      "108:\tlearn: 0.2411377\ttotal: 1.6s\tremaining: 13.1s\n",
      "109:\tlearn: 0.2409320\ttotal: 1.62s\tremaining: 13.1s\n",
      "110:\tlearn: 0.2406982\ttotal: 1.63s\tremaining: 13.1s\n",
      "111:\tlearn: 0.2400940\ttotal: 1.64s\tremaining: 13s\n",
      "112:\tlearn: 0.2394970\ttotal: 1.66s\tremaining: 13s\n",
      "113:\tlearn: 0.2390723\ttotal: 1.67s\tremaining: 13s\n",
      "114:\tlearn: 0.2387496\ttotal: 1.69s\tremaining: 13s\n",
      "115:\tlearn: 0.2384361\ttotal: 1.7s\tremaining: 13s\n",
      "116:\tlearn: 0.2369099\ttotal: 1.71s\tremaining: 12.9s\n",
      "117:\tlearn: 0.2364322\ttotal: 1.72s\tremaining: 12.9s\n",
      "118:\tlearn: 0.2362762\ttotal: 1.74s\tremaining: 12.9s\n",
      "119:\tlearn: 0.2361260\ttotal: 1.75s\tremaining: 12.9s\n",
      "120:\tlearn: 0.2347535\ttotal: 1.77s\tremaining: 12.8s\n",
      "121:\tlearn: 0.2344126\ttotal: 1.78s\tremaining: 12.8s\n",
      "122:\tlearn: 0.2338201\ttotal: 1.79s\tremaining: 12.8s\n",
      "123:\tlearn: 0.2333583\ttotal: 1.81s\tremaining: 12.8s\n",
      "124:\tlearn: 0.2321197\ttotal: 1.82s\tremaining: 12.7s\n",
      "125:\tlearn: 0.2314735\ttotal: 1.83s\tremaining: 12.7s\n",
      "126:\tlearn: 0.2312644\ttotal: 1.85s\tremaining: 12.7s\n",
      "127:\tlearn: 0.2311361\ttotal: 1.86s\tremaining: 12.7s\n",
      "128:\tlearn: 0.2309708\ttotal: 1.88s\tremaining: 12.7s\n",
      "129:\tlearn: 0.2307485\ttotal: 1.89s\tremaining: 12.7s\n",
      "130:\tlearn: 0.2302010\ttotal: 1.91s\tremaining: 12.6s\n",
      "131:\tlearn: 0.2300548\ttotal: 1.92s\tremaining: 12.6s\n",
      "132:\tlearn: 0.2298800\ttotal: 1.93s\tremaining: 12.6s\n",
      "133:\tlearn: 0.2293804\ttotal: 1.94s\tremaining: 12.6s\n",
      "134:\tlearn: 0.2291113\ttotal: 1.96s\tremaining: 12.5s\n",
      "135:\tlearn: 0.2288925\ttotal: 1.97s\tremaining: 12.5s\n",
      "136:\tlearn: 0.2285609\ttotal: 1.98s\tremaining: 12.5s\n",
      "137:\tlearn: 0.2280367\ttotal: 2s\tremaining: 12.5s\n",
      "138:\tlearn: 0.2269391\ttotal: 2.01s\tremaining: 12.5s\n",
      "139:\tlearn: 0.2260183\ttotal: 2.03s\tremaining: 12.4s\n",
      "140:\tlearn: 0.2258260\ttotal: 2.04s\tremaining: 12.4s\n",
      "141:\tlearn: 0.2256870\ttotal: 2.05s\tremaining: 12.4s\n",
      "142:\tlearn: 0.2255283\ttotal: 2.07s\tremaining: 12.4s\n",
      "143:\tlearn: 0.2253754\ttotal: 2.08s\tremaining: 12.4s\n",
      "144:\tlearn: 0.2251072\ttotal: 2.09s\tremaining: 12.4s\n",
      "145:\tlearn: 0.2241026\ttotal: 2.11s\tremaining: 12.3s\n",
      "146:\tlearn: 0.2237322\ttotal: 2.12s\tremaining: 12.3s\n",
      "147:\tlearn: 0.2230513\ttotal: 2.13s\tremaining: 12.3s\n",
      "148:\tlearn: 0.2228587\ttotal: 2.15s\tremaining: 12.3s\n",
      "149:\tlearn: 0.2222624\ttotal: 2.16s\tremaining: 12.2s\n",
      "150:\tlearn: 0.2221380\ttotal: 2.17s\tremaining: 12.2s\n",
      "151:\tlearn: 0.2218454\ttotal: 2.18s\tremaining: 12.2s\n",
      "152:\tlearn: 0.2216186\ttotal: 2.2s\tremaining: 12.2s\n",
      "153:\tlearn: 0.2208976\ttotal: 2.21s\tremaining: 12.1s\n",
      "154:\tlearn: 0.2206635\ttotal: 2.22s\tremaining: 12.1s\n",
      "155:\tlearn: 0.2205417\ttotal: 2.25s\tremaining: 12.2s\n",
      "156:\tlearn: 0.2203973\ttotal: 2.26s\tremaining: 12.2s\n",
      "157:\tlearn: 0.2200303\ttotal: 2.28s\tremaining: 12.1s\n",
      "158:\tlearn: 0.2198767\ttotal: 2.29s\tremaining: 12.1s\n",
      "159:\tlearn: 0.2191956\ttotal: 2.3s\tremaining: 12.1s\n",
      "160:\tlearn: 0.2190363\ttotal: 2.32s\tremaining: 12.1s\n",
      "161:\tlearn: 0.2185330\ttotal: 2.33s\tremaining: 12.1s\n",
      "162:\tlearn: 0.2183196\ttotal: 2.35s\tremaining: 12s\n",
      "163:\tlearn: 0.2181634\ttotal: 2.36s\tremaining: 12s\n",
      "164:\tlearn: 0.2178218\ttotal: 2.37s\tremaining: 12s\n",
      "165:\tlearn: 0.2176727\ttotal: 2.38s\tremaining: 12s\n",
      "166:\tlearn: 0.2173161\ttotal: 2.4s\tremaining: 12s\n",
      "167:\tlearn: 0.2170366\ttotal: 2.41s\tremaining: 11.9s\n",
      "168:\tlearn: 0.2169163\ttotal: 2.42s\tremaining: 11.9s\n",
      "169:\tlearn: 0.2166740\ttotal: 2.44s\tremaining: 11.9s\n",
      "170:\tlearn: 0.2164228\ttotal: 2.45s\tremaining: 11.9s\n",
      "171:\tlearn: 0.2161642\ttotal: 2.46s\tremaining: 11.9s\n",
      "172:\tlearn: 0.2159733\ttotal: 2.48s\tremaining: 11.8s\n",
      "173:\tlearn: 0.2157128\ttotal: 2.49s\tremaining: 11.8s\n",
      "174:\tlearn: 0.2151747\ttotal: 2.51s\tremaining: 11.8s\n",
      "175:\tlearn: 0.2149581\ttotal: 2.52s\tremaining: 11.8s\n",
      "176:\tlearn: 0.2145387\ttotal: 2.53s\tremaining: 11.8s\n",
      "177:\tlearn: 0.2140938\ttotal: 2.55s\tremaining: 11.8s\n",
      "178:\tlearn: 0.2139502\ttotal: 2.56s\tremaining: 11.7s\n",
      "179:\tlearn: 0.2136118\ttotal: 2.57s\tremaining: 11.7s\n",
      "180:\tlearn: 0.2134552\ttotal: 2.58s\tremaining: 11.7s\n",
      "181:\tlearn: 0.2132817\ttotal: 2.6s\tremaining: 11.7s\n",
      "182:\tlearn: 0.2129682\ttotal: 2.61s\tremaining: 11.7s\n",
      "183:\tlearn: 0.2128212\ttotal: 2.63s\tremaining: 11.7s\n",
      "184:\tlearn: 0.2126597\ttotal: 2.64s\tremaining: 11.6s\n",
      "185:\tlearn: 0.2125554\ttotal: 2.65s\tremaining: 11.6s\n",
      "186:\tlearn: 0.2121860\ttotal: 2.67s\tremaining: 11.6s\n",
      "187:\tlearn: 0.2120700\ttotal: 2.68s\tremaining: 11.6s\n",
      "188:\tlearn: 0.2119609\ttotal: 2.69s\tremaining: 11.6s\n",
      "189:\tlearn: 0.2114895\ttotal: 2.71s\tremaining: 11.5s\n",
      "190:\tlearn: 0.2113676\ttotal: 2.72s\tremaining: 11.5s\n",
      "191:\tlearn: 0.2112878\ttotal: 2.74s\tremaining: 11.5s\n",
      "192:\tlearn: 0.2110778\ttotal: 2.75s\tremaining: 11.5s\n",
      "193:\tlearn: 0.2108537\ttotal: 2.76s\tremaining: 11.5s\n",
      "194:\tlearn: 0.2105641\ttotal: 2.77s\tremaining: 11.5s\n",
      "195:\tlearn: 0.2104691\ttotal: 2.79s\tremaining: 11.4s\n",
      "196:\tlearn: 0.2100354\ttotal: 2.8s\tremaining: 11.4s\n",
      "197:\tlearn: 0.2096654\ttotal: 2.82s\tremaining: 11.4s\n",
      "198:\tlearn: 0.2095136\ttotal: 2.83s\tremaining: 11.4s\n",
      "199:\tlearn: 0.2090506\ttotal: 2.85s\tremaining: 11.4s\n",
      "200:\tlearn: 0.2083485\ttotal: 2.86s\tremaining: 11.4s\n",
      "201:\tlearn: 0.2081249\ttotal: 2.87s\tremaining: 11.4s\n",
      "202:\tlearn: 0.2078557\ttotal: 2.89s\tremaining: 11.3s\n",
      "203:\tlearn: 0.2074078\ttotal: 2.9s\tremaining: 11.3s\n",
      "204:\tlearn: 0.2072898\ttotal: 2.91s\tremaining: 11.3s\n",
      "205:\tlearn: 0.2071286\ttotal: 2.93s\tremaining: 11.3s\n",
      "206:\tlearn: 0.2069335\ttotal: 2.94s\tremaining: 11.3s\n",
      "207:\tlearn: 0.2066445\ttotal: 2.95s\tremaining: 11.2s\n",
      "208:\tlearn: 0.2061899\ttotal: 2.96s\tremaining: 11.2s\n",
      "209:\tlearn: 0.2058512\ttotal: 2.98s\tremaining: 11.2s\n",
      "210:\tlearn: 0.2057201\ttotal: 2.99s\tremaining: 11.2s\n",
      "211:\tlearn: 0.2056248\ttotal: 3s\tremaining: 11.2s\n",
      "212:\tlearn: 0.2053109\ttotal: 3.02s\tremaining: 11.2s\n",
      "213:\tlearn: 0.2050662\ttotal: 3.03s\tremaining: 11.1s\n",
      "214:\tlearn: 0.2049159\ttotal: 3.04s\tremaining: 11.1s\n",
      "215:\tlearn: 0.2047992\ttotal: 3.06s\tremaining: 11.1s\n",
      "216:\tlearn: 0.2046996\ttotal: 3.07s\tremaining: 11.1s\n",
      "217:\tlearn: 0.2045579\ttotal: 3.09s\tremaining: 11.1s\n",
      "218:\tlearn: 0.2042813\ttotal: 3.1s\tremaining: 11.1s\n",
      "219:\tlearn: 0.2036916\ttotal: 3.11s\tremaining: 11s\n",
      "220:\tlearn: 0.2034597\ttotal: 3.12s\tremaining: 11s\n",
      "221:\tlearn: 0.2031198\ttotal: 3.14s\tremaining: 11s\n",
      "222:\tlearn: 0.2029167\ttotal: 3.15s\tremaining: 11s\n",
      "223:\tlearn: 0.2026382\ttotal: 3.16s\tremaining: 11s\n",
      "224:\tlearn: 0.2025612\ttotal: 3.17s\tremaining: 10.9s\n",
      "225:\tlearn: 0.2024126\ttotal: 3.19s\tremaining: 10.9s\n",
      "226:\tlearn: 0.2023249\ttotal: 3.2s\tremaining: 10.9s\n",
      "227:\tlearn: 0.2022238\ttotal: 3.21s\tremaining: 10.9s\n",
      "228:\tlearn: 0.2018097\ttotal: 3.23s\tremaining: 10.9s\n",
      "229:\tlearn: 0.2016034\ttotal: 3.24s\tremaining: 10.9s\n",
      "230:\tlearn: 0.2014545\ttotal: 3.26s\tremaining: 10.9s\n",
      "231:\tlearn: 0.2012378\ttotal: 3.28s\tremaining: 10.8s\n",
      "232:\tlearn: 0.2010966\ttotal: 3.29s\tremaining: 10.8s\n",
      "233:\tlearn: 0.2007362\ttotal: 3.31s\tremaining: 10.8s\n",
      "234:\tlearn: 0.2006655\ttotal: 3.33s\tremaining: 10.8s\n",
      "235:\tlearn: 0.2005758\ttotal: 3.34s\tremaining: 10.8s\n",
      "236:\tlearn: 0.2003243\ttotal: 3.35s\tremaining: 10.8s\n",
      "237:\tlearn: 0.2002283\ttotal: 3.37s\tremaining: 10.8s\n",
      "238:\tlearn: 0.2001404\ttotal: 3.38s\tremaining: 10.8s\n",
      "239:\tlearn: 0.2000319\ttotal: 3.39s\tremaining: 10.7s\n",
      "240:\tlearn: 0.1999414\ttotal: 3.41s\tremaining: 10.7s\n",
      "241:\tlearn: 0.1996706\ttotal: 3.42s\tremaining: 10.7s\n",
      "242:\tlearn: 0.1991733\ttotal: 3.44s\tremaining: 10.7s\n",
      "243:\tlearn: 0.1990433\ttotal: 3.45s\tremaining: 10.7s\n",
      "244:\tlearn: 0.1989442\ttotal: 3.47s\tremaining: 10.7s\n",
      "245:\tlearn: 0.1988351\ttotal: 3.48s\tremaining: 10.7s\n",
      "246:\tlearn: 0.1987575\ttotal: 3.49s\tremaining: 10.6s\n",
      "247:\tlearn: 0.1985912\ttotal: 3.5s\tremaining: 10.6s\n",
      "248:\tlearn: 0.1984829\ttotal: 3.52s\tremaining: 10.6s\n",
      "249:\tlearn: 0.1983943\ttotal: 3.53s\tremaining: 10.6s\n",
      "250:\tlearn: 0.1980346\ttotal: 3.54s\tremaining: 10.6s\n",
      "251:\tlearn: 0.1977649\ttotal: 3.56s\tremaining: 10.6s\n",
      "252:\tlearn: 0.1976035\ttotal: 3.57s\tremaining: 10.5s\n",
      "253:\tlearn: 0.1973798\ttotal: 3.58s\tremaining: 10.5s\n",
      "254:\tlearn: 0.1971889\ttotal: 3.59s\tremaining: 10.5s\n",
      "255:\tlearn: 0.1970206\ttotal: 3.61s\tremaining: 10.5s\n",
      "256:\tlearn: 0.1967620\ttotal: 3.62s\tremaining: 10.5s\n",
      "257:\tlearn: 0.1965512\ttotal: 3.63s\tremaining: 10.5s\n",
      "258:\tlearn: 0.1964589\ttotal: 3.65s\tremaining: 10.4s\n",
      "259:\tlearn: 0.1961846\ttotal: 3.66s\tremaining: 10.4s\n",
      "260:\tlearn: 0.1961236\ttotal: 3.67s\tremaining: 10.4s\n",
      "261:\tlearn: 0.1960498\ttotal: 3.69s\tremaining: 10.4s\n",
      "262:\tlearn: 0.1957883\ttotal: 3.7s\tremaining: 10.4s\n",
      "263:\tlearn: 0.1955708\ttotal: 3.71s\tremaining: 10.4s\n",
      "264:\tlearn: 0.1955102\ttotal: 3.73s\tremaining: 10.3s\n",
      "265:\tlearn: 0.1954022\ttotal: 3.74s\tremaining: 10.3s\n",
      "266:\tlearn: 0.1953182\ttotal: 3.75s\tremaining: 10.3s\n",
      "267:\tlearn: 0.1952494\ttotal: 3.77s\tremaining: 10.3s\n",
      "268:\tlearn: 0.1951118\ttotal: 3.78s\tremaining: 10.3s\n",
      "269:\tlearn: 0.1950043\ttotal: 3.79s\tremaining: 10.3s\n",
      "270:\tlearn: 0.1946651\ttotal: 3.81s\tremaining: 10.2s\n",
      "271:\tlearn: 0.1945644\ttotal: 3.82s\tremaining: 10.2s\n",
      "272:\tlearn: 0.1944291\ttotal: 3.83s\tremaining: 10.2s\n",
      "273:\tlearn: 0.1943356\ttotal: 3.85s\tremaining: 10.2s\n",
      "274:\tlearn: 0.1942291\ttotal: 3.86s\tremaining: 10.2s\n",
      "275:\tlearn: 0.1941495\ttotal: 3.87s\tremaining: 10.2s\n",
      "276:\tlearn: 0.1938594\ttotal: 3.89s\tremaining: 10.1s\n",
      "277:\tlearn: 0.1937910\ttotal: 3.92s\tremaining: 10.2s\n",
      "278:\tlearn: 0.1937147\ttotal: 3.94s\tremaining: 10.2s\n",
      "279:\tlearn: 0.1936609\ttotal: 3.95s\tremaining: 10.2s\n",
      "280:\tlearn: 0.1934650\ttotal: 3.96s\tremaining: 10.1s\n",
      "281:\tlearn: 0.1933359\ttotal: 3.98s\tremaining: 10.1s\n",
      "282:\tlearn: 0.1932137\ttotal: 3.99s\tremaining: 10.1s\n",
      "283:\tlearn: 0.1931421\ttotal: 4s\tremaining: 10.1s\n",
      "284:\tlearn: 0.1929541\ttotal: 4.02s\tremaining: 10.1s\n",
      "285:\tlearn: 0.1928183\ttotal: 4.03s\tremaining: 10.1s\n",
      "286:\tlearn: 0.1927424\ttotal: 4.05s\tremaining: 10.1s\n",
      "287:\tlearn: 0.1926658\ttotal: 4.06s\tremaining: 10s\n",
      "288:\tlearn: 0.1925512\ttotal: 4.07s\tremaining: 10s\n",
      "289:\tlearn: 0.1924888\ttotal: 4.09s\tremaining: 10s\n",
      "290:\tlearn: 0.1923000\ttotal: 4.1s\tremaining: 9.99s\n",
      "291:\tlearn: 0.1922346\ttotal: 4.11s\tremaining: 9.97s\n",
      "292:\tlearn: 0.1919759\ttotal: 4.13s\tremaining: 9.96s\n",
      "293:\tlearn: 0.1918926\ttotal: 4.14s\tremaining: 9.94s\n",
      "294:\tlearn: 0.1918251\ttotal: 4.15s\tremaining: 9.92s\n",
      "295:\tlearn: 0.1917412\ttotal: 4.16s\tremaining: 9.9s\n",
      "296:\tlearn: 0.1916592\ttotal: 4.17s\tremaining: 9.88s\n",
      "297:\tlearn: 0.1915984\ttotal: 4.19s\tremaining: 9.87s\n",
      "298:\tlearn: 0.1915002\ttotal: 4.2s\tremaining: 9.85s\n",
      "299:\tlearn: 0.1914535\ttotal: 4.21s\tremaining: 9.83s\n",
      "300:\tlearn: 0.1913418\ttotal: 4.23s\tremaining: 9.82s\n",
      "301:\tlearn: 0.1913064\ttotal: 4.24s\tremaining: 9.8s\n",
      "302:\tlearn: 0.1912489\ttotal: 4.25s\tremaining: 9.79s\n",
      "303:\tlearn: 0.1910561\ttotal: 4.27s\tremaining: 9.77s\n",
      "304:\tlearn: 0.1907436\ttotal: 4.28s\tremaining: 9.76s\n",
      "305:\tlearn: 0.1905716\ttotal: 4.29s\tremaining: 9.74s\n",
      "306:\tlearn: 0.1904469\ttotal: 4.31s\tremaining: 9.73s\n",
      "307:\tlearn: 0.1903808\ttotal: 4.32s\tremaining: 9.71s\n",
      "308:\tlearn: 0.1902457\ttotal: 4.33s\tremaining: 9.7s\n",
      "309:\tlearn: 0.1899504\ttotal: 4.35s\tremaining: 9.68s\n",
      "310:\tlearn: 0.1898924\ttotal: 4.36s\tremaining: 9.66s\n",
      "311:\tlearn: 0.1898496\ttotal: 4.37s\tremaining: 9.65s\n",
      "312:\tlearn: 0.1897467\ttotal: 4.39s\tremaining: 9.63s\n",
      "313:\tlearn: 0.1896770\ttotal: 4.4s\tremaining: 9.61s\n",
      "314:\tlearn: 0.1895697\ttotal: 4.41s\tremaining: 9.6s\n",
      "315:\tlearn: 0.1894932\ttotal: 4.43s\tremaining: 9.58s\n",
      "316:\tlearn: 0.1894351\ttotal: 4.44s\tremaining: 9.57s\n",
      "317:\tlearn: 0.1893417\ttotal: 4.45s\tremaining: 9.55s\n",
      "318:\tlearn: 0.1892653\ttotal: 4.47s\tremaining: 9.54s\n",
      "319:\tlearn: 0.1891944\ttotal: 4.48s\tremaining: 9.52s\n",
      "320:\tlearn: 0.1890130\ttotal: 4.49s\tremaining: 9.51s\n",
      "321:\tlearn: 0.1889555\ttotal: 4.51s\tremaining: 9.49s\n",
      "322:\tlearn: 0.1887958\ttotal: 4.52s\tremaining: 9.47s\n",
      "323:\tlearn: 0.1886564\ttotal: 4.53s\tremaining: 9.45s\n",
      "324:\tlearn: 0.1885066\ttotal: 4.54s\tremaining: 9.44s\n",
      "325:\tlearn: 0.1884495\ttotal: 4.56s\tremaining: 9.42s\n",
      "326:\tlearn: 0.1883531\ttotal: 4.57s\tremaining: 9.41s\n",
      "327:\tlearn: 0.1881380\ttotal: 4.58s\tremaining: 9.39s\n",
      "328:\tlearn: 0.1880361\ttotal: 4.6s\tremaining: 9.38s\n",
      "329:\tlearn: 0.1879744\ttotal: 4.61s\tremaining: 9.36s\n",
      "330:\tlearn: 0.1879131\ttotal: 4.62s\tremaining: 9.35s\n",
      "331:\tlearn: 0.1878371\ttotal: 4.64s\tremaining: 9.33s\n",
      "332:\tlearn: 0.1877416\ttotal: 4.65s\tremaining: 9.31s\n",
      "333:\tlearn: 0.1875751\ttotal: 4.66s\tremaining: 9.29s\n",
      "334:\tlearn: 0.1874717\ttotal: 4.67s\tremaining: 9.28s\n",
      "335:\tlearn: 0.1872220\ttotal: 4.69s\tremaining: 9.26s\n",
      "336:\tlearn: 0.1871252\ttotal: 4.7s\tremaining: 9.25s\n",
      "337:\tlearn: 0.1868726\ttotal: 4.72s\tremaining: 9.24s\n",
      "338:\tlearn: 0.1866881\ttotal: 4.73s\tremaining: 9.22s\n",
      "339:\tlearn: 0.1866093\ttotal: 4.74s\tremaining: 9.21s\n",
      "340:\tlearn: 0.1865091\ttotal: 4.75s\tremaining: 9.19s\n",
      "341:\tlearn: 0.1864541\ttotal: 4.77s\tremaining: 9.18s\n",
      "342:\tlearn: 0.1864108\ttotal: 4.78s\tremaining: 9.16s\n",
      "343:\tlearn: 0.1863592\ttotal: 4.79s\tremaining: 9.14s\n",
      "344:\tlearn: 0.1861500\ttotal: 4.81s\tremaining: 9.13s\n",
      "345:\tlearn: 0.1860366\ttotal: 4.82s\tremaining: 9.12s\n",
      "346:\tlearn: 0.1858997\ttotal: 4.83s\tremaining: 9.1s\n",
      "347:\tlearn: 0.1858479\ttotal: 4.85s\tremaining: 9.08s\n",
      "348:\tlearn: 0.1857199\ttotal: 4.86s\tremaining: 9.07s\n",
      "349:\tlearn: 0.1854793\ttotal: 4.87s\tremaining: 9.05s\n",
      "350:\tlearn: 0.1854140\ttotal: 4.89s\tremaining: 9.04s\n",
      "351:\tlearn: 0.1853490\ttotal: 4.9s\tremaining: 9.02s\n",
      "352:\tlearn: 0.1852812\ttotal: 4.93s\tremaining: 9.04s\n",
      "353:\tlearn: 0.1852265\ttotal: 4.95s\tremaining: 9.03s\n",
      "354:\tlearn: 0.1851683\ttotal: 4.96s\tremaining: 9.02s\n",
      "355:\tlearn: 0.1851031\ttotal: 4.98s\tremaining: 9.01s\n",
      "356:\tlearn: 0.1850362\ttotal: 4.99s\tremaining: 8.99s\n",
      "357:\tlearn: 0.1849576\ttotal: 5.01s\tremaining: 8.98s\n",
      "358:\tlearn: 0.1848508\ttotal: 5.02s\tremaining: 8.96s\n",
      "359:\tlearn: 0.1847557\ttotal: 5.03s\tremaining: 8.95s\n",
      "360:\tlearn: 0.1845302\ttotal: 5.05s\tremaining: 8.94s\n",
      "361:\tlearn: 0.1843630\ttotal: 5.06s\tremaining: 8.92s\n",
      "362:\tlearn: 0.1842856\ttotal: 5.07s\tremaining: 8.9s\n",
      "363:\tlearn: 0.1841306\ttotal: 5.09s\tremaining: 8.89s\n",
      "364:\tlearn: 0.1840386\ttotal: 5.1s\tremaining: 8.87s\n",
      "365:\tlearn: 0.1839624\ttotal: 5.11s\tremaining: 8.86s\n",
      "366:\tlearn: 0.1838967\ttotal: 5.13s\tremaining: 8.84s\n",
      "367:\tlearn: 0.1837231\ttotal: 5.14s\tremaining: 8.83s\n",
      "368:\tlearn: 0.1836700\ttotal: 5.16s\tremaining: 8.81s\n",
      "369:\tlearn: 0.1835844\ttotal: 5.17s\tremaining: 8.8s\n",
      "370:\tlearn: 0.1835222\ttotal: 5.18s\tremaining: 8.79s\n",
      "371:\tlearn: 0.1834655\ttotal: 5.2s\tremaining: 8.77s\n",
      "372:\tlearn: 0.1833852\ttotal: 5.21s\tremaining: 8.76s\n",
      "373:\tlearn: 0.1833323\ttotal: 5.22s\tremaining: 8.74s\n",
      "374:\tlearn: 0.1832083\ttotal: 5.24s\tremaining: 8.72s\n",
      "375:\tlearn: 0.1830218\ttotal: 5.25s\tremaining: 8.71s\n",
      "376:\tlearn: 0.1829010\ttotal: 5.26s\tremaining: 8.7s\n",
      "377:\tlearn: 0.1827110\ttotal: 5.28s\tremaining: 8.69s\n",
      "378:\tlearn: 0.1826647\ttotal: 5.29s\tremaining: 8.68s\n",
      "379:\tlearn: 0.1825495\ttotal: 5.31s\tremaining: 8.66s\n",
      "380:\tlearn: 0.1824350\ttotal: 5.32s\tremaining: 8.65s\n",
      "381:\tlearn: 0.1822588\ttotal: 5.33s\tremaining: 8.63s\n",
      "382:\tlearn: 0.1822017\ttotal: 5.35s\tremaining: 8.62s\n",
      "383:\tlearn: 0.1820702\ttotal: 5.36s\tremaining: 8.6s\n",
      "384:\tlearn: 0.1820129\ttotal: 5.37s\tremaining: 8.59s\n",
      "385:\tlearn: 0.1819386\ttotal: 5.39s\tremaining: 8.57s\n",
      "386:\tlearn: 0.1818593\ttotal: 5.4s\tremaining: 8.56s\n",
      "387:\tlearn: 0.1817597\ttotal: 5.42s\tremaining: 8.55s\n",
      "388:\tlearn: 0.1816957\ttotal: 5.45s\tremaining: 8.56s\n",
      "389:\tlearn: 0.1816312\ttotal: 5.47s\tremaining: 8.55s\n",
      "390:\tlearn: 0.1815897\ttotal: 5.48s\tremaining: 8.54s\n",
      "391:\tlearn: 0.1814655\ttotal: 5.49s\tremaining: 8.52s\n",
      "392:\tlearn: 0.1813681\ttotal: 5.51s\tremaining: 8.51s\n",
      "393:\tlearn: 0.1813172\ttotal: 5.52s\tremaining: 8.49s\n",
      "394:\tlearn: 0.1812843\ttotal: 5.53s\tremaining: 8.47s\n",
      "395:\tlearn: 0.1810958\ttotal: 5.54s\tremaining: 8.46s\n",
      "396:\tlearn: 0.1809935\ttotal: 5.56s\tremaining: 8.44s\n",
      "397:\tlearn: 0.1808953\ttotal: 5.57s\tremaining: 8.43s\n",
      "398:\tlearn: 0.1807429\ttotal: 5.59s\tremaining: 8.41s\n",
      "399:\tlearn: 0.1806556\ttotal: 5.6s\tremaining: 8.4s\n",
      "400:\tlearn: 0.1805153\ttotal: 5.62s\tremaining: 8.39s\n",
      "401:\tlearn: 0.1804343\ttotal: 5.63s\tremaining: 8.37s\n",
      "402:\tlearn: 0.1802948\ttotal: 5.64s\tremaining: 8.36s\n",
      "403:\tlearn: 0.1802013\ttotal: 5.66s\tremaining: 8.34s\n",
      "404:\tlearn: 0.1801638\ttotal: 5.67s\tremaining: 8.33s\n",
      "405:\tlearn: 0.1800481\ttotal: 5.68s\tremaining: 8.31s\n",
      "406:\tlearn: 0.1799739\ttotal: 5.71s\tremaining: 8.32s\n",
      "407:\tlearn: 0.1799063\ttotal: 5.73s\tremaining: 8.31s\n",
      "408:\tlearn: 0.1798449\ttotal: 5.74s\tremaining: 8.29s\n",
      "409:\tlearn: 0.1797821\ttotal: 5.75s\tremaining: 8.28s\n",
      "410:\tlearn: 0.1797264\ttotal: 5.77s\tremaining: 8.26s\n",
      "411:\tlearn: 0.1796863\ttotal: 5.78s\tremaining: 8.25s\n",
      "412:\tlearn: 0.1796301\ttotal: 5.79s\tremaining: 8.23s\n",
      "413:\tlearn: 0.1795834\ttotal: 5.81s\tremaining: 8.22s\n",
      "414:\tlearn: 0.1795423\ttotal: 5.82s\tremaining: 8.2s\n",
      "415:\tlearn: 0.1794490\ttotal: 5.83s\tremaining: 8.19s\n",
      "416:\tlearn: 0.1793495\ttotal: 5.85s\tremaining: 8.18s\n",
      "417:\tlearn: 0.1792168\ttotal: 5.86s\tremaining: 8.16s\n",
      "418:\tlearn: 0.1791820\ttotal: 5.87s\tremaining: 8.14s\n",
      "419:\tlearn: 0.1789682\ttotal: 5.89s\tremaining: 8.13s\n",
      "420:\tlearn: 0.1787493\ttotal: 5.9s\tremaining: 8.11s\n",
      "421:\tlearn: 0.1785825\ttotal: 5.91s\tremaining: 8.1s\n",
      "422:\tlearn: 0.1785104\ttotal: 5.93s\tremaining: 8.08s\n",
      "423:\tlearn: 0.1784351\ttotal: 5.94s\tremaining: 8.07s\n",
      "424:\tlearn: 0.1784002\ttotal: 5.95s\tremaining: 8.06s\n",
      "425:\tlearn: 0.1783178\ttotal: 5.97s\tremaining: 8.04s\n",
      "426:\tlearn: 0.1782217\ttotal: 5.98s\tremaining: 8.02s\n",
      "427:\tlearn: 0.1781456\ttotal: 5.99s\tremaining: 8.01s\n",
      "428:\tlearn: 0.1781104\ttotal: 6.01s\tremaining: 8s\n",
      "429:\tlearn: 0.1780298\ttotal: 6.02s\tremaining: 7.98s\n",
      "430:\tlearn: 0.1779016\ttotal: 6.03s\tremaining: 7.96s\n",
      "431:\tlearn: 0.1777499\ttotal: 6.05s\tremaining: 7.95s\n",
      "432:\tlearn: 0.1775349\ttotal: 6.06s\tremaining: 7.94s\n",
      "433:\tlearn: 0.1774641\ttotal: 6.07s\tremaining: 7.92s\n",
      "434:\tlearn: 0.1774001\ttotal: 6.09s\tremaining: 7.91s\n",
      "435:\tlearn: 0.1773456\ttotal: 6.1s\tremaining: 7.89s\n",
      "436:\tlearn: 0.1772515\ttotal: 6.11s\tremaining: 7.87s\n",
      "437:\tlearn: 0.1771761\ttotal: 6.13s\tremaining: 7.86s\n",
      "438:\tlearn: 0.1770623\ttotal: 6.14s\tremaining: 7.84s\n",
      "439:\tlearn: 0.1769408\ttotal: 6.15s\tremaining: 7.83s\n",
      "440:\tlearn: 0.1767897\ttotal: 6.17s\tremaining: 7.82s\n",
      "441:\tlearn: 0.1767614\ttotal: 6.18s\tremaining: 7.8s\n",
      "442:\tlearn: 0.1767010\ttotal: 6.19s\tremaining: 7.79s\n",
      "443:\tlearn: 0.1766153\ttotal: 6.21s\tremaining: 7.77s\n",
      "444:\tlearn: 0.1765372\ttotal: 6.22s\tremaining: 7.76s\n",
      "445:\tlearn: 0.1764814\ttotal: 6.23s\tremaining: 7.74s\n",
      "446:\tlearn: 0.1763763\ttotal: 6.25s\tremaining: 7.73s\n",
      "447:\tlearn: 0.1763163\ttotal: 6.26s\tremaining: 7.71s\n",
      "448:\tlearn: 0.1762770\ttotal: 6.27s\tremaining: 7.7s\n",
      "449:\tlearn: 0.1762181\ttotal: 6.29s\tremaining: 7.68s\n",
      "450:\tlearn: 0.1761793\ttotal: 6.3s\tremaining: 7.67s\n",
      "451:\tlearn: 0.1760492\ttotal: 6.31s\tremaining: 7.65s\n",
      "452:\tlearn: 0.1758904\ttotal: 6.33s\tremaining: 7.64s\n",
      "453:\tlearn: 0.1757902\ttotal: 6.34s\tremaining: 7.62s\n",
      "454:\tlearn: 0.1757472\ttotal: 6.35s\tremaining: 7.61s\n",
      "455:\tlearn: 0.1755936\ttotal: 6.37s\tremaining: 7.59s\n",
      "456:\tlearn: 0.1755495\ttotal: 6.38s\tremaining: 7.58s\n",
      "457:\tlearn: 0.1754865\ttotal: 6.39s\tremaining: 7.57s\n",
      "458:\tlearn: 0.1754241\ttotal: 6.41s\tremaining: 7.55s\n",
      "459:\tlearn: 0.1753752\ttotal: 6.42s\tremaining: 7.54s\n",
      "460:\tlearn: 0.1752726\ttotal: 6.43s\tremaining: 7.52s\n",
      "461:\tlearn: 0.1751048\ttotal: 6.45s\tremaining: 7.51s\n",
      "462:\tlearn: 0.1750557\ttotal: 6.46s\tremaining: 7.5s\n",
      "463:\tlearn: 0.1749970\ttotal: 6.48s\tremaining: 7.48s\n",
      "464:\tlearn: 0.1749212\ttotal: 6.49s\tremaining: 7.47s\n",
      "465:\tlearn: 0.1748666\ttotal: 6.5s\tremaining: 7.45s\n",
      "466:\tlearn: 0.1747866\ttotal: 6.52s\tremaining: 7.44s\n",
      "467:\tlearn: 0.1747029\ttotal: 6.53s\tremaining: 7.42s\n",
      "468:\tlearn: 0.1746681\ttotal: 6.54s\tremaining: 7.41s\n",
      "469:\tlearn: 0.1746089\ttotal: 6.56s\tremaining: 7.39s\n",
      "470:\tlearn: 0.1745489\ttotal: 6.57s\tremaining: 7.38s\n",
      "471:\tlearn: 0.1744463\ttotal: 6.58s\tremaining: 7.37s\n",
      "472:\tlearn: 0.1742999\ttotal: 6.6s\tremaining: 7.35s\n",
      "473:\tlearn: 0.1742416\ttotal: 6.61s\tremaining: 7.34s\n",
      "474:\tlearn: 0.1741600\ttotal: 6.63s\tremaining: 7.33s\n",
      "475:\tlearn: 0.1740843\ttotal: 6.64s\tremaining: 7.31s\n",
      "476:\tlearn: 0.1740153\ttotal: 6.65s\tremaining: 7.29s\n",
      "477:\tlearn: 0.1738972\ttotal: 6.67s\tremaining: 7.28s\n",
      "478:\tlearn: 0.1738239\ttotal: 6.68s\tremaining: 7.27s\n",
      "479:\tlearn: 0.1737529\ttotal: 6.7s\tremaining: 7.25s\n",
      "480:\tlearn: 0.1736662\ttotal: 6.71s\tremaining: 7.24s\n",
      "481:\tlearn: 0.1735991\ttotal: 6.72s\tremaining: 7.23s\n",
      "482:\tlearn: 0.1735671\ttotal: 6.74s\tremaining: 7.21s\n",
      "483:\tlearn: 0.1735005\ttotal: 6.75s\tremaining: 7.2s\n",
      "484:\tlearn: 0.1734407\ttotal: 6.77s\tremaining: 7.19s\n",
      "485:\tlearn: 0.1733754\ttotal: 6.78s\tremaining: 7.17s\n",
      "486:\tlearn: 0.1733038\ttotal: 6.8s\tremaining: 7.16s\n",
      "487:\tlearn: 0.1732399\ttotal: 6.81s\tremaining: 7.14s\n",
      "488:\tlearn: 0.1731965\ttotal: 6.82s\tremaining: 7.13s\n",
      "489:\tlearn: 0.1731491\ttotal: 6.83s\tremaining: 7.11s\n",
      "490:\tlearn: 0.1731178\ttotal: 6.85s\tremaining: 7.1s\n",
      "491:\tlearn: 0.1730599\ttotal: 6.86s\tremaining: 7.08s\n",
      "492:\tlearn: 0.1729205\ttotal: 6.88s\tremaining: 7.07s\n",
      "493:\tlearn: 0.1728520\ttotal: 6.89s\tremaining: 7.05s\n",
      "494:\tlearn: 0.1727902\ttotal: 6.9s\tremaining: 7.04s\n",
      "495:\tlearn: 0.1727130\ttotal: 6.92s\tremaining: 7.03s\n",
      "496:\tlearn: 0.1726359\ttotal: 6.93s\tremaining: 7.01s\n",
      "497:\tlearn: 0.1725965\ttotal: 6.94s\tremaining: 7s\n",
      "498:\tlearn: 0.1725027\ttotal: 6.96s\tremaining: 6.98s\n",
      "499:\tlearn: 0.1724211\ttotal: 6.97s\tremaining: 6.97s\n",
      "500:\tlearn: 0.1723849\ttotal: 6.98s\tremaining: 6.95s\n",
      "501:\tlearn: 0.1723541\ttotal: 6.99s\tremaining: 6.94s\n",
      "502:\tlearn: 0.1722342\ttotal: 7.01s\tremaining: 6.93s\n",
      "503:\tlearn: 0.1721898\ttotal: 7.02s\tremaining: 6.91s\n",
      "504:\tlearn: 0.1720887\ttotal: 7.04s\tremaining: 6.9s\n",
      "505:\tlearn: 0.1719979\ttotal: 7.05s\tremaining: 6.88s\n",
      "506:\tlearn: 0.1719195\ttotal: 7.07s\tremaining: 6.87s\n",
      "507:\tlearn: 0.1718631\ttotal: 7.08s\tremaining: 6.86s\n",
      "508:\tlearn: 0.1717902\ttotal: 7.09s\tremaining: 6.84s\n",
      "509:\tlearn: 0.1717519\ttotal: 7.11s\tremaining: 6.83s\n",
      "510:\tlearn: 0.1716971\ttotal: 7.12s\tremaining: 6.82s\n",
      "511:\tlearn: 0.1716516\ttotal: 7.13s\tremaining: 6.8s\n",
      "512:\tlearn: 0.1716074\ttotal: 7.15s\tremaining: 6.79s\n",
      "513:\tlearn: 0.1715243\ttotal: 7.16s\tremaining: 6.77s\n",
      "514:\tlearn: 0.1714704\ttotal: 7.18s\tremaining: 6.76s\n",
      "515:\tlearn: 0.1714202\ttotal: 7.19s\tremaining: 6.75s\n",
      "516:\tlearn: 0.1713664\ttotal: 7.21s\tremaining: 6.73s\n",
      "517:\tlearn: 0.1713161\ttotal: 7.22s\tremaining: 6.72s\n",
      "518:\tlearn: 0.1712744\ttotal: 7.23s\tremaining: 6.7s\n",
      "519:\tlearn: 0.1711750\ttotal: 7.25s\tremaining: 6.69s\n",
      "520:\tlearn: 0.1711299\ttotal: 7.26s\tremaining: 6.68s\n",
      "521:\tlearn: 0.1710645\ttotal: 7.28s\tremaining: 6.67s\n",
      "522:\tlearn: 0.1710108\ttotal: 7.29s\tremaining: 6.65s\n",
      "523:\tlearn: 0.1709804\ttotal: 7.3s\tremaining: 6.63s\n",
      "524:\tlearn: 0.1709349\ttotal: 7.32s\tremaining: 6.62s\n",
      "525:\tlearn: 0.1708890\ttotal: 7.33s\tremaining: 6.61s\n",
      "526:\tlearn: 0.1708358\ttotal: 7.34s\tremaining: 6.59s\n",
      "527:\tlearn: 0.1707714\ttotal: 7.36s\tremaining: 6.58s\n",
      "528:\tlearn: 0.1707274\ttotal: 7.37s\tremaining: 6.56s\n",
      "529:\tlearn: 0.1706730\ttotal: 7.39s\tremaining: 6.55s\n",
      "530:\tlearn: 0.1706250\ttotal: 7.4s\tremaining: 6.54s\n",
      "531:\tlearn: 0.1705694\ttotal: 7.42s\tremaining: 6.53s\n",
      "532:\tlearn: 0.1704945\ttotal: 7.43s\tremaining: 6.51s\n",
      "533:\tlearn: 0.1704511\ttotal: 7.44s\tremaining: 6.5s\n",
      "534:\tlearn: 0.1703851\ttotal: 7.46s\tremaining: 6.48s\n",
      "535:\tlearn: 0.1703011\ttotal: 7.48s\tremaining: 6.47s\n",
      "536:\tlearn: 0.1702325\ttotal: 7.49s\tremaining: 6.46s\n",
      "537:\tlearn: 0.1701224\ttotal: 7.51s\tremaining: 6.45s\n",
      "538:\tlearn: 0.1700779\ttotal: 7.52s\tremaining: 6.43s\n",
      "539:\tlearn: 0.1700229\ttotal: 7.54s\tremaining: 6.42s\n",
      "540:\tlearn: 0.1699571\ttotal: 7.55s\tremaining: 6.4s\n",
      "541:\tlearn: 0.1698753\ttotal: 7.56s\tremaining: 6.39s\n",
      "542:\tlearn: 0.1698022\ttotal: 7.57s\tremaining: 6.37s\n",
      "543:\tlearn: 0.1697629\ttotal: 7.59s\tremaining: 6.36s\n",
      "544:\tlearn: 0.1696742\ttotal: 7.6s\tremaining: 6.35s\n",
      "545:\tlearn: 0.1696234\ttotal: 7.62s\tremaining: 6.33s\n",
      "546:\tlearn: 0.1695486\ttotal: 7.63s\tremaining: 6.32s\n",
      "547:\tlearn: 0.1695037\ttotal: 7.64s\tremaining: 6.3s\n",
      "548:\tlearn: 0.1694574\ttotal: 7.66s\tremaining: 6.29s\n",
      "549:\tlearn: 0.1693396\ttotal: 7.67s\tremaining: 6.28s\n",
      "550:\tlearn: 0.1692555\ttotal: 7.69s\tremaining: 6.26s\n",
      "551:\tlearn: 0.1692010\ttotal: 7.7s\tremaining: 6.25s\n",
      "552:\tlearn: 0.1691178\ttotal: 7.71s\tremaining: 6.24s\n",
      "553:\tlearn: 0.1690609\ttotal: 7.73s\tremaining: 6.22s\n",
      "554:\tlearn: 0.1690083\ttotal: 7.74s\tremaining: 6.21s\n",
      "555:\tlearn: 0.1689689\ttotal: 7.76s\tremaining: 6.19s\n",
      "556:\tlearn: 0.1687725\ttotal: 7.77s\tremaining: 6.18s\n",
      "557:\tlearn: 0.1687207\ttotal: 7.78s\tremaining: 6.17s\n",
      "558:\tlearn: 0.1686725\ttotal: 7.8s\tremaining: 6.15s\n",
      "559:\tlearn: 0.1686062\ttotal: 7.81s\tremaining: 6.14s\n",
      "560:\tlearn: 0.1685490\ttotal: 7.82s\tremaining: 6.12s\n",
      "561:\tlearn: 0.1685162\ttotal: 7.84s\tremaining: 6.11s\n",
      "562:\tlearn: 0.1684698\ttotal: 7.85s\tremaining: 6.09s\n",
      "563:\tlearn: 0.1684157\ttotal: 7.86s\tremaining: 6.08s\n",
      "564:\tlearn: 0.1683743\ttotal: 7.87s\tremaining: 6.06s\n",
      "565:\tlearn: 0.1683185\ttotal: 7.89s\tremaining: 6.05s\n",
      "566:\tlearn: 0.1682564\ttotal: 7.9s\tremaining: 6.03s\n",
      "567:\tlearn: 0.1682201\ttotal: 7.91s\tremaining: 6.02s\n",
      "568:\tlearn: 0.1680687\ttotal: 7.93s\tremaining: 6.01s\n",
      "569:\tlearn: 0.1680066\ttotal: 7.94s\tremaining: 5.99s\n",
      "570:\tlearn: 0.1679168\ttotal: 7.96s\tremaining: 5.98s\n",
      "571:\tlearn: 0.1678368\ttotal: 7.97s\tremaining: 5.96s\n",
      "572:\tlearn: 0.1677701\ttotal: 7.98s\tremaining: 5.95s\n",
      "573:\tlearn: 0.1676730\ttotal: 8s\tremaining: 5.93s\n",
      "574:\tlearn: 0.1676418\ttotal: 8.01s\tremaining: 5.92s\n",
      "575:\tlearn: 0.1675841\ttotal: 8.02s\tremaining: 5.91s\n",
      "576:\tlearn: 0.1675400\ttotal: 8.04s\tremaining: 5.89s\n",
      "577:\tlearn: 0.1674665\ttotal: 8.05s\tremaining: 5.88s\n",
      "578:\tlearn: 0.1674009\ttotal: 8.06s\tremaining: 5.86s\n",
      "579:\tlearn: 0.1673316\ttotal: 8.07s\tremaining: 5.85s\n",
      "580:\tlearn: 0.1672713\ttotal: 8.09s\tremaining: 5.83s\n",
      "581:\tlearn: 0.1671952\ttotal: 8.1s\tremaining: 5.82s\n",
      "582:\tlearn: 0.1671406\ttotal: 8.11s\tremaining: 5.8s\n",
      "583:\tlearn: 0.1670696\ttotal: 8.13s\tremaining: 5.79s\n",
      "584:\tlearn: 0.1670256\ttotal: 8.14s\tremaining: 5.77s\n",
      "585:\tlearn: 0.1669626\ttotal: 8.15s\tremaining: 5.76s\n",
      "586:\tlearn: 0.1668842\ttotal: 8.17s\tremaining: 5.75s\n",
      "587:\tlearn: 0.1668082\ttotal: 8.18s\tremaining: 5.73s\n",
      "588:\tlearn: 0.1667198\ttotal: 8.19s\tremaining: 5.72s\n",
      "589:\tlearn: 0.1666918\ttotal: 8.21s\tremaining: 5.7s\n",
      "590:\tlearn: 0.1666342\ttotal: 8.22s\tremaining: 5.69s\n",
      "591:\tlearn: 0.1665793\ttotal: 8.23s\tremaining: 5.68s\n",
      "592:\tlearn: 0.1665388\ttotal: 8.25s\tremaining: 5.66s\n",
      "593:\tlearn: 0.1664922\ttotal: 8.27s\tremaining: 5.65s\n",
      "594:\tlearn: 0.1664119\ttotal: 8.28s\tremaining: 5.63s\n",
      "595:\tlearn: 0.1662595\ttotal: 8.29s\tremaining: 5.62s\n",
      "596:\tlearn: 0.1661956\ttotal: 8.31s\tremaining: 5.61s\n",
      "597:\tlearn: 0.1661359\ttotal: 8.32s\tremaining: 5.59s\n",
      "598:\tlearn: 0.1660690\ttotal: 8.33s\tremaining: 5.58s\n",
      "599:\tlearn: 0.1660199\ttotal: 8.35s\tremaining: 5.56s\n",
      "600:\tlearn: 0.1659508\ttotal: 8.36s\tremaining: 5.55s\n",
      "601:\tlearn: 0.1659191\ttotal: 8.37s\tremaining: 5.54s\n",
      "602:\tlearn: 0.1658526\ttotal: 8.39s\tremaining: 5.52s\n",
      "603:\tlearn: 0.1657998\ttotal: 8.4s\tremaining: 5.51s\n",
      "604:\tlearn: 0.1657588\ttotal: 8.41s\tremaining: 5.49s\n",
      "605:\tlearn: 0.1657091\ttotal: 8.43s\tremaining: 5.48s\n",
      "606:\tlearn: 0.1656418\ttotal: 8.44s\tremaining: 5.47s\n",
      "607:\tlearn: 0.1656066\ttotal: 8.45s\tremaining: 5.45s\n",
      "608:\tlearn: 0.1655188\ttotal: 8.47s\tremaining: 5.44s\n",
      "609:\tlearn: 0.1654719\ttotal: 8.48s\tremaining: 5.42s\n",
      "610:\tlearn: 0.1654024\ttotal: 8.49s\tremaining: 5.41s\n",
      "611:\tlearn: 0.1653405\ttotal: 8.51s\tremaining: 5.39s\n",
      "612:\tlearn: 0.1652125\ttotal: 8.52s\tremaining: 5.38s\n",
      "613:\tlearn: 0.1651509\ttotal: 8.54s\tremaining: 5.37s\n",
      "614:\tlearn: 0.1651074\ttotal: 8.55s\tremaining: 5.35s\n",
      "615:\tlearn: 0.1650509\ttotal: 8.56s\tremaining: 5.34s\n",
      "616:\tlearn: 0.1650169\ttotal: 8.58s\tremaining: 5.32s\n",
      "617:\tlearn: 0.1649475\ttotal: 8.61s\tremaining: 5.32s\n",
      "618:\tlearn: 0.1649215\ttotal: 8.62s\tremaining: 5.3s\n",
      "619:\tlearn: 0.1648571\ttotal: 8.63s\tremaining: 5.29s\n",
      "620:\tlearn: 0.1648045\ttotal: 8.65s\tremaining: 5.28s\n",
      "621:\tlearn: 0.1647149\ttotal: 8.66s\tremaining: 5.26s\n",
      "622:\tlearn: 0.1646796\ttotal: 8.68s\tremaining: 5.25s\n",
      "623:\tlearn: 0.1646581\ttotal: 8.69s\tremaining: 5.24s\n",
      "624:\tlearn: 0.1646191\ttotal: 8.7s\tremaining: 5.22s\n",
      "625:\tlearn: 0.1645794\ttotal: 8.71s\tremaining: 5.21s\n",
      "626:\tlearn: 0.1645169\ttotal: 8.73s\tremaining: 5.19s\n",
      "627:\tlearn: 0.1644768\ttotal: 8.74s\tremaining: 5.18s\n",
      "628:\tlearn: 0.1644167\ttotal: 8.76s\tremaining: 5.16s\n",
      "629:\tlearn: 0.1643299\ttotal: 8.77s\tremaining: 5.15s\n",
      "630:\tlearn: 0.1642767\ttotal: 8.79s\tremaining: 5.14s\n",
      "631:\tlearn: 0.1641878\ttotal: 8.8s\tremaining: 5.12s\n",
      "632:\tlearn: 0.1641577\ttotal: 8.81s\tremaining: 5.11s\n",
      "633:\tlearn: 0.1641266\ttotal: 8.82s\tremaining: 5.09s\n",
      "634:\tlearn: 0.1640774\ttotal: 8.84s\tremaining: 5.08s\n",
      "635:\tlearn: 0.1640479\ttotal: 8.85s\tremaining: 5.06s\n",
      "636:\tlearn: 0.1639956\ttotal: 8.86s\tremaining: 5.05s\n",
      "637:\tlearn: 0.1639060\ttotal: 8.88s\tremaining: 5.04s\n",
      "638:\tlearn: 0.1638584\ttotal: 8.89s\tremaining: 5.02s\n",
      "639:\tlearn: 0.1638101\ttotal: 8.9s\tremaining: 5.01s\n",
      "640:\tlearn: 0.1637668\ttotal: 8.91s\tremaining: 4.99s\n",
      "641:\tlearn: 0.1637120\ttotal: 8.93s\tremaining: 4.98s\n",
      "642:\tlearn: 0.1636536\ttotal: 8.94s\tremaining: 4.96s\n",
      "643:\tlearn: 0.1635306\ttotal: 8.96s\tremaining: 4.95s\n",
      "644:\tlearn: 0.1635068\ttotal: 8.97s\tremaining: 4.94s\n",
      "645:\tlearn: 0.1634480\ttotal: 8.98s\tremaining: 4.92s\n",
      "646:\tlearn: 0.1633652\ttotal: 9s\tremaining: 4.91s\n",
      "647:\tlearn: 0.1632862\ttotal: 9.01s\tremaining: 4.89s\n",
      "648:\tlearn: 0.1632415\ttotal: 9.03s\tremaining: 4.88s\n",
      "649:\tlearn: 0.1632068\ttotal: 9.04s\tremaining: 4.87s\n",
      "650:\tlearn: 0.1631286\ttotal: 9.05s\tremaining: 4.85s\n",
      "651:\tlearn: 0.1630441\ttotal: 9.06s\tremaining: 4.84s\n",
      "652:\tlearn: 0.1628477\ttotal: 9.08s\tremaining: 4.82s\n",
      "653:\tlearn: 0.1627996\ttotal: 9.09s\tremaining: 4.81s\n",
      "654:\tlearn: 0.1627763\ttotal: 9.11s\tremaining: 4.8s\n",
      "655:\tlearn: 0.1627270\ttotal: 9.12s\tremaining: 4.78s\n",
      "656:\tlearn: 0.1626998\ttotal: 9.13s\tremaining: 4.77s\n",
      "657:\tlearn: 0.1626272\ttotal: 9.14s\tremaining: 4.75s\n",
      "658:\tlearn: 0.1625897\ttotal: 9.16s\tremaining: 4.74s\n",
      "659:\tlearn: 0.1625484\ttotal: 9.17s\tremaining: 4.72s\n",
      "660:\tlearn: 0.1624920\ttotal: 9.18s\tremaining: 4.71s\n",
      "661:\tlearn: 0.1624100\ttotal: 9.2s\tremaining: 4.7s\n",
      "662:\tlearn: 0.1623918\ttotal: 9.21s\tremaining: 4.68s\n",
      "663:\tlearn: 0.1623369\ttotal: 9.23s\tremaining: 4.67s\n",
      "664:\tlearn: 0.1622880\ttotal: 9.24s\tremaining: 4.66s\n",
      "665:\tlearn: 0.1622448\ttotal: 9.26s\tremaining: 4.64s\n",
      "666:\tlearn: 0.1621536\ttotal: 9.27s\tremaining: 4.63s\n",
      "667:\tlearn: 0.1621015\ttotal: 9.28s\tremaining: 4.61s\n",
      "668:\tlearn: 0.1620390\ttotal: 9.32s\tremaining: 4.61s\n",
      "669:\tlearn: 0.1619883\ttotal: 9.34s\tremaining: 4.6s\n",
      "670:\tlearn: 0.1619026\ttotal: 9.35s\tremaining: 4.58s\n",
      "671:\tlearn: 0.1618176\ttotal: 9.36s\tremaining: 4.57s\n",
      "672:\tlearn: 0.1617438\ttotal: 9.38s\tremaining: 4.56s\n",
      "673:\tlearn: 0.1617010\ttotal: 9.39s\tremaining: 4.54s\n",
      "674:\tlearn: 0.1616297\ttotal: 9.41s\tremaining: 4.53s\n",
      "675:\tlearn: 0.1615701\ttotal: 9.42s\tremaining: 4.51s\n",
      "676:\tlearn: 0.1615498\ttotal: 9.43s\tremaining: 4.5s\n",
      "677:\tlearn: 0.1615178\ttotal: 9.45s\tremaining: 4.49s\n",
      "678:\tlearn: 0.1614705\ttotal: 9.46s\tremaining: 4.47s\n",
      "679:\tlearn: 0.1614218\ttotal: 9.47s\tremaining: 4.46s\n",
      "680:\tlearn: 0.1613756\ttotal: 9.48s\tremaining: 4.44s\n",
      "681:\tlearn: 0.1613258\ttotal: 9.5s\tremaining: 4.43s\n",
      "682:\tlearn: 0.1612810\ttotal: 9.51s\tremaining: 4.41s\n",
      "683:\tlearn: 0.1612306\ttotal: 9.52s\tremaining: 4.4s\n",
      "684:\tlearn: 0.1611766\ttotal: 9.54s\tremaining: 4.38s\n",
      "685:\tlearn: 0.1611334\ttotal: 9.55s\tremaining: 4.37s\n",
      "686:\tlearn: 0.1610786\ttotal: 9.56s\tremaining: 4.36s\n",
      "687:\tlearn: 0.1610266\ttotal: 9.57s\tremaining: 4.34s\n",
      "688:\tlearn: 0.1609896\ttotal: 9.59s\tremaining: 4.33s\n",
      "689:\tlearn: 0.1608925\ttotal: 9.6s\tremaining: 4.31s\n",
      "690:\tlearn: 0.1608593\ttotal: 9.62s\tremaining: 4.3s\n",
      "691:\tlearn: 0.1608126\ttotal: 9.64s\tremaining: 4.29s\n",
      "692:\tlearn: 0.1607194\ttotal: 9.66s\tremaining: 4.28s\n",
      "693:\tlearn: 0.1606393\ttotal: 9.67s\tremaining: 4.26s\n",
      "694:\tlearn: 0.1605878\ttotal: 9.69s\tremaining: 4.25s\n",
      "695:\tlearn: 0.1605022\ttotal: 9.7s\tremaining: 4.24s\n",
      "696:\tlearn: 0.1604526\ttotal: 9.71s\tremaining: 4.22s\n",
      "697:\tlearn: 0.1604044\ttotal: 9.73s\tremaining: 4.21s\n",
      "698:\tlearn: 0.1603615\ttotal: 9.74s\tremaining: 4.19s\n",
      "699:\tlearn: 0.1602943\ttotal: 9.76s\tremaining: 4.18s\n",
      "700:\tlearn: 0.1602645\ttotal: 9.77s\tremaining: 4.17s\n",
      "701:\tlearn: 0.1601378\ttotal: 9.79s\tremaining: 4.15s\n",
      "702:\tlearn: 0.1600548\ttotal: 9.8s\tremaining: 4.14s\n",
      "703:\tlearn: 0.1600318\ttotal: 9.81s\tremaining: 4.13s\n",
      "704:\tlearn: 0.1599901\ttotal: 9.82s\tremaining: 4.11s\n",
      "705:\tlearn: 0.1598669\ttotal: 9.84s\tremaining: 4.1s\n",
      "706:\tlearn: 0.1598086\ttotal: 9.85s\tremaining: 4.08s\n",
      "707:\tlearn: 0.1597577\ttotal: 9.86s\tremaining: 4.07s\n",
      "708:\tlearn: 0.1596850\ttotal: 9.88s\tremaining: 4.05s\n",
      "709:\tlearn: 0.1596403\ttotal: 9.89s\tremaining: 4.04s\n",
      "710:\tlearn: 0.1596028\ttotal: 9.9s\tremaining: 4.03s\n",
      "711:\tlearn: 0.1595443\ttotal: 9.92s\tremaining: 4.01s\n",
      "712:\tlearn: 0.1594963\ttotal: 9.93s\tremaining: 4s\n",
      "713:\tlearn: 0.1594205\ttotal: 9.94s\tremaining: 3.98s\n",
      "714:\tlearn: 0.1593631\ttotal: 9.96s\tremaining: 3.97s\n",
      "715:\tlearn: 0.1593151\ttotal: 9.97s\tremaining: 3.96s\n",
      "716:\tlearn: 0.1592889\ttotal: 9.98s\tremaining: 3.94s\n",
      "717:\tlearn: 0.1592549\ttotal: 10s\tremaining: 3.93s\n",
      "718:\tlearn: 0.1591822\ttotal: 10s\tremaining: 3.91s\n",
      "719:\tlearn: 0.1591539\ttotal: 10s\tremaining: 3.9s\n",
      "720:\tlearn: 0.1590767\ttotal: 10s\tremaining: 3.88s\n",
      "721:\tlearn: 0.1590361\ttotal: 10.1s\tremaining: 3.87s\n",
      "722:\tlearn: 0.1589828\ttotal: 10.1s\tremaining: 3.86s\n",
      "723:\tlearn: 0.1589189\ttotal: 10.1s\tremaining: 3.84s\n",
      "724:\tlearn: 0.1588305\ttotal: 10.1s\tremaining: 3.83s\n",
      "725:\tlearn: 0.1587613\ttotal: 10.1s\tremaining: 3.81s\n",
      "726:\tlearn: 0.1587277\ttotal: 10.1s\tremaining: 3.8s\n",
      "727:\tlearn: 0.1586651\ttotal: 10.1s\tremaining: 3.78s\n",
      "728:\tlearn: 0.1586380\ttotal: 10.1s\tremaining: 3.77s\n",
      "729:\tlearn: 0.1585932\ttotal: 10.2s\tremaining: 3.75s\n",
      "730:\tlearn: 0.1585380\ttotal: 10.2s\tremaining: 3.74s\n",
      "731:\tlearn: 0.1585243\ttotal: 10.2s\tremaining: 3.73s\n",
      "732:\tlearn: 0.1583737\ttotal: 10.2s\tremaining: 3.71s\n",
      "733:\tlearn: 0.1583337\ttotal: 10.2s\tremaining: 3.7s\n",
      "734:\tlearn: 0.1582928\ttotal: 10.2s\tremaining: 3.69s\n",
      "735:\tlearn: 0.1582624\ttotal: 10.2s\tremaining: 3.67s\n",
      "736:\tlearn: 0.1582157\ttotal: 10.3s\tremaining: 3.66s\n",
      "737:\tlearn: 0.1581773\ttotal: 10.3s\tremaining: 3.64s\n",
      "738:\tlearn: 0.1580802\ttotal: 10.3s\tremaining: 3.63s\n",
      "739:\tlearn: 0.1580162\ttotal: 10.3s\tremaining: 3.62s\n",
      "740:\tlearn: 0.1579753\ttotal: 10.3s\tremaining: 3.6s\n",
      "741:\tlearn: 0.1579248\ttotal: 10.3s\tremaining: 3.59s\n",
      "742:\tlearn: 0.1578941\ttotal: 10.3s\tremaining: 3.57s\n",
      "743:\tlearn: 0.1578656\ttotal: 10.3s\tremaining: 3.56s\n",
      "744:\tlearn: 0.1578040\ttotal: 10.4s\tremaining: 3.54s\n",
      "745:\tlearn: 0.1577882\ttotal: 10.4s\tremaining: 3.53s\n",
      "746:\tlearn: 0.1577256\ttotal: 10.4s\tremaining: 3.52s\n",
      "747:\tlearn: 0.1576518\ttotal: 10.4s\tremaining: 3.5s\n",
      "748:\tlearn: 0.1576138\ttotal: 10.4s\tremaining: 3.49s\n",
      "749:\tlearn: 0.1575407\ttotal: 10.4s\tremaining: 3.48s\n",
      "750:\tlearn: 0.1574979\ttotal: 10.4s\tremaining: 3.46s\n",
      "751:\tlearn: 0.1574268\ttotal: 10.5s\tremaining: 3.45s\n",
      "752:\tlearn: 0.1573846\ttotal: 10.5s\tremaining: 3.43s\n",
      "753:\tlearn: 0.1573506\ttotal: 10.5s\tremaining: 3.42s\n",
      "754:\tlearn: 0.1573113\ttotal: 10.5s\tremaining: 3.4s\n",
      "755:\tlearn: 0.1572676\ttotal: 10.5s\tremaining: 3.39s\n",
      "756:\tlearn: 0.1572360\ttotal: 10.5s\tremaining: 3.38s\n",
      "757:\tlearn: 0.1571804\ttotal: 10.5s\tremaining: 3.36s\n",
      "758:\tlearn: 0.1571484\ttotal: 10.5s\tremaining: 3.35s\n",
      "759:\tlearn: 0.1571175\ttotal: 10.6s\tremaining: 3.33s\n",
      "760:\tlearn: 0.1570776\ttotal: 10.6s\tremaining: 3.32s\n",
      "761:\tlearn: 0.1570611\ttotal: 10.6s\tremaining: 3.31s\n",
      "762:\tlearn: 0.1569938\ttotal: 10.6s\tremaining: 3.29s\n",
      "763:\tlearn: 0.1569722\ttotal: 10.6s\tremaining: 3.28s\n",
      "764:\tlearn: 0.1569179\ttotal: 10.6s\tremaining: 3.27s\n",
      "765:\tlearn: 0.1568878\ttotal: 10.6s\tremaining: 3.25s\n",
      "766:\tlearn: 0.1568464\ttotal: 10.7s\tremaining: 3.24s\n",
      "767:\tlearn: 0.1568249\ttotal: 10.7s\tremaining: 3.22s\n",
      "768:\tlearn: 0.1568000\ttotal: 10.7s\tremaining: 3.21s\n",
      "769:\tlearn: 0.1567488\ttotal: 10.7s\tremaining: 3.19s\n",
      "770:\tlearn: 0.1567193\ttotal: 10.7s\tremaining: 3.18s\n",
      "771:\tlearn: 0.1566756\ttotal: 10.7s\tremaining: 3.17s\n",
      "772:\tlearn: 0.1566360\ttotal: 10.7s\tremaining: 3.15s\n",
      "773:\tlearn: 0.1565981\ttotal: 10.7s\tremaining: 3.14s\n",
      "774:\tlearn: 0.1565664\ttotal: 10.8s\tremaining: 3.12s\n",
      "775:\tlearn: 0.1565216\ttotal: 10.8s\tremaining: 3.11s\n",
      "776:\tlearn: 0.1564889\ttotal: 10.8s\tremaining: 3.1s\n",
      "777:\tlearn: 0.1563519\ttotal: 10.8s\tremaining: 3.08s\n",
      "778:\tlearn: 0.1563110\ttotal: 10.8s\tremaining: 3.07s\n",
      "779:\tlearn: 0.1562543\ttotal: 10.8s\tremaining: 3.05s\n",
      "780:\tlearn: 0.1562019\ttotal: 10.8s\tremaining: 3.04s\n",
      "781:\tlearn: 0.1561445\ttotal: 10.9s\tremaining: 3.02s\n",
      "782:\tlearn: 0.1560842\ttotal: 10.9s\tremaining: 3.01s\n",
      "783:\tlearn: 0.1560126\ttotal: 10.9s\tremaining: 3s\n",
      "784:\tlearn: 0.1559504\ttotal: 10.9s\tremaining: 2.98s\n",
      "785:\tlearn: 0.1558892\ttotal: 10.9s\tremaining: 2.97s\n",
      "786:\tlearn: 0.1558147\ttotal: 10.9s\tremaining: 2.95s\n",
      "787:\tlearn: 0.1557809\ttotal: 10.9s\tremaining: 2.94s\n",
      "788:\tlearn: 0.1557461\ttotal: 10.9s\tremaining: 2.93s\n",
      "789:\tlearn: 0.1556716\ttotal: 11s\tremaining: 2.91s\n",
      "790:\tlearn: 0.1556245\ttotal: 11s\tremaining: 2.9s\n",
      "791:\tlearn: 0.1555782\ttotal: 11s\tremaining: 2.88s\n",
      "792:\tlearn: 0.1555280\ttotal: 11s\tremaining: 2.87s\n",
      "793:\tlearn: 0.1554889\ttotal: 11s\tremaining: 2.86s\n",
      "794:\tlearn: 0.1554412\ttotal: 11s\tremaining: 2.84s\n",
      "795:\tlearn: 0.1554189\ttotal: 11s\tremaining: 2.83s\n",
      "796:\tlearn: 0.1553834\ttotal: 11.1s\tremaining: 2.81s\n",
      "797:\tlearn: 0.1553529\ttotal: 11.1s\tremaining: 2.8s\n",
      "798:\tlearn: 0.1552889\ttotal: 11.1s\tremaining: 2.79s\n",
      "799:\tlearn: 0.1551713\ttotal: 11.1s\tremaining: 2.77s\n",
      "800:\tlearn: 0.1551157\ttotal: 11.1s\tremaining: 2.76s\n",
      "801:\tlearn: 0.1550833\ttotal: 11.1s\tremaining: 2.74s\n",
      "802:\tlearn: 0.1550401\ttotal: 11.1s\tremaining: 2.73s\n",
      "803:\tlearn: 0.1549957\ttotal: 11.1s\tremaining: 2.72s\n",
      "804:\tlearn: 0.1549452\ttotal: 11.2s\tremaining: 2.7s\n",
      "805:\tlearn: 0.1549018\ttotal: 11.2s\tremaining: 2.69s\n",
      "806:\tlearn: 0.1548258\ttotal: 11.2s\tremaining: 2.67s\n",
      "807:\tlearn: 0.1547757\ttotal: 11.2s\tremaining: 2.66s\n",
      "808:\tlearn: 0.1547358\ttotal: 11.2s\tremaining: 2.65s\n",
      "809:\tlearn: 0.1546795\ttotal: 11.2s\tremaining: 2.63s\n",
      "810:\tlearn: 0.1546189\ttotal: 11.2s\tremaining: 2.62s\n",
      "811:\tlearn: 0.1545863\ttotal: 11.3s\tremaining: 2.6s\n",
      "812:\tlearn: 0.1545575\ttotal: 11.3s\tremaining: 2.59s\n",
      "813:\tlearn: 0.1544673\ttotal: 11.3s\tremaining: 2.58s\n",
      "814:\tlearn: 0.1544153\ttotal: 11.3s\tremaining: 2.56s\n",
      "815:\tlearn: 0.1544011\ttotal: 11.3s\tremaining: 2.55s\n",
      "816:\tlearn: 0.1543518\ttotal: 11.3s\tremaining: 2.54s\n",
      "817:\tlearn: 0.1543331\ttotal: 11.3s\tremaining: 2.52s\n",
      "818:\tlearn: 0.1543030\ttotal: 11.3s\tremaining: 2.51s\n",
      "819:\tlearn: 0.1542566\ttotal: 11.4s\tremaining: 2.49s\n",
      "820:\tlearn: 0.1542174\ttotal: 11.4s\tremaining: 2.48s\n",
      "821:\tlearn: 0.1541936\ttotal: 11.4s\tremaining: 2.47s\n",
      "822:\tlearn: 0.1541375\ttotal: 11.4s\tremaining: 2.45s\n",
      "823:\tlearn: 0.1540885\ttotal: 11.4s\tremaining: 2.44s\n",
      "824:\tlearn: 0.1540556\ttotal: 11.4s\tremaining: 2.43s\n",
      "825:\tlearn: 0.1540101\ttotal: 11.5s\tremaining: 2.41s\n",
      "826:\tlearn: 0.1539430\ttotal: 11.5s\tremaining: 2.4s\n",
      "827:\tlearn: 0.1538923\ttotal: 11.5s\tremaining: 2.38s\n",
      "828:\tlearn: 0.1538621\ttotal: 11.5s\tremaining: 2.37s\n",
      "829:\tlearn: 0.1538450\ttotal: 11.5s\tremaining: 2.36s\n",
      "830:\tlearn: 0.1537873\ttotal: 11.5s\tremaining: 2.34s\n",
      "831:\tlearn: 0.1537223\ttotal: 11.5s\tremaining: 2.33s\n",
      "832:\tlearn: 0.1536753\ttotal: 11.5s\tremaining: 2.31s\n",
      "833:\tlearn: 0.1536322\ttotal: 11.6s\tremaining: 2.3s\n",
      "834:\tlearn: 0.1535975\ttotal: 11.6s\tremaining: 2.29s\n",
      "835:\tlearn: 0.1535768\ttotal: 11.6s\tremaining: 2.27s\n",
      "836:\tlearn: 0.1535409\ttotal: 11.6s\tremaining: 2.26s\n",
      "837:\tlearn: 0.1535187\ttotal: 11.6s\tremaining: 2.25s\n",
      "838:\tlearn: 0.1535063\ttotal: 11.6s\tremaining: 2.23s\n",
      "839:\tlearn: 0.1534281\ttotal: 11.6s\tremaining: 2.22s\n",
      "840:\tlearn: 0.1533916\ttotal: 11.7s\tremaining: 2.2s\n",
      "841:\tlearn: 0.1533439\ttotal: 11.7s\tremaining: 2.19s\n",
      "842:\tlearn: 0.1533098\ttotal: 11.7s\tremaining: 2.18s\n",
      "843:\tlearn: 0.1532753\ttotal: 11.7s\tremaining: 2.16s\n",
      "844:\tlearn: 0.1532531\ttotal: 11.7s\tremaining: 2.15s\n",
      "845:\tlearn: 0.1532183\ttotal: 11.7s\tremaining: 2.13s\n",
      "846:\tlearn: 0.1531688\ttotal: 11.7s\tremaining: 2.12s\n",
      "847:\tlearn: 0.1531346\ttotal: 11.8s\tremaining: 2.11s\n",
      "848:\tlearn: 0.1530801\ttotal: 11.8s\tremaining: 2.09s\n",
      "849:\tlearn: 0.1530530\ttotal: 11.8s\tremaining: 2.08s\n",
      "850:\tlearn: 0.1530077\ttotal: 11.8s\tremaining: 2.06s\n",
      "851:\tlearn: 0.1529863\ttotal: 11.8s\tremaining: 2.05s\n",
      "852:\tlearn: 0.1529373\ttotal: 11.8s\tremaining: 2.04s\n",
      "853:\tlearn: 0.1528573\ttotal: 11.8s\tremaining: 2.02s\n",
      "854:\tlearn: 0.1528363\ttotal: 11.8s\tremaining: 2.01s\n",
      "855:\tlearn: 0.1527694\ttotal: 11.9s\tremaining: 1.99s\n",
      "856:\tlearn: 0.1527010\ttotal: 11.9s\tremaining: 1.98s\n",
      "857:\tlearn: 0.1526541\ttotal: 11.9s\tremaining: 1.97s\n",
      "858:\tlearn: 0.1525889\ttotal: 11.9s\tremaining: 1.95s\n",
      "859:\tlearn: 0.1525637\ttotal: 11.9s\tremaining: 1.94s\n",
      "860:\tlearn: 0.1525210\ttotal: 11.9s\tremaining: 1.93s\n",
      "861:\tlearn: 0.1524763\ttotal: 11.9s\tremaining: 1.91s\n",
      "862:\tlearn: 0.1524076\ttotal: 12s\tremaining: 1.9s\n",
      "863:\tlearn: 0.1523617\ttotal: 12s\tremaining: 1.88s\n",
      "864:\tlearn: 0.1523164\ttotal: 12s\tremaining: 1.87s\n",
      "865:\tlearn: 0.1522727\ttotal: 12s\tremaining: 1.86s\n",
      "866:\tlearn: 0.1522425\ttotal: 12s\tremaining: 1.84s\n",
      "867:\tlearn: 0.1522063\ttotal: 12s\tremaining: 1.83s\n",
      "868:\tlearn: 0.1521641\ttotal: 12s\tremaining: 1.81s\n",
      "869:\tlearn: 0.1521208\ttotal: 12s\tremaining: 1.8s\n",
      "870:\tlearn: 0.1520821\ttotal: 12.1s\tremaining: 1.79s\n",
      "871:\tlearn: 0.1520384\ttotal: 12.1s\tremaining: 1.77s\n",
      "872:\tlearn: 0.1520081\ttotal: 12.1s\tremaining: 1.76s\n",
      "873:\tlearn: 0.1519487\ttotal: 12.1s\tremaining: 1.74s\n",
      "874:\tlearn: 0.1519249\ttotal: 12.1s\tremaining: 1.73s\n",
      "875:\tlearn: 0.1518799\ttotal: 12.1s\tremaining: 1.72s\n",
      "876:\tlearn: 0.1518375\ttotal: 12.1s\tremaining: 1.7s\n",
      "877:\tlearn: 0.1517943\ttotal: 12.2s\tremaining: 1.69s\n",
      "878:\tlearn: 0.1517537\ttotal: 12.2s\tremaining: 1.67s\n",
      "879:\tlearn: 0.1517205\ttotal: 12.2s\tremaining: 1.66s\n",
      "880:\tlearn: 0.1516588\ttotal: 12.2s\tremaining: 1.65s\n",
      "881:\tlearn: 0.1516125\ttotal: 12.2s\tremaining: 1.63s\n",
      "882:\tlearn: 0.1515554\ttotal: 12.2s\tremaining: 1.62s\n",
      "883:\tlearn: 0.1515180\ttotal: 12.2s\tremaining: 1.61s\n",
      "884:\tlearn: 0.1514839\ttotal: 12.3s\tremaining: 1.59s\n",
      "885:\tlearn: 0.1514365\ttotal: 12.3s\tremaining: 1.58s\n",
      "886:\tlearn: 0.1514054\ttotal: 12.3s\tremaining: 1.56s\n",
      "887:\tlearn: 0.1513117\ttotal: 12.3s\tremaining: 1.55s\n",
      "888:\tlearn: 0.1512640\ttotal: 12.3s\tremaining: 1.54s\n",
      "889:\tlearn: 0.1512373\ttotal: 12.3s\tremaining: 1.52s\n",
      "890:\tlearn: 0.1512049\ttotal: 12.3s\tremaining: 1.51s\n",
      "891:\tlearn: 0.1511521\ttotal: 12.3s\tremaining: 1.5s\n",
      "892:\tlearn: 0.1511235\ttotal: 12.4s\tremaining: 1.48s\n",
      "893:\tlearn: 0.1510906\ttotal: 12.4s\tremaining: 1.47s\n",
      "894:\tlearn: 0.1510388\ttotal: 12.4s\tremaining: 1.45s\n",
      "895:\tlearn: 0.1510050\ttotal: 12.4s\tremaining: 1.44s\n",
      "896:\tlearn: 0.1509873\ttotal: 12.4s\tremaining: 1.43s\n",
      "897:\tlearn: 0.1509505\ttotal: 12.4s\tremaining: 1.41s\n",
      "898:\tlearn: 0.1509036\ttotal: 12.4s\tremaining: 1.4s\n",
      "899:\tlearn: 0.1508833\ttotal: 12.5s\tremaining: 1.38s\n",
      "900:\tlearn: 0.1508479\ttotal: 12.5s\tremaining: 1.37s\n",
      "901:\tlearn: 0.1507892\ttotal: 12.5s\tremaining: 1.36s\n",
      "902:\tlearn: 0.1507548\ttotal: 12.5s\tremaining: 1.34s\n",
      "903:\tlearn: 0.1507167\ttotal: 12.5s\tremaining: 1.33s\n",
      "904:\tlearn: 0.1506533\ttotal: 12.5s\tremaining: 1.31s\n",
      "905:\tlearn: 0.1506081\ttotal: 12.5s\tremaining: 1.3s\n",
      "906:\tlearn: 0.1505558\ttotal: 12.5s\tremaining: 1.29s\n",
      "907:\tlearn: 0.1505176\ttotal: 12.6s\tremaining: 1.27s\n",
      "908:\tlearn: 0.1504905\ttotal: 12.6s\tremaining: 1.26s\n",
      "909:\tlearn: 0.1504419\ttotal: 12.6s\tremaining: 1.25s\n",
      "910:\tlearn: 0.1504309\ttotal: 12.6s\tremaining: 1.23s\n",
      "911:\tlearn: 0.1504047\ttotal: 12.6s\tremaining: 1.22s\n",
      "912:\tlearn: 0.1503726\ttotal: 12.6s\tremaining: 1.2s\n",
      "913:\tlearn: 0.1503311\ttotal: 12.6s\tremaining: 1.19s\n",
      "914:\tlearn: 0.1502947\ttotal: 12.7s\tremaining: 1.18s\n",
      "915:\tlearn: 0.1502688\ttotal: 12.7s\tremaining: 1.16s\n",
      "916:\tlearn: 0.1502346\ttotal: 12.7s\tremaining: 1.15s\n",
      "917:\tlearn: 0.1501920\ttotal: 12.7s\tremaining: 1.13s\n",
      "918:\tlearn: 0.1501551\ttotal: 12.7s\tremaining: 1.12s\n",
      "919:\tlearn: 0.1501122\ttotal: 12.7s\tremaining: 1.11s\n",
      "920:\tlearn: 0.1500702\ttotal: 12.8s\tremaining: 1.09s\n",
      "921:\tlearn: 0.1500365\ttotal: 12.8s\tremaining: 1.08s\n",
      "922:\tlearn: 0.1500004\ttotal: 12.8s\tremaining: 1.07s\n",
      "923:\tlearn: 0.1499865\ttotal: 12.8s\tremaining: 1.05s\n",
      "924:\tlearn: 0.1499639\ttotal: 12.8s\tremaining: 1.04s\n",
      "925:\tlearn: 0.1498984\ttotal: 12.8s\tremaining: 1.02s\n",
      "926:\tlearn: 0.1498518\ttotal: 12.9s\tremaining: 1.01s\n",
      "927:\tlearn: 0.1498196\ttotal: 12.9s\tremaining: 998ms\n",
      "928:\tlearn: 0.1497511\ttotal: 12.9s\tremaining: 984ms\n",
      "929:\tlearn: 0.1497311\ttotal: 12.9s\tremaining: 970ms\n",
      "930:\tlearn: 0.1497151\ttotal: 12.9s\tremaining: 956ms\n",
      "931:\tlearn: 0.1496566\ttotal: 12.9s\tremaining: 942ms\n",
      "932:\tlearn: 0.1496064\ttotal: 12.9s\tremaining: 929ms\n",
      "933:\tlearn: 0.1495623\ttotal: 12.9s\tremaining: 915ms\n",
      "934:\tlearn: 0.1495221\ttotal: 13s\tremaining: 901ms\n",
      "935:\tlearn: 0.1494812\ttotal: 13s\tremaining: 887ms\n",
      "936:\tlearn: 0.1494399\ttotal: 13s\tremaining: 873ms\n",
      "937:\tlearn: 0.1494058\ttotal: 13s\tremaining: 859ms\n",
      "938:\tlearn: 0.1493501\ttotal: 13s\tremaining: 846ms\n",
      "939:\tlearn: 0.1492992\ttotal: 13s\tremaining: 832ms\n",
      "940:\tlearn: 0.1492721\ttotal: 13s\tremaining: 818ms\n",
      "941:\tlearn: 0.1492296\ttotal: 13.1s\tremaining: 804ms\n",
      "942:\tlearn: 0.1491692\ttotal: 13.1s\tremaining: 790ms\n",
      "943:\tlearn: 0.1491445\ttotal: 13.1s\tremaining: 776ms\n",
      "944:\tlearn: 0.1490938\ttotal: 13.1s\tremaining: 762ms\n",
      "945:\tlearn: 0.1490542\ttotal: 13.1s\tremaining: 748ms\n",
      "946:\tlearn: 0.1490091\ttotal: 13.1s\tremaining: 734ms\n",
      "947:\tlearn: 0.1489850\ttotal: 13.1s\tremaining: 720ms\n",
      "948:\tlearn: 0.1489243\ttotal: 13.1s\tremaining: 706ms\n",
      "949:\tlearn: 0.1488887\ttotal: 13.2s\tremaining: 692ms\n",
      "950:\tlearn: 0.1488461\ttotal: 13.2s\tremaining: 679ms\n",
      "951:\tlearn: 0.1487999\ttotal: 13.2s\tremaining: 665ms\n",
      "952:\tlearn: 0.1487608\ttotal: 13.2s\tremaining: 651ms\n",
      "953:\tlearn: 0.1487001\ttotal: 13.2s\tremaining: 637ms\n",
      "954:\tlearn: 0.1486342\ttotal: 13.2s\tremaining: 623ms\n",
      "955:\tlearn: 0.1485594\ttotal: 13.2s\tremaining: 610ms\n",
      "956:\tlearn: 0.1485211\ttotal: 13.3s\tremaining: 596ms\n",
      "957:\tlearn: 0.1484844\ttotal: 13.3s\tremaining: 582ms\n",
      "958:\tlearn: 0.1484429\ttotal: 13.3s\tremaining: 568ms\n",
      "959:\tlearn: 0.1484049\ttotal: 13.3s\tremaining: 554ms\n",
      "960:\tlearn: 0.1483745\ttotal: 13.3s\tremaining: 540ms\n",
      "961:\tlearn: 0.1483500\ttotal: 13.3s\tremaining: 526ms\n",
      "962:\tlearn: 0.1483148\ttotal: 13.3s\tremaining: 512ms\n",
      "963:\tlearn: 0.1482763\ttotal: 13.3s\tremaining: 499ms\n",
      "964:\tlearn: 0.1482341\ttotal: 13.4s\tremaining: 485ms\n",
      "965:\tlearn: 0.1482008\ttotal: 13.4s\tremaining: 471ms\n",
      "966:\tlearn: 0.1481472\ttotal: 13.4s\tremaining: 457ms\n",
      "967:\tlearn: 0.1480899\ttotal: 13.4s\tremaining: 443ms\n",
      "968:\tlearn: 0.1480549\ttotal: 13.4s\tremaining: 429ms\n",
      "969:\tlearn: 0.1480103\ttotal: 13.4s\tremaining: 415ms\n",
      "970:\tlearn: 0.1479898\ttotal: 13.4s\tremaining: 402ms\n",
      "971:\tlearn: 0.1479425\ttotal: 13.5s\tremaining: 388ms\n",
      "972:\tlearn: 0.1478970\ttotal: 13.5s\tremaining: 374ms\n",
      "973:\tlearn: 0.1478765\ttotal: 13.5s\tremaining: 360ms\n",
      "974:\tlearn: 0.1478376\ttotal: 13.5s\tremaining: 346ms\n",
      "975:\tlearn: 0.1478125\ttotal: 13.5s\tremaining: 333ms\n",
      "976:\tlearn: 0.1476802\ttotal: 13.5s\tremaining: 319ms\n",
      "977:\tlearn: 0.1476596\ttotal: 13.5s\tremaining: 305ms\n",
      "978:\tlearn: 0.1476280\ttotal: 13.6s\tremaining: 291ms\n",
      "979:\tlearn: 0.1475187\ttotal: 13.6s\tremaining: 278ms\n",
      "980:\tlearn: 0.1474756\ttotal: 13.6s\tremaining: 264ms\n",
      "981:\tlearn: 0.1474033\ttotal: 13.6s\tremaining: 250ms\n",
      "982:\tlearn: 0.1473819\ttotal: 13.7s\tremaining: 236ms\n",
      "983:\tlearn: 0.1473369\ttotal: 13.7s\tremaining: 222ms\n",
      "984:\tlearn: 0.1472707\ttotal: 13.7s\tremaining: 208ms\n",
      "985:\tlearn: 0.1472438\ttotal: 13.7s\tremaining: 194ms\n",
      "986:\tlearn: 0.1472184\ttotal: 13.7s\tremaining: 181ms\n",
      "987:\tlearn: 0.1471707\ttotal: 13.7s\tremaining: 167ms\n",
      "988:\tlearn: 0.1471010\ttotal: 13.7s\tremaining: 153ms\n",
      "989:\tlearn: 0.1470307\ttotal: 13.8s\tremaining: 139ms\n",
      "990:\tlearn: 0.1470099\ttotal: 13.8s\tremaining: 125ms\n",
      "991:\tlearn: 0.1469698\ttotal: 13.8s\tremaining: 111ms\n",
      "992:\tlearn: 0.1469292\ttotal: 13.8s\tremaining: 97.2ms\n",
      "993:\tlearn: 0.1468835\ttotal: 13.8s\tremaining: 83.3ms\n",
      "994:\tlearn: 0.1468590\ttotal: 13.8s\tremaining: 69.4ms\n",
      "995:\tlearn: 0.1468088\ttotal: 13.8s\tremaining: 55.5ms\n",
      "996:\tlearn: 0.1467723\ttotal: 13.8s\tremaining: 41.7ms\n",
      "997:\tlearn: 0.1467187\ttotal: 13.9s\tremaining: 27.8ms\n",
      "998:\tlearn: 0.1466798\ttotal: 13.9s\tremaining: 13.9ms\n",
      "999:\tlearn: 0.1466520\ttotal: 13.9s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall mean</th>\n",
       "      <th>recall std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.602336</td>\n",
       "      <td>0.013978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.381571</td>\n",
       "      <td>0.016798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.305899</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.149730</td>\n",
       "      <td>0.012931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.018113</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.014692</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     recall mean  recall std\n",
       "model                                       \n",
       "Logistic Regression     0.602336    0.013978\n",
       "KNN                     0.381571    0.016798\n",
       "AdaBoost                0.305899    0.012076\n",
       "Decision Tree           0.149730    0.012931\n",
       "Random Forest           0.022135    0.007606\n",
       "XGBoost                 0.018113    0.002852\n",
       "CatBoost                0.014692    0.002076"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find benchmark model\n",
    "models = [logreg, knn, dt, rf, adb, xgb, cat]\n",
    "score = []\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "for i in models:\n",
    "    estimator = Pipeline([\n",
    "        ('preprocess',transformer),\n",
    "        ('scale', scaler),\n",
    "        ('resampler', smote),\n",
    "        ('model',i)\n",
    "        ])\n",
    "\n",
    "    model_cv = cross_val_score(\n",
    "        estimator,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring='recall')\n",
    "\n",
    "    score.append(model_cv)\n",
    "    mean.append(model_cv.mean())\n",
    "    std.append(model_cv.std())\n",
    "    \n",
    "pd.DataFrame({\n",
    "    'model':['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest', 'AdaBoost', 'XGBoost', 'CatBoost'],\n",
    "    'recall mean':mean,'recall std':std}).set_index('model').sort_values(by='recall mean',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our benchmark model which is Logistic Regression with recall mean 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.61      0.74     13558\n",
      "           1       0.12      0.58      0.20      1203\n",
      "\n",
      "    accuracy                           0.61     14761\n",
      "   macro avg       0.53      0.60      0.47     14761\n",
      "weighted avg       0.88      0.61      0.70     14761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model before hyperparameter tuning\n",
    "base_model = Pipeline([\n",
    "        ('transform', transformer),\n",
    "        ('scale', scaler),\n",
    "        ('resampler', smote),\n",
    "        ('model', logreg)\n",
    "    ])\n",
    "\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_base = base_model.predict(X_test)\n",
    "\n",
    "# Check classification report\n",
    "print(classification_report(y_test, y_pred_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "175 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Margaretha Elaine\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "One or more of the test scores are non-finite: [0.59951856        nan 0.60374574        nan        nan        nan\n",
      " 0.59992138 0.59972038 0.59992138 0.60475238        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.60032359\n",
      "        nan        nan 0.60414896 0.59911615 0.60012239 0.60012279\n",
      " 0.60012239        nan 0.60173326        nan 0.59831112        nan\n",
      "        nan        nan        nan        nan        nan 0.59911635\n",
      "        nan        nan 0.60133125        nan        nan        nan\n",
      " 0.60012279        nan        nan 0.60052561        nan        nan\n",
      "        nan        nan 0.60495379 0.59992118        nan 0.59972017\n",
      " 0.60475198 0.60012239 0.59931756        nan        nan 0.60112923]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('preprocess',\n",
       "                                              ColumnTransformer(remainder='passthrough',\n",
       "                                                                transformers=[('onehot',\n",
       "                                                                               OneHotEncoder(drop='first'),\n",
       "                                                                               Index(['CONTRACT_TYPE', 'GENDER', 'INCOME_TYPE', 'EDUCATION', 'FAMILY_STATUS',\n",
       "       'HOUSING_TYPE', 'WEEKDAYS_APPLY', 'Inst-Behav', 'HOUR_BIN'],\n",
       "      dtype='object'))])),\n",
       "                                             ('scale', RobustScaler()),\n",
       "                                             ('resampler',\n",
       "                                              SMOTE(random_state=0)),\n",
       "                                             ('model',\n",
       "                                              LogisticRegression(max_iter=1000,\n",
       "                                                                 random_state=0))]),\n",
       "                   n_iter=60,\n",
       "                   param_distributions={'model__C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        'model__penalty': ['l1', 'l2',\n",
       "                                                           'elasticnet'],\n",
       "                                        'model__solver': ['newton-cg', 'lbfgs',\n",
       "                                                          'liblinear', 'sag',\n",
       "                                                          'saga']},\n",
       "                   random_state=0, scoring='recall')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning the benchmark model to get better score\n",
    "logreg = LogisticRegression(random_state=0, max_iter=1000,dual=False)\n",
    "logreg_pipe = Pipeline([\n",
    "    ('preprocess', transformer),\n",
    "    ('scale', scaler),\n",
    "    ('resampler', smote),\n",
    "    ('model', logreg)\n",
    "])\n",
    "\n",
    "hyperparam_space = {\n",
    " 'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    " 'model__penalty': ['l1','l2','elasticnet'],\n",
    " 'model__C': [100,10,1.0,0.1,0.01]\n",
    "}\n",
    "\n",
    "# Took too long with gridsearch, thus changed to randomizedsearch\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=logreg_pipe,\n",
    "#     param_grid=hyperparam_space,\n",
    "#     cv=10,\n",
    "#     scoring='recall'\n",
    "# )\n",
    "\n",
    "# # fitting gridsearch\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=logreg_pipe,\n",
    "    param_distributions=hyperparam_space,\n",
    "    cv=5,\n",
    "    n_iter=60,\n",
    "    random_state=0,\n",
    "    scoring='recall'\n",
    ")\n",
    "\n",
    "# Fitting random search\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best recall score:  0.6049537912267158\n",
      "Best param:  {'model__solver': 'newton-cg', 'model__penalty': 'l2', 'model__C': 100}\n"
     ]
    }
   ],
   "source": [
    "# The tuning increases the recall score, although not significant\n",
    "print('Best recall score: ', random_search.best_score_)\n",
    "print('Best param: ', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.62      0.75     13558\n",
      "           1       0.12      0.59      0.20      1203\n",
      "\n",
      "    accuracy                           0.62     14761\n",
      "   macro avg       0.53      0.60      0.47     14761\n",
      "weighted avg       0.88      0.62      0.70     14761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification report after tuning\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, max_iter=1000, random_state=0, solver='newton-cg')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the best model to find feature importance (we will not use smote, becase it keeps getting error)\n",
    "logreg_best = LogisticRegression(random_state=0, max_iter=1000,dual=False, solver='newton-cg',penalty='l2',C=100)\n",
    "\n",
    "# Encode & Scale manually\n",
    "X_train_encode = transformer.fit_transform(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train_encode)\n",
    "\n",
    "# Fitting the model\n",
    "logreg_best.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.575414</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>0.195291</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>-1.317505</td>\n",
       "      <td>0.492614</td>\n",
       "      <td>0.311849</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.194795</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>-0.773117</td>\n",
       "      <td>-0.404863</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>-3.721127</td>\n",
       "      <td>-0.091191</td>\n",
       "      <td>-0.288306</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.205979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.617185</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.177640</td>\n",
       "      <td>0.624855</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.152247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.115931</td>\n",
       "      <td>-1.853119</td>\n",
       "      <td>0.398504</td>\n",
       "      <td>0.213817</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.912418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.727845</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>0.071494</td>\n",
       "      <td>-0.358163</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61499</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.719490</td>\n",
       "      <td>-0.143260</td>\n",
       "      <td>-1.339777</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.283016</td>\n",
       "      <td>-1.187525</td>\n",
       "      <td>-0.102499</td>\n",
       "      <td>-0.167889</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.573074</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>-1.513040</td>\n",
       "      <td>-0.324971</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61502 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9   ...   30   31  \\\n",
       "0      1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -1.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0  0.0  0.0  ... -1.0  0.0   \n",
       "2      0.0  0.0  1.0  0.0  0.0  0.0 -1.0  0.0  0.0  0.0  ... -1.0  0.0   \n",
       "3      0.0  1.0  0.0  1.0  0.0  0.0 -1.0  1.0  0.0  0.0  ...  0.0  0.0   \n",
       "4      1.0  1.0  0.0  0.0  0.0  0.0 -1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "61497  0.0  1.0  0.0  1.0  0.0  0.0 -1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "61498  0.0  0.0  1.0  0.0  0.0  0.0 -1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "61499  1.0  0.0  0.0  0.0  0.0  0.0 -1.0  1.0  0.0  0.0  ... -1.0  0.0   \n",
       "61500  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -1.0  0.0   \n",
       "61501  0.0  0.0  1.0  0.0  0.0  0.0 -1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "             32   33    34        35          36        37        38    39  \n",
       "0      0.308837  1.0 -0.90 -0.575414    0.115493  0.798650  0.195291  0.75  \n",
       "1      0.718019  1.0  0.60  0.861905   -1.317505  0.492614  0.311849  0.00  \n",
       "2     -0.107632  0.0  0.35 -0.194795  147.465191 -0.773117 -0.404863 -0.25  \n",
       "3      0.747980  0.0  0.85  0.549900   -3.721127 -0.091191 -0.288306  0.75  \n",
       "4     -0.205979  0.0 -0.15 -0.617185    0.007243  0.177640  0.624855  0.00  \n",
       "...         ...  ...   ...       ...         ...       ...       ...   ...  \n",
       "61497 -0.152247  2.0  2.35 -0.115931   -1.853119  0.398504  0.213817  0.00  \n",
       "61498  0.912418  0.0  3.25  0.727845  147.465191  0.071494 -0.358163  0.00  \n",
       "61499 -0.524516  0.0  0.60  0.719490   -0.143260 -1.339777  0.771903 -0.75  \n",
       "61500  0.199375  0.0 -0.60 -0.283016   -1.187525 -0.102499 -0.167889 -0.25  \n",
       "61501  0.083050  0.0 -0.65 -0.573074  147.465191 -1.513040 -0.324971  0.75  \n",
       "\n",
       "[61502 rows x 40 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_DF = pd.DataFrame(X_train_scaled)\n",
    "X_train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_Revolving loans</th>\n",
       "      <th>x1_M</th>\n",
       "      <th>x2_Pensioner</th>\n",
       "      <th>x2_State servant</th>\n",
       "      <th>x2_Student</th>\n",
       "      <th>x2_Unemployed</th>\n",
       "      <th>x2_Working</th>\n",
       "      <th>x3_Higher education</th>\n",
       "      <th>x3_Incomplete higher</th>\n",
       "      <th>x3_Lower secondary</th>\n",
       "      <th>...</th>\n",
       "      <th>x8_Morning</th>\n",
       "      <th>x8_Night</th>\n",
       "      <th>LN_ID</th>\n",
       "      <th>NUM_CHILDREN</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>APPROVED_CREDIT</th>\n",
       "      <th>DAYS_WORK</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_CHANGE</th>\n",
       "      <th>Approval Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.575414</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>0.195291</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>-1.317505</td>\n",
       "      <td>0.492614</td>\n",
       "      <td>0.311849</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.107632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.194795</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>-0.773117</td>\n",
       "      <td>-0.404863</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>-3.721127</td>\n",
       "      <td>-0.091191</td>\n",
       "      <td>-0.288306</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.205979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.617185</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.177640</td>\n",
       "      <td>0.624855</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.152247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.115931</td>\n",
       "      <td>-1.853119</td>\n",
       "      <td>0.398504</td>\n",
       "      <td>0.213817</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.912418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.727845</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>0.071494</td>\n",
       "      <td>-0.358163</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61499</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.524516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.719490</td>\n",
       "      <td>-0.143260</td>\n",
       "      <td>-1.339777</td>\n",
       "      <td>0.771903</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.283016</td>\n",
       "      <td>-1.187525</td>\n",
       "      <td>-0.102499</td>\n",
       "      <td>-0.167889</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.573074</td>\n",
       "      <td>147.465191</td>\n",
       "      <td>-1.513040</td>\n",
       "      <td>-0.324971</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61502 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0_Revolving loans  x1_M  x2_Pensioner  x2_State servant  x2_Student  \\\n",
       "0                     1.0   0.0           0.0               0.0         0.0   \n",
       "1                     0.0   0.0           0.0               0.0         0.0   \n",
       "2                     0.0   0.0           1.0               0.0         0.0   \n",
       "3                     0.0   1.0           0.0               1.0         0.0   \n",
       "4                     1.0   1.0           0.0               0.0         0.0   \n",
       "...                   ...   ...           ...               ...         ...   \n",
       "61497                 0.0   1.0           0.0               1.0         0.0   \n",
       "61498                 0.0   0.0           1.0               0.0         0.0   \n",
       "61499                 1.0   0.0           0.0               0.0         0.0   \n",
       "61500                 0.0   0.0           0.0               0.0         0.0   \n",
       "61501                 0.0   0.0           1.0               0.0         0.0   \n",
       "\n",
       "       x2_Unemployed  x2_Working  x3_Higher education  x3_Incomplete higher  \\\n",
       "0                0.0         0.0                  0.0                   0.0   \n",
       "1                0.0        -1.0                  0.0                   0.0   \n",
       "2                0.0        -1.0                  0.0                   0.0   \n",
       "3                0.0        -1.0                  1.0                   0.0   \n",
       "4                0.0        -1.0                  0.0                   0.0   \n",
       "...              ...         ...                  ...                   ...   \n",
       "61497            0.0        -1.0                  0.0                   0.0   \n",
       "61498            0.0        -1.0                  0.0                   0.0   \n",
       "61499            0.0        -1.0                  1.0                   0.0   \n",
       "61500            0.0         0.0                  0.0                   0.0   \n",
       "61501            0.0        -1.0                  0.0                   0.0   \n",
       "\n",
       "       x3_Lower secondary  ...  x8_Morning  x8_Night     LN_ID  NUM_CHILDREN  \\\n",
       "0                     0.0  ...        -1.0       0.0  0.308837           1.0   \n",
       "1                     0.0  ...        -1.0       0.0  0.718019           1.0   \n",
       "2                     0.0  ...        -1.0       0.0 -0.107632           0.0   \n",
       "3                     0.0  ...         0.0       0.0  0.747980           0.0   \n",
       "4                     0.0  ...         0.0       0.0 -0.205979           0.0   \n",
       "...                   ...  ...         ...       ...       ...           ...   \n",
       "61497                 0.0  ...         0.0       0.0 -0.152247           2.0   \n",
       "61498                 0.0  ...         0.0       0.0  0.912418           0.0   \n",
       "61499                 0.0  ...        -1.0       0.0 -0.524516           0.0   \n",
       "61500                 0.0  ...        -1.0       0.0  0.199375           0.0   \n",
       "61501                 0.0  ...         0.0       0.0  0.083050           0.0   \n",
       "\n",
       "       INCOME  APPROVED_CREDIT   DAYS_WORK  DAYS_REGISTRATION  DAYS_ID_CHANGE  \\\n",
       "0       -0.90        -0.575414    0.115493           0.798650        0.195291   \n",
       "1        0.60         0.861905   -1.317505           0.492614        0.311849   \n",
       "2        0.35        -0.194795  147.465191          -0.773117       -0.404863   \n",
       "3        0.85         0.549900   -3.721127          -0.091191       -0.288306   \n",
       "4       -0.15        -0.617185    0.007243           0.177640        0.624855   \n",
       "...       ...              ...         ...                ...             ...   \n",
       "61497    2.35        -0.115931   -1.853119           0.398504        0.213817   \n",
       "61498    3.25         0.727845  147.465191           0.071494       -0.358163   \n",
       "61499    0.60         0.719490   -0.143260          -1.339777        0.771903   \n",
       "61500   -0.60        -0.283016   -1.187525          -0.102499       -0.167889   \n",
       "61501   -0.65        -0.573074  147.465191          -1.513040       -0.324971   \n",
       "\n",
       "       Approval Rate  \n",
       "0               0.75  \n",
       "1               0.00  \n",
       "2              -0.25  \n",
       "3               0.75  \n",
       "4               0.00  \n",
       "...              ...  \n",
       "61497           0.00  \n",
       "61498           0.00  \n",
       "61499          -0.75  \n",
       "61500          -0.25  \n",
       "61501           0.75  \n",
       "\n",
       "[61502 rows x 40 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine column names\n",
    "features = list(transformer.transformers_[0][1].get_feature_names()) + list(con_var.drop(['TARGET']))\n",
    "\n",
    "# Set the feature names on the dataframe\n",
    "X_train_DF.columns = features\n",
    "\n",
    "X_train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the coefficient of the model\n",
    "importance = logreg_best.coef_[0]\n",
    "\n",
    "# Plot barchart\n",
    "feat_importances = pd.Series(importance, index=X_train_DF.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh', title = 'Feature Importances')\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLANATION**\n",
    "We can see that the top features that have high importance to our model are from EDUCATION column, followed by PAYMENT BEHAVIOUR, HOUSING TYPE, GENDER, etc.\n",
    "In this chart we cannot define whether how the features affect our model (whether if the values are low or high), that is why we will use SHAP to check the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "Using 1000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  0%|          | 1/1000 [00:06<1:52:27,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  0%|          | 2/1000 [00:13<1:52:23,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  0%|          | 3/1000 [00:20<1:52:07,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  0%|          | 4/1000 [00:27<1:52:05,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  0%|          | 5/1000 [00:33<1:51:54,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 6/1000 [00:40<1:51:09,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 7/1000 [00:47<1:50:44,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 8/1000 [00:53<1:51:20,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 9/1000 [01:00<1:51:06,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 10/1000 [01:07<1:50:36,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 11/1000 [01:13<1:50:25,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|          | 12/1000 [01:20<1:50:16,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|▏         | 13/1000 [01:27<1:50:39,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  1%|▏         | 14/1000 [01:34<1:50:44,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 15/1000 [01:40<1:50:52,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 16/1000 [01:47<1:50:25,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 17/1000 [01:54<1:49:58,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 18/1000 [02:01<1:50:02,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 19/1000 [02:07<1:50:12,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 20/1000 [02:14<1:50:04,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 21/1000 [02:21<1:49:54,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 22/1000 [02:28<1:49:43,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 23/1000 [02:34<1:49:43,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▏         | 24/1000 [02:41<1:50:08,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  2%|▎         | 25/1000 [02:48<1:49:28,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 26/1000 [02:54<1:49:06,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 27/1000 [03:01<1:48:57,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 28/1000 [03:08<1:48:40,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 29/1000 [03:15<1:48:20,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 30/1000 [03:21<1:48:15,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 31/1000 [03:28<1:48:03,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 32/1000 [03:35<1:47:46,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 33/1000 [03:41<1:47:53,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|▎         | 34/1000 [03:48<1:47:53,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▎         | 35/1000 [03:55<1:48:35,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▎         | 36/1000 [04:02<1:48:11,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▎         | 37/1000 [04:08<1:48:32,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 38/1000 [04:15<1:48:06,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 39/1000 [04:22<1:48:00,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 40/1000 [04:29<1:47:47,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 41/1000 [04:35<1:47:29,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 42/1000 [04:42<1:47:23,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 43/1000 [04:49<1:47:11,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 44/1000 [04:55<1:46:58,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  4%|▍         | 45/1000 [05:02<1:46:44,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▍         | 46/1000 [05:09<1:46:40,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▍         | 47/1000 [05:15<1:46:23,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▍         | 48/1000 [05:22<1:46:12,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▍         | 49/1000 [05:29<1:46:29,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▌         | 50/1000 [05:36<1:46:28,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▌         | 51/1000 [05:42<1:46:06,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▌         | 52/1000 [05:49<1:46:07,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▌         | 53/1000 [05:56<1:45:40,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  5%|▌         | 54/1000 [06:02<1:45:30,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 55/1000 [06:09<1:45:41,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 56/1000 [06:16<1:45:42,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 57/1000 [06:23<1:46:14,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 58/1000 [06:29<1:45:55,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 59/1000 [06:36<1:45:31,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 60/1000 [06:43<1:45:19,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 61/1000 [06:50<1:45:43,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▌         | 62/1000 [06:56<1:45:23,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▋         | 63/1000 [07:03<1:45:04,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▋         | 64/1000 [07:10<1:44:57,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  6%|▋         | 65/1000 [07:17<1:44:42,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 66/1000 [07:23<1:44:19,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 67/1000 [07:30<1:44:10,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 68/1000 [07:37<1:44:03,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 69/1000 [07:43<1:43:50,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 70/1000 [07:50<1:43:49,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 71/1000 [07:57<1:43:50,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 72/1000 [08:03<1:43:43,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 73/1000 [08:10<1:44:20,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|▋         | 74/1000 [08:17<1:44:01,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 75/1000 [08:24<1:44:15,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 76/1000 [08:30<1:43:52,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 77/1000 [08:37<1:43:27,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 78/1000 [08:44<1:43:04,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 79/1000 [08:51<1:42:53,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 80/1000 [08:57<1:42:51,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 81/1000 [09:04<1:42:35,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 82/1000 [09:11<1:42:33,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 83/1000 [09:17<1:42:30,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 84/1000 [09:24<1:42:26,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  8%|▊         | 85/1000 [09:31<1:42:19,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▊         | 86/1000 [09:37<1:42:01,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▊         | 87/1000 [09:44<1:41:50,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 88/1000 [09:51<1:41:34,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 89/1000 [09:57<1:41:33,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 90/1000 [10:04<1:41:30,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 91/1000 [10:11<1:41:39,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 92/1000 [10:18<1:41:40,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 93/1000 [10:24<1:41:22,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  9%|▉         | 94/1000 [10:31<1:41:14,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|▉         | 95/1000 [10:38<1:41:10,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|▉         | 96/1000 [10:44<1:40:59,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|▉         | 97/1000 [10:51<1:41:31,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|▉         | 98/1000 [10:58<1:41:08,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|▉         | 99/1000 [11:05<1:41:31,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 100/1000 [11:12<1:41:11,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 101/1000 [11:18<1:40:40,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 102/1000 [11:25<1:40:26,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 103/1000 [11:32<1:40:21,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 104/1000 [11:38<1:40:01,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|█         | 105/1000 [11:45<1:39:53,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 106/1000 [11:52<1:40:01,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 107/1000 [11:58<1:39:53,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 108/1000 [12:05<1:39:42,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 109/1000 [12:12<1:39:21,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 110/1000 [12:18<1:39:04,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 111/1000 [12:25<1:39:03,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█         | 112/1000 [12:32<1:39:02,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█▏        | 113/1000 [12:39<1:38:48,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 11%|█▏        | 114/1000 [12:45<1:38:39,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 115/1000 [12:52<1:38:36,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 116/1000 [12:59<1:38:35,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 117/1000 [13:05<1:38:26,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 118/1000 [13:12<1:38:16,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 119/1000 [13:19<1:38:04,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 120/1000 [13:25<1:38:14,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 121/1000 [13:32<1:38:06,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 122/1000 [13:39<1:37:43,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 123/1000 [13:45<1:37:35,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▏        | 124/1000 [13:52<1:37:24,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 12%|█▎        | 125/1000 [13:59<1:37:15,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 126/1000 [14:05<1:37:15,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 127/1000 [14:12<1:37:20,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 128/1000 [14:19<1:37:12,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 129/1000 [14:25<1:37:13,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 130/1000 [14:32<1:37:33,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 131/1000 [14:39<1:36:59,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 132/1000 [14:46<1:36:48,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 133/1000 [14:52<1:37:08,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|█▎        | 134/1000 [14:59<1:36:59,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▎        | 135/1000 [15:06<1:36:48,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▎        | 136/1000 [15:13<1:36:52,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▎        | 137/1000 [15:19<1:36:49,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 138/1000 [15:26<1:36:29,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 139/1000 [15:33<1:36:23,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 140/1000 [15:39<1:36:07,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 141/1000 [15:46<1:36:08,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 142/1000 [15:53<1:35:51,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 143/1000 [15:59<1:35:36,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 144/1000 [16:06<1:35:28,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 14%|█▍        | 145/1000 [16:13<1:35:47,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▍        | 146/1000 [16:20<1:35:43,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▍        | 147/1000 [16:26<1:35:30,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▍        | 148/1000 [16:33<1:35:23,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▍        | 149/1000 [16:40<1:35:13,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▌        | 150/1000 [16:47<1:35:13,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▌        | 151/1000 [16:53<1:35:05,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▌        | 152/1000 [17:00<1:34:49,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▌        | 153/1000 [17:07<1:34:24,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 15%|█▌        | 154/1000 [17:13<1:34:02,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 155/1000 [17:20<1:33:51,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 156/1000 [17:27<1:34:21,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 157/1000 [17:33<1:34:14,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 158/1000 [17:40<1:33:55,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 159/1000 [17:47<1:33:25,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 160/1000 [17:53<1:33:30,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 161/1000 [18:00<1:33:30,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▌        | 162/1000 [18:07<1:33:26,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▋        | 163/1000 [18:13<1:33:24,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▋        | 164/1000 [18:20<1:33:16,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 16%|█▋        | 165/1000 [18:27<1:33:13,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 166/1000 [18:33<1:32:47,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 167/1000 [18:40<1:33:17,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 168/1000 [18:47<1:33:06,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 169/1000 [18:54<1:32:55,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 170/1000 [19:00<1:32:32,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 171/1000 [19:07<1:32:21,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 172/1000 [19:14<1:32:19,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 173/1000 [19:20<1:32:07,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|█▋        | 174/1000 [19:27<1:31:59,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 175/1000 [19:34<1:31:48,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 176/1000 [19:40<1:31:40,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 177/1000 [19:47<1:31:45,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 178/1000 [19:54<1:31:36,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 179/1000 [20:00<1:31:22,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 180/1000 [20:07<1:31:19,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 181/1000 [20:14<1:31:24,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 182/1000 [20:21<1:31:36,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 183/1000 [20:27<1:31:39,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 184/1000 [20:34<1:31:27,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 18%|█▊        | 185/1000 [20:41<1:31:09,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▊        | 186/1000 [20:47<1:30:50,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▊        | 187/1000 [20:54<1:30:38,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 188/1000 [21:01<1:30:23,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 189/1000 [21:08<1:30:34,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 190/1000 [21:14<1:30:29,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 191/1000 [21:21<1:30:30,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 192/1000 [21:28<1:30:49,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 193/1000 [21:34<1:30:18,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 19%|█▉        | 194/1000 [21:41<1:30:09,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|█▉        | 195/1000 [21:48<1:30:00,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|█▉        | 196/1000 [21:55<1:29:42,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|█▉        | 197/1000 [22:01<1:29:42,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|█▉        | 198/1000 [22:08<1:29:23,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|█▉        | 199/1000 [22:15<1:29:22,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 200/1000 [22:21<1:29:18,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 201/1000 [22:28<1:29:11,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 202/1000 [22:35<1:29:05,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 203/1000 [22:41<1:29:07,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 204/1000 [22:48<1:28:46,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|██        | 205/1000 [22:55<1:28:43,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 206/1000 [23:01<1:28:25,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 207/1000 [23:08<1:28:22,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 208/1000 [23:15<1:28:11,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 209/1000 [23:22<1:28:15,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 210/1000 [23:28<1:28:23,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 211/1000 [23:35<1:28:10,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██        | 212/1000 [23:42<1:27:37,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██▏       | 213/1000 [23:48<1:28:03,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 21%|██▏       | 214/1000 [23:55<1:27:44,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 215/1000 [24:02<1:27:46,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 216/1000 [24:09<1:27:38,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 217/1000 [24:15<1:27:22,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 218/1000 [24:22<1:27:11,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 219/1000 [24:29<1:27:01,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 220/1000 [24:35<1:26:53,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 221/1000 [24:42<1:26:55,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 222/1000 [24:49<1:27:19,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 223/1000 [24:55<1:27:06,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▏       | 224/1000 [25:02<1:26:47,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 22%|██▎       | 225/1000 [25:09<1:27:05,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 226/1000 [25:16<1:26:44,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 227/1000 [25:22<1:26:52,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 228/1000 [25:29<1:26:11,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 229/1000 [25:36<1:25:56,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 230/1000 [25:42<1:25:45,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 231/1000 [25:49<1:25:36,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 232/1000 [25:56<1:25:23,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 233/1000 [26:02<1:25:08,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|██▎       | 234/1000 [26:09<1:24:58,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▎       | 235/1000 [26:16<1:24:59,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▎       | 236/1000 [26:22<1:24:47,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▎       | 237/1000 [26:29<1:25:01,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 238/1000 [26:36<1:25:19,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 239/1000 [26:43<1:25:13,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 240/1000 [26:49<1:25:03,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 241/1000 [26:56<1:25:06,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 242/1000 [27:03<1:25:34,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 243/1000 [27:10<1:25:09,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 244/1000 [27:16<1:25:07,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 24%|██▍       | 245/1000 [27:23<1:25:04,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▍       | 246/1000 [27:30<1:24:54,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▍       | 247/1000 [27:37<1:24:42,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▍       | 248/1000 [27:43<1:24:27,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▍       | 249/1000 [27:50<1:24:16,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▌       | 250/1000 [27:57<1:24:16,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▌       | 251/1000 [28:04<1:24:23,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▌       | 252/1000 [28:10<1:24:08,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▌       | 253/1000 [28:17<1:24:00,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 25%|██▌       | 254/1000 [28:24<1:23:46,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 255/1000 [28:31<1:23:46,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 256/1000 [28:37<1:23:38,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 257/1000 [28:44<1:23:29,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 258/1000 [28:51<1:23:08,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 259/1000 [28:58<1:23:29,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 260/1000 [29:04<1:23:18,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 261/1000 [29:11<1:23:20,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▌       | 262/1000 [29:18<1:23:02,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▋       | 263/1000 [29:25<1:22:59,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▋       | 264/1000 [29:31<1:23:04,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 26%|██▋       | 265/1000 [29:38<1:22:53,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 266/1000 [29:45<1:22:37,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 267/1000 [29:52<1:22:30,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 268/1000 [29:58<1:22:20,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 269/1000 [30:05<1:22:21,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 270/1000 [30:12<1:22:04,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 271/1000 [30:19<1:21:52,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 272/1000 [30:25<1:22:16,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 273/1000 [30:32<1:22:05,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|██▋       | 274/1000 [30:39<1:21:49,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 275/1000 [30:46<1:21:44,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 276/1000 [30:53<1:21:42,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 277/1000 [30:59<1:21:25,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 278/1000 [31:06<1:20:54,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 279/1000 [31:13<1:20:48,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 280/1000 [31:19<1:20:44,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 281/1000 [31:26<1:20:35,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 282/1000 [31:33<1:20:30,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 283/1000 [31:40<1:20:28,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 284/1000 [31:46<1:20:07,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 28%|██▊       | 285/1000 [31:53<1:20:32,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▊       | 286/1000 [32:00<1:20:20,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▊       | 287/1000 [32:06<1:19:57,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 288/1000 [32:13<1:19:52,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 289/1000 [32:20<1:19:43,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 290/1000 [32:27<1:19:31,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 291/1000 [32:33<1:19:34,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 292/1000 [32:40<1:19:33,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 293/1000 [32:47<1:19:28,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 29%|██▉       | 294/1000 [32:54<1:19:44,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|██▉       | 295/1000 [33:00<1:19:26,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|██▉       | 296/1000 [33:07<1:19:15,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|██▉       | 297/1000 [33:14<1:18:52,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|██▉       | 298/1000 [33:21<1:18:42,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|██▉       | 299/1000 [33:27<1:18:35,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 300/1000 [33:34<1:18:29,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 301/1000 [33:41<1:18:33,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 302/1000 [33:48<1:18:20,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 303/1000 [33:54<1:18:11,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 304/1000 [34:01<1:18:01,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|███       | 305/1000 [34:08<1:17:58,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 306/1000 [34:15<1:18:19,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 307/1000 [34:21<1:17:50,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 308/1000 [34:28<1:17:52,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 309/1000 [34:35<1:17:49,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 310/1000 [34:42<1:17:33,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 311/1000 [34:48<1:17:22,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███       | 312/1000 [34:55<1:17:11,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███▏      | 313/1000 [35:02<1:16:49,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 31%|███▏      | 314/1000 [35:08<1:16:38,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 315/1000 [35:15<1:16:28,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 316/1000 [35:22<1:16:37,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 317/1000 [35:28<1:16:24,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 318/1000 [35:35<1:16:10,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 319/1000 [35:42<1:15:47,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 320/1000 [35:48<1:15:41,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 321/1000 [35:55<1:15:28,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 322/1000 [36:02<1:15:47,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 323/1000 [36:09<1:15:30,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▏      | 324/1000 [36:15<1:15:57,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 32%|███▎      | 325/1000 [36:22<1:15:39,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 326/1000 [36:29<1:15:22,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 327/1000 [36:36<1:15:28,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 328/1000 [36:42<1:15:32,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 329/1000 [36:49<1:15:04,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 330/1000 [36:56<1:14:57,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 331/1000 [37:03<1:15:12,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 332/1000 [37:09<1:14:56,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 333/1000 [37:16<1:14:53,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|███▎      | 334/1000 [37:23<1:14:35,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▎      | 335/1000 [37:29<1:14:24,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▎      | 336/1000 [37:36<1:14:28,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▎      | 337/1000 [37:43<1:14:22,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 338/1000 [37:50<1:14:13,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 339/1000 [37:56<1:14:10,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 340/1000 [38:03<1:14:07,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 341/1000 [38:10<1:13:54,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 342/1000 [38:16<1:13:33,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 343/1000 [38:23<1:13:58,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 344/1000 [38:30<1:13:57,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 34%|███▍      | 345/1000 [38:37<1:13:56,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▍      | 346/1000 [38:44<1:13:35,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▍      | 347/1000 [38:50<1:13:13,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▍      | 348/1000 [38:57<1:13:24,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▍      | 349/1000 [39:04<1:13:43,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▌      | 350/1000 [39:11<1:13:27,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▌      | 351/1000 [39:17<1:12:59,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▌      | 352/1000 [39:24<1:12:57,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▌      | 353/1000 [39:31<1:12:45,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 35%|███▌      | 354/1000 [39:38<1:12:37,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 355/1000 [39:44<1:12:18,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 356/1000 [39:51<1:12:09,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 357/1000 [39:58<1:11:40,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 358/1000 [40:04<1:11:34,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 359/1000 [40:11<1:11:32,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 360/1000 [40:18<1:11:18,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 361/1000 [40:24<1:11:04,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▌      | 362/1000 [40:31<1:11:15,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▋      | 363/1000 [40:38<1:11:06,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▋      | 364/1000 [40:44<1:10:52,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 36%|███▋      | 365/1000 [40:51<1:10:47,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 366/1000 [40:58<1:10:58,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 367/1000 [41:05<1:10:50,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 368/1000 [41:11<1:10:40,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 369/1000 [41:18<1:10:45,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 370/1000 [41:25<1:10:30,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 371/1000 [41:32<1:10:31,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 372/1000 [41:38<1:10:07,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 373/1000 [41:45<1:10:04,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|███▋      | 374/1000 [41:52<1:10:04,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 375/1000 [41:58<1:09:51,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 376/1000 [42:05<1:09:48,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 377/1000 [42:12<1:09:50,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 378/1000 [42:19<1:09:37,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 379/1000 [42:25<1:09:19,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 380/1000 [42:32<1:09:17,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 381/1000 [42:39<1:09:21,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 382/1000 [42:45<1:09:05,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 383/1000 [42:52<1:08:59,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 384/1000 [42:59<1:08:53,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 38%|███▊      | 385/1000 [43:05<1:08:48,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▊      | 386/1000 [43:12<1:08:47,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▊      | 387/1000 [43:19<1:08:42,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 388/1000 [43:26<1:08:56,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 389/1000 [43:32<1:08:37,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 390/1000 [43:39<1:08:53,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 391/1000 [43:46<1:08:42,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 392/1000 [43:53<1:08:22,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 393/1000 [43:59<1:07:56,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 39%|███▉      | 394/1000 [44:06<1:07:51,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|███▉      | 395/1000 [44:13<1:07:50,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|███▉      | 396/1000 [44:20<1:07:35,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|███▉      | 397/1000 [44:26<1:07:40,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|███▉      | 398/1000 [44:33<1:07:32,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|███▉      | 399/1000 [44:40<1:07:21,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 400/1000 [44:46<1:07:07,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 401/1000 [44:53<1:07:22,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 402/1000 [45:00<1:08:10,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 403/1000 [45:07<1:08:16,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 404/1000 [45:14<1:07:42,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|████      | 405/1000 [45:21<1:07:32,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 406/1000 [45:28<1:07:10,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 407/1000 [45:34<1:06:57,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 408/1000 [45:41<1:06:34,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 409/1000 [45:48<1:06:40,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 410/1000 [45:55<1:06:34,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 411/1000 [46:01<1:06:29,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████      | 412/1000 [46:08<1:06:08,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████▏     | 413/1000 [46:15<1:05:55,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 41%|████▏     | 414/1000 [46:21<1:05:46,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 415/1000 [46:28<1:05:27,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 416/1000 [46:35<1:05:04,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 417/1000 [46:41<1:05:09,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 418/1000 [46:48<1:05:02,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 419/1000 [46:55<1:05:01,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 420/1000 [47:02<1:05:00,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 421/1000 [47:08<1:04:49,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 422/1000 [47:15<1:04:38,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 423/1000 [47:22<1:04:51,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▏     | 424/1000 [47:29<1:04:26,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 42%|████▎     | 425/1000 [47:35<1:04:33,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 426/1000 [47:42<1:04:25,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 427/1000 [47:49<1:04:22,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 428/1000 [47:56<1:04:38,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 429/1000 [48:02<1:04:27,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 430/1000 [48:09<1:04:15,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 431/1000 [48:16<1:04:06,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 432/1000 [48:23<1:04:33,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 433/1000 [48:30<1:04:15,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|████▎     | 434/1000 [48:36<1:04:06,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▎     | 435/1000 [48:43<1:03:46,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▎     | 436/1000 [48:50<1:03:30,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▎     | 437/1000 [48:57<1:03:09,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 438/1000 [49:03<1:03:00,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 439/1000 [49:10<1:03:10,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 440/1000 [49:17<1:03:03,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 441/1000 [49:24<1:02:52,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 442/1000 [49:30<1:02:43,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 443/1000 [49:37<1:02:16,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 444/1000 [49:44<1:02:14,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 44%|████▍     | 445/1000 [49:50<1:02:21,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▍     | 446/1000 [49:57<1:02:17,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▍     | 447/1000 [50:04<1:02:07,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▍     | 448/1000 [50:11<1:02:20,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▍     | 449/1000 [50:18<1:02:20,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▌     | 450/1000 [50:24<1:01:58,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▌     | 451/1000 [50:31<1:01:51,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▌     | 452/1000 [50:38<1:01:24,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▌     | 453/1000 [50:45<1:01:34,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 45%|████▌     | 454/1000 [50:51<1:01:24,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 455/1000 [50:58<1:01:16,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 456/1000 [51:05<1:01:03,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 457/1000 [51:11<1:00:53,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 458/1000 [51:18<1:00:38,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 459/1000 [51:25<1:00:43,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 460/1000 [51:32<1:00:43,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 461/1000 [51:38<1:00:38,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▌     | 462/1000 [51:45<1:00:20,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▋     | 463/1000 [51:52<1:00:09,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▋     | 464/1000 [51:59<59:54,  6.71s/it]  X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 46%|████▋     | 465/1000 [52:05<59:48,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 466/1000 [52:12<59:44,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 467/1000 [52:19<59:39,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 468/1000 [52:25<59:30,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 469/1000 [52:32<59:28,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 470/1000 [52:39<59:12,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 471/1000 [52:46<59:39,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 472/1000 [52:52<59:32,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 473/1000 [52:59<59:20,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|████▋     | 474/1000 [53:06<59:23,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 475/1000 [53:13<59:01,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 476/1000 [53:19<58:54,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 477/1000 [53:26<58:47,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 478/1000 [53:33<58:34,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 479/1000 [53:40<58:44,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 480/1000 [53:46<58:26,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 481/1000 [53:53<58:13,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 482/1000 [54:00<58:06,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 483/1000 [54:07<58:01,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 484/1000 [54:13<57:55,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 48%|████▊     | 485/1000 [54:20<57:42,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▊     | 486/1000 [54:27<57:35,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▊     | 487/1000 [54:33<57:34,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 488/1000 [54:40<57:35,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 489/1000 [54:47<57:22,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 490/1000 [54:54<57:10,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 491/1000 [55:00<56:58,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 492/1000 [55:07<56:49,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 493/1000 [55:14<56:53,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 49%|████▉     | 494/1000 [55:21<56:54,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|████▉     | 495/1000 [55:27<56:51,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|████▉     | 496/1000 [55:34<56:39,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|████▉     | 497/1000 [55:41<56:20,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|████▉     | 498/1000 [55:48<56:12,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|████▉     | 499/1000 [55:54<56:05,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 500/1000 [56:01<55:57,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 501/1000 [56:08<56:11,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 502/1000 [56:15<56:05,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 503/1000 [56:21<55:59,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 504/1000 [56:28<56:34,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|█████     | 505/1000 [56:36<58:18,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 506/1000 [56:43<59:02,  7.17s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 507/1000 [56:51<59:19,  7.22s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 508/1000 [56:58<58:22,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 509/1000 [57:04<56:59,  6.96s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 510/1000 [57:11<56:19,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 511/1000 [57:18<55:45,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████     | 512/1000 [57:24<55:23,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████▏    | 513/1000 [57:31<55:00,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 51%|█████▏    | 514/1000 [57:38<54:37,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 515/1000 [57:44<54:06,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 516/1000 [57:51<54:05,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 517/1000 [57:58<53:53,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 518/1000 [58:04<53:55,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 519/1000 [58:11<53:44,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 520/1000 [58:18<53:33,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 521/1000 [58:25<53:22,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 522/1000 [58:31<53:14,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 523/1000 [58:38<53:01,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▏    | 524/1000 [58:44<52:52,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 52%|█████▎    | 525/1000 [58:51<52:31,  6.64s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 526/1000 [58:58<52:35,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 527/1000 [59:04<52:30,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 528/1000 [59:11<52:29,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 529/1000 [59:18<52:30,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 530/1000 [59:24<52:18,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 531/1000 [59:31<52:16,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 532/1000 [59:38<52:11,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 533/1000 [59:45<52:06,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|█████▎    | 534/1000 [59:51<51:47,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▎    | 535/1000 [59:58<51:48,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▎    | 536/1000 [1:00:05<51:35,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▎    | 537/1000 [1:00:11<51:22,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 538/1000 [1:00:18<51:11,  6.65s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 539/1000 [1:00:24<51:05,  6.65s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 540/1000 [1:00:31<51:09,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 541/1000 [1:00:38<51:07,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 542/1000 [1:00:45<50:51,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 543/1000 [1:00:51<50:42,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 54%|█████▍    | 544/1000 [1:00:58<50:36,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▍    | 545/1000 [1:01:05<50:36,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▍    | 546/1000 [1:01:11<50:24,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▍    | 547/1000 [1:01:18<50:26,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▍    | 548/1000 [1:01:25<50:19,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▍    | 549/1000 [1:01:31<50:09,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▌    | 550/1000 [1:01:38<50:04,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▌    | 551/1000 [1:01:45<50:15,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▌    | 552/1000 [1:01:51<50:09,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▌    | 553/1000 [1:01:58<49:57,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 55%|█████▌    | 554/1000 [1:02:05<49:45,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 555/1000 [1:02:12<49:49,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 556/1000 [1:02:18<49:40,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 557/1000 [1:02:25<49:30,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 558/1000 [1:02:32<49:48,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 559/1000 [1:02:39<49:57,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 560/1000 [1:02:45<49:43,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 561/1000 [1:02:52<49:23,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▌    | 562/1000 [1:02:59<49:05,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▋    | 563/1000 [1:03:05<48:51,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▋    | 564/1000 [1:03:12<48:50,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 56%|█████▋    | 565/1000 [1:03:19<48:59,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 566/1000 [1:03:26<48:43,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 567/1000 [1:03:32<48:27,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 568/1000 [1:03:39<48:20,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 569/1000 [1:03:46<48:08,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 570/1000 [1:03:52<47:52,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 571/1000 [1:03:59<48:04,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 572/1000 [1:04:06<48:27,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 573/1000 [1:04:13<48:13,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▋    | 574/1000 [1:04:20<48:25,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|█████▊    | 575/1000 [1:04:27<48:07,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 576/1000 [1:04:34<48:13,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 577/1000 [1:04:40<48:24,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 578/1000 [1:04:47<47:50,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 579/1000 [1:04:54<48:09,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 580/1000 [1:05:01<47:51,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 581/1000 [1:05:08<47:44,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 582/1000 [1:05:15<47:59,  6.89s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 583/1000 [1:05:22<47:49,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 584/1000 [1:05:29<47:57,  6.92s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 58%|█████▊    | 585/1000 [1:05:36<48:08,  6.96s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▊    | 586/1000 [1:05:42<47:40,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▊    | 587/1000 [1:05:49<47:07,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 588/1000 [1:05:56<46:47,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 589/1000 [1:06:03<46:14,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 590/1000 [1:06:09<45:59,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 591/1000 [1:06:17<47:17,  6.94s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 592/1000 [1:06:24<47:35,  7.00s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 593/1000 [1:06:31<47:35,  7.02s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 59%|█████▉    | 594/1000 [1:06:38<48:16,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|█████▉    | 595/1000 [1:06:45<48:21,  7.16s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|█████▉    | 596/1000 [1:06:52<47:15,  7.02s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|█████▉    | 597/1000 [1:06:59<47:02,  7.00s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|█████▉    | 598/1000 [1:07:06<47:01,  7.02s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|█████▉    | 599/1000 [1:07:13<47:11,  7.06s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 600/1000 [1:07:20<47:06,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 601/1000 [1:07:27<46:14,  6.95s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 602/1000 [1:07:34<45:30,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 603/1000 [1:07:40<44:54,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 604/1000 [1:07:47<44:33,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|██████    | 605/1000 [1:07:54<44:03,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 606/1000 [1:08:00<43:47,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 607/1000 [1:08:07<43:41,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 608/1000 [1:08:14<43:40,  6.68s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 609/1000 [1:08:20<43:27,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 610/1000 [1:08:27<43:16,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 611/1000 [1:08:34<43:16,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████    | 612/1000 [1:08:40<43:14,  6.69s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████▏   | 613/1000 [1:08:47<43:11,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 61%|██████▏   | 614/1000 [1:08:54<42:55,  6.67s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 615/1000 [1:09:00<42:38,  6.64s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 616/1000 [1:09:07<42:30,  6.64s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 617/1000 [1:09:13<42:18,  6.63s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 618/1000 [1:09:20<42:10,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 619/1000 [1:09:27<42:05,  6.63s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 620/1000 [1:09:33<41:56,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 621/1000 [1:09:40<41:53,  6.63s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 622/1000 [1:09:47<42:23,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 623/1000 [1:09:54<42:07,  6.71s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▏   | 624/1000 [1:10:00<41:38,  6.65s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 62%|██████▎   | 625/1000 [1:10:07<41:29,  6.64s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 626/1000 [1:10:13<41:15,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 627/1000 [1:10:20<41:08,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 628/1000 [1:10:27<41:05,  6.63s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 629/1000 [1:10:33<40:55,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 630/1000 [1:10:40<40:52,  6.63s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 631/1000 [1:10:46<40:44,  6.62s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 632/1000 [1:10:53<40:30,  6.61s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 633/1000 [1:11:00<40:39,  6.65s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|██████▎   | 634/1000 [1:11:06<40:30,  6.64s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▎   | 635/1000 [1:11:13<40:27,  6.65s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▎   | 636/1000 [1:11:20<40:40,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▎   | 637/1000 [1:11:26<40:19,  6.66s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 638/1000 [1:11:33<40:24,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 639/1000 [1:11:40<40:40,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 640/1000 [1:11:47<41:17,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 641/1000 [1:11:54<41:34,  6.95s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 642/1000 [1:12:01<41:07,  6.89s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 643/1000 [1:12:08<40:42,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 644/1000 [1:12:14<40:16,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 64%|██████▍   | 645/1000 [1:12:21<39:57,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▍   | 646/1000 [1:12:28<39:51,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▍   | 647/1000 [1:12:35<39:55,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▍   | 648/1000 [1:12:41<39:39,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▍   | 649/1000 [1:12:48<39:28,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▌   | 650/1000 [1:12:55<39:35,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▌   | 651/1000 [1:13:02<39:54,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▌   | 652/1000 [1:13:09<39:31,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▌   | 653/1000 [1:13:16<39:46,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 65%|██████▌   | 654/1000 [1:13:23<39:23,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 655/1000 [1:13:30<39:34,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 656/1000 [1:13:36<39:11,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 657/1000 [1:13:43<38:55,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 658/1000 [1:13:50<38:43,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 659/1000 [1:13:57<38:54,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 660/1000 [1:14:04<39:10,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 661/1000 [1:14:11<39:58,  7.08s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▌   | 662/1000 [1:14:19<40:13,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▋   | 663/1000 [1:14:25<39:42,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▋   | 664/1000 [1:14:32<39:30,  7.05s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 66%|██████▋   | 665/1000 [1:14:39<39:15,  7.03s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 666/1000 [1:14:46<38:58,  7.00s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 667/1000 [1:14:53<38:53,  7.01s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 668/1000 [1:15:00<38:21,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 669/1000 [1:15:07<38:23,  6.96s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 670/1000 [1:15:14<38:28,  6.99s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 671/1000 [1:15:22<39:00,  7.11s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 672/1000 [1:15:29<38:57,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 673/1000 [1:15:36<39:13,  7.20s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|██████▋   | 674/1000 [1:15:43<39:04,  7.19s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 675/1000 [1:15:50<38:40,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 676/1000 [1:15:58<38:31,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 677/1000 [1:16:05<38:36,  7.17s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 678/1000 [1:16:12<38:32,  7.18s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 679/1000 [1:16:19<38:06,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 680/1000 [1:16:26<38:01,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 681/1000 [1:16:33<38:01,  7.15s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 682/1000 [1:16:41<37:59,  7.17s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 683/1000 [1:16:48<38:12,  7.23s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 684/1000 [1:16:55<37:50,  7.19s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 68%|██████▊   | 685/1000 [1:17:02<37:31,  7.15s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▊   | 686/1000 [1:17:09<37:04,  7.09s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▊   | 687/1000 [1:17:16<36:52,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 688/1000 [1:17:23<36:50,  7.09s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 689/1000 [1:17:30<36:41,  7.08s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 690/1000 [1:17:38<37:15,  7.21s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 691/1000 [1:17:45<36:44,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 692/1000 [1:17:52<36:39,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 693/1000 [1:17:59<36:28,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 69%|██████▉   | 694/1000 [1:18:06<36:23,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|██████▉   | 695/1000 [1:18:13<36:22,  7.16s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|██████▉   | 696/1000 [1:18:20<36:08,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|██████▉   | 697/1000 [1:18:27<35:51,  7.10s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|██████▉   | 698/1000 [1:18:34<35:36,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|██████▉   | 699/1000 [1:18:41<35:22,  7.05s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 700/1000 [1:18:49<35:34,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 701/1000 [1:18:56<35:29,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 702/1000 [1:19:03<35:26,  7.14s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 703/1000 [1:19:10<35:24,  7.15s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 704/1000 [1:19:17<35:27,  7.19s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|███████   | 705/1000 [1:19:24<35:08,  7.15s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 706/1000 [1:19:32<34:52,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 707/1000 [1:19:39<34:43,  7.11s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 708/1000 [1:19:46<34:34,  7.11s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 709/1000 [1:19:53<34:26,  7.10s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 710/1000 [1:20:00<34:13,  7.08s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 711/1000 [1:20:07<34:09,  7.09s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████   | 712/1000 [1:20:14<34:21,  7.16s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████▏  | 713/1000 [1:20:21<34:13,  7.16s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 71%|███████▏  | 714/1000 [1:20:29<34:09,  7.17s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 715/1000 [1:20:36<34:06,  7.18s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 716/1000 [1:20:43<34:09,  7.22s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 717/1000 [1:20:51<34:22,  7.29s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 718/1000 [1:20:58<34:05,  7.25s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 719/1000 [1:21:05<33:20,  7.12s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 720/1000 [1:21:12<32:59,  7.07s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 721/1000 [1:21:18<32:27,  6.98s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 722/1000 [1:21:25<32:10,  6.94s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 723/1000 [1:21:32<32:20,  7.00s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▏  | 724/1000 [1:21:39<31:58,  6.95s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 72%|███████▎  | 725/1000 [1:21:46<31:34,  6.89s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 726/1000 [1:21:53<31:16,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 727/1000 [1:21:59<31:03,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 728/1000 [1:22:06<30:41,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 729/1000 [1:22:13<30:36,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 730/1000 [1:22:20<30:31,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 731/1000 [1:22:26<30:26,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 732/1000 [1:22:33<30:19,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 733/1000 [1:22:40<30:14,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|███████▎  | 734/1000 [1:22:47<30:05,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▎  | 735/1000 [1:22:54<29:58,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▎  | 736/1000 [1:23:00<29:46,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▎  | 737/1000 [1:23:07<29:37,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 738/1000 [1:23:14<29:35,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 739/1000 [1:23:21<29:27,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 740/1000 [1:23:27<29:24,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 741/1000 [1:23:34<29:17,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 742/1000 [1:23:41<29:17,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 743/1000 [1:23:48<29:32,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 744/1000 [1:23:55<29:17,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 74%|███████▍  | 745/1000 [1:24:02<29:05,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▍  | 746/1000 [1:24:09<28:53,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▍  | 747/1000 [1:24:16<29:07,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▍  | 748/1000 [1:24:22<28:47,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▍  | 749/1000 [1:24:29<28:32,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▌  | 750/1000 [1:24:36<28:19,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▌  | 751/1000 [1:24:43<28:06,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▌  | 752/1000 [1:24:49<28:00,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▌  | 753/1000 [1:24:56<27:45,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 75%|███████▌  | 754/1000 [1:25:03<27:39,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 755/1000 [1:25:10<27:36,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 756/1000 [1:25:16<27:31,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 757/1000 [1:25:23<27:45,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 758/1000 [1:25:30<27:31,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 759/1000 [1:25:37<27:35,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 760/1000 [1:25:44<27:22,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 761/1000 [1:25:51<27:10,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▌  | 762/1000 [1:25:57<26:53,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▋  | 763/1000 [1:26:04<26:43,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▋  | 764/1000 [1:26:11<26:36,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 76%|███████▋  | 765/1000 [1:26:18<26:24,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 766/1000 [1:26:25<26:37,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 767/1000 [1:26:31<26:30,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 768/1000 [1:26:38<26:25,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 769/1000 [1:26:45<26:13,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 770/1000 [1:26:52<25:54,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 771/1000 [1:26:58<25:50,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 772/1000 [1:27:05<25:45,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 773/1000 [1:27:12<25:38,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|███████▋  | 774/1000 [1:27:19<25:29,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 775/1000 [1:27:26<25:21,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 776/1000 [1:27:32<25:16,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 777/1000 [1:27:39<25:07,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 778/1000 [1:27:46<25:25,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 779/1000 [1:27:53<25:23,  6.89s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 780/1000 [1:28:00<25:13,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 781/1000 [1:28:07<25:17,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 782/1000 [1:28:14<25:00,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 783/1000 [1:28:21<24:44,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 784/1000 [1:28:27<24:40,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 78%|███████▊  | 785/1000 [1:28:34<24:23,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▊  | 786/1000 [1:28:41<24:18,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▊  | 787/1000 [1:28:48<24:07,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 788/1000 [1:28:55<24:01,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 789/1000 [1:29:01<24:00,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 790/1000 [1:29:08<23:46,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 791/1000 [1:29:15<23:49,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 792/1000 [1:29:22<24:00,  6.92s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 793/1000 [1:29:29<23:49,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 79%|███████▉  | 794/1000 [1:29:36<23:35,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|███████▉  | 795/1000 [1:29:43<23:17,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|███████▉  | 796/1000 [1:29:50<23:30,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|███████▉  | 797/1000 [1:29:56<23:15,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|███████▉  | 798/1000 [1:30:03<22:58,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|███████▉  | 799/1000 [1:30:10<22:56,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 800/1000 [1:30:17<22:44,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 801/1000 [1:30:24<22:35,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 802/1000 [1:30:30<22:24,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 803/1000 [1:30:37<22:24,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 804/1000 [1:30:44<22:14,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|████████  | 805/1000 [1:30:51<22:10,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 806/1000 [1:30:58<22:10,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 807/1000 [1:31:05<21:56,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 808/1000 [1:31:12<22:26,  7.01s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 809/1000 [1:31:19<22:05,  6.94s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 810/1000 [1:31:26<21:53,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 811/1000 [1:31:33<21:43,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████  | 812/1000 [1:31:39<21:31,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████▏ | 813/1000 [1:31:46<21:14,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 81%|████████▏ | 814/1000 [1:31:53<21:14,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 815/1000 [1:32:00<21:03,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 816/1000 [1:32:06<20:52,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 817/1000 [1:32:13<20:41,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 818/1000 [1:32:20<20:33,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 819/1000 [1:32:27<20:24,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 820/1000 [1:32:33<20:17,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 821/1000 [1:32:40<20:09,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 822/1000 [1:32:47<19:59,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 823/1000 [1:32:54<19:53,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▏ | 824/1000 [1:33:01<19:57,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 82%|████████▎ | 825/1000 [1:33:08<20:02,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 826/1000 [1:33:15<20:01,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 827/1000 [1:33:21<19:50,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 828/1000 [1:33:28<19:40,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 829/1000 [1:33:35<19:35,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 830/1000 [1:33:42<19:26,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 831/1000 [1:33:49<19:14,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 832/1000 [1:33:56<19:10,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 833/1000 [1:34:02<18:55,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|████████▎ | 834/1000 [1:34:09<18:47,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▎ | 835/1000 [1:34:16<18:39,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▎ | 836/1000 [1:34:23<18:30,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▎ | 837/1000 [1:34:29<18:23,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 838/1000 [1:34:36<18:18,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 839/1000 [1:34:43<18:10,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 840/1000 [1:34:50<18:06,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 841/1000 [1:34:57<18:00,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 842/1000 [1:35:03<17:51,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 843/1000 [1:35:10<17:44,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 844/1000 [1:35:17<17:38,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 84%|████████▍ | 845/1000 [1:35:24<17:26,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▍ | 846/1000 [1:35:30<17:22,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▍ | 847/1000 [1:35:37<17:14,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▍ | 848/1000 [1:35:44<17:09,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▍ | 849/1000 [1:35:51<17:05,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▌ | 850/1000 [1:35:58<16:57,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▌ | 851/1000 [1:36:04<16:52,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▌ | 852/1000 [1:36:11<16:44,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▌ | 853/1000 [1:36:18<16:32,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 85%|████████▌ | 854/1000 [1:36:25<16:26,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 855/1000 [1:36:31<16:19,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 856/1000 [1:36:38<16:27,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 857/1000 [1:36:45<16:17,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 858/1000 [1:36:52<16:09,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 859/1000 [1:36:59<16:00,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 860/1000 [1:37:06<15:54,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 861/1000 [1:37:13<15:58,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▌ | 862/1000 [1:37:20<15:55,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▋ | 863/1000 [1:37:26<15:42,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▋ | 864/1000 [1:37:33<15:32,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 86%|████████▋ | 865/1000 [1:37:40<15:35,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 866/1000 [1:37:48<15:42,  7.04s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 867/1000 [1:37:55<15:41,  7.08s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 868/1000 [1:38:02<15:26,  7.02s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 869/1000 [1:38:09<15:17,  7.00s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 870/1000 [1:38:16<15:15,  7.04s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 871/1000 [1:38:23<15:20,  7.13s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 872/1000 [1:38:30<15:03,  7.06s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 873/1000 [1:38:37<14:50,  7.01s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|████████▋ | 874/1000 [1:38:44<14:36,  6.96s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 875/1000 [1:38:51<14:30,  6.96s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 876/1000 [1:38:58<14:19,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 877/1000 [1:39:04<14:09,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 878/1000 [1:39:11<13:57,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 879/1000 [1:39:18<13:58,  6.93s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 880/1000 [1:39:25<13:49,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 881/1000 [1:39:32<13:39,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 882/1000 [1:39:39<13:34,  6.90s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 883/1000 [1:39:46<13:24,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 884/1000 [1:39:53<13:15,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 88%|████████▊ | 885/1000 [1:39:59<13:06,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▊ | 886/1000 [1:40:06<12:57,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▊ | 887/1000 [1:40:13<12:47,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 888/1000 [1:40:20<12:43,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 889/1000 [1:40:26<12:35,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 890/1000 [1:40:33<12:32,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 891/1000 [1:40:40<12:25,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 892/1000 [1:40:47<12:19,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 893/1000 [1:40:54<12:14,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 89%|████████▉ | 894/1000 [1:41:01<12:04,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|████████▉ | 895/1000 [1:41:07<11:53,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|████████▉ | 896/1000 [1:41:14<11:45,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|████████▉ | 897/1000 [1:41:21<11:38,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|████████▉ | 898/1000 [1:41:28<11:31,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|████████▉ | 899/1000 [1:41:35<11:22,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 900/1000 [1:41:41<11:14,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 901/1000 [1:41:48<11:07,  6.74s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 902/1000 [1:41:55<10:59,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 903/1000 [1:42:01<10:50,  6.70s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 904/1000 [1:42:08<10:44,  6.72s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|█████████ | 905/1000 [1:42:15<10:39,  6.73s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 906/1000 [1:42:22<10:38,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 907/1000 [1:42:28<10:30,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 908/1000 [1:42:35<10:25,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 909/1000 [1:42:42<10:18,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 910/1000 [1:42:49<10:11,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 911/1000 [1:42:56<10:04,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████ | 912/1000 [1:43:02<09:57,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████▏| 913/1000 [1:43:09<09:52,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 91%|█████████▏| 914/1000 [1:43:16<09:45,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 915/1000 [1:43:23<09:38,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 916/1000 [1:43:30<09:38,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 917/1000 [1:43:37<09:26,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 918/1000 [1:43:44<09:19,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 919/1000 [1:43:50<09:16,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 920/1000 [1:43:57<09:07,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 921/1000 [1:44:04<08:59,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 922/1000 [1:44:11<08:52,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 923/1000 [1:44:18<08:43,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▏| 924/1000 [1:44:24<08:36,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 92%|█████████▎| 925/1000 [1:44:31<08:29,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 926/1000 [1:44:38<08:20,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 927/1000 [1:44:45<08:15,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 928/1000 [1:44:51<08:07,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 929/1000 [1:44:58<08:02,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 930/1000 [1:45:05<07:54,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 931/1000 [1:45:12<07:47,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 932/1000 [1:45:19<07:39,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 933/1000 [1:45:25<07:34,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|█████████▎| 934/1000 [1:45:32<07:29,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▎| 935/1000 [1:45:39<07:21,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▎| 936/1000 [1:45:46<07:18,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▎| 937/1000 [1:45:53<07:09,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 938/1000 [1:46:00<07:02,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 939/1000 [1:46:06<06:54,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 940/1000 [1:46:13<06:54,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 941/1000 [1:46:21<06:51,  6.97s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 942/1000 [1:46:27<06:40,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 943/1000 [1:46:34<06:30,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 944/1000 [1:46:41<06:21,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 94%|█████████▍| 945/1000 [1:46:48<06:14,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▍| 946/1000 [1:46:54<06:06,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▍| 947/1000 [1:47:01<05:59,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▍| 948/1000 [1:47:08<05:51,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▍| 949/1000 [1:47:15<05:45,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▌| 950/1000 [1:47:21<05:37,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▌| 951/1000 [1:47:28<05:30,  6.75s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▌| 952/1000 [1:47:35<05:28,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▌| 953/1000 [1:47:42<05:20,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 95%|█████████▌| 954/1000 [1:47:49<05:13,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 955/1000 [1:47:55<05:05,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 956/1000 [1:48:03<05:03,  6.89s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 957/1000 [1:48:09<04:54,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 958/1000 [1:48:16<04:51,  6.94s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 959/1000 [1:48:23<04:42,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 960/1000 [1:48:30<04:34,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 961/1000 [1:48:37<04:26,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▌| 962/1000 [1:48:43<04:18,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▋| 963/1000 [1:48:50<04:13,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▋| 964/1000 [1:48:57<04:05,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 96%|█████████▋| 965/1000 [1:49:04<03:58,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 966/1000 [1:49:11<03:50,  6.77s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 967/1000 [1:49:17<03:43,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 968/1000 [1:49:24<03:36,  6.76s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 969/1000 [1:49:31<03:33,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 970/1000 [1:49:38<03:25,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 971/1000 [1:49:45<03:18,  6.85s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 972/1000 [1:49:52<03:11,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 973/1000 [1:49:59<03:06,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|█████████▋| 974/1000 [1:50:06<02:58,  6.87s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 975/1000 [1:50:12<02:51,  6.86s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 976/1000 [1:50:19<02:43,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 977/1000 [1:50:26<02:36,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 978/1000 [1:50:33<02:29,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 979/1000 [1:50:40<02:23,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 980/1000 [1:50:46<02:16,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 981/1000 [1:50:53<02:09,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 982/1000 [1:51:00<02:02,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 983/1000 [1:51:07<01:55,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 984/1000 [1:51:14<01:49,  6.81s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 98%|█████████▊| 985/1000 [1:51:20<01:42,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▊| 986/1000 [1:51:27<01:35,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▊| 987/1000 [1:51:34<01:28,  6.79s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 988/1000 [1:51:41<01:21,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 989/1000 [1:51:48<01:15,  6.84s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 990/1000 [1:51:54<01:08,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 991/1000 [1:52:01<01:01,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 992/1000 [1:52:08<00:54,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 993/1000 [1:52:15<00:47,  6.78s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 99%|█████████▉| 994/1000 [1:52:22<00:40,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|█████████▉| 995/1000 [1:52:29<00:34,  6.91s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|█████████▉| 996/1000 [1:52:36<00:27,  6.88s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|█████████▉| 997/1000 [1:52:42<00:20,  6.83s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|█████████▉| 998/1000 [1:52:49<00:13,  6.80s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|█████████▉| 999/1000 [1:52:56<00:06,  6.82s/it]X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|██████████| 1000/1000 [1:53:03<00:00,  6.78s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x684 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model\n",
    "model_shap = logreg_best.fit(X_train_DF, y_train)\n",
    "\n",
    "# Define explainer (we only use sampling because of the huge amount of data)\n",
    "explainer = shap.KernelExplainer(model_shap.predict, shap.sample(X_train_DF, 1000))\n",
    "\n",
    "# Calculate the SHAP value\n",
    "shap_val = explainer.shap_values(shap.sample(X_train_DF, 1000))\n",
    "\n",
    "# Plot\n",
    "shap.summary_plot(shap_val, shap.sample(X_train_DF, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLANATION**\n",
    "- To read the chart we see the color first if it is red then the value of the specific feature is high, if it is blue then it is low. \n",
    "- We also see whether the point is on the right side or the left side, if it is on the right, then it have positive influence on the model, if it is on the left it have negative influence on the model\n",
    "\n",
    "- A bit different from the feature importance, the top feature is whether the customer is a pensioner or not (if it is then it will have negative impact on the model).\n",
    "- We can also see that the higher the days of work the more positive it have on our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6187ecb71c1773254e0c51638a13fd9fdc3e54342dd04fa191cb1110d1ed153d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
